[
  {
    "objectID": "presentations.html",
    "href": "presentations.html",
    "title": "Presentations",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nReproducible publication-ready plots and tables with ggplot2 and flextable\n\n\n\n\n\n\n\n\n\nOct 12, 2023\n\n\nDaniel Padfield\n\n\n\n\n\n\n  \n\n\n\n\nMaking reproducible projects\n\n\nAn introduction to version control with R Studio and GitHub\n\n\n\n\n\n\nJun 20, 2023\n\n\nStephen Lang, Dan Padfield  & Matt Jones\n\n\n\n\n\n\n  \n\n\n\n\nOpen and reproducible science symposium\n\n\nwith Mark Kelson and Erik Postma\n\n\n\n\n\n\nMar 28, 2023\n\n\nExDataHub, Mark Kelson & Erik Postma\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "presentations/intro_to_github/intro_to_github.html#workshop-outline",
    "href": "presentations/intro_to_github/intro_to_github.html#workshop-outline",
    "title": "Making reproducible projects",
    "section": "Workshop outline",
    "text": "Workshop outline\n\nWorkshop aims (reproducibility tools)\nIntroduction to version control\nOverview of git and GitHub\nUsing GitHub with R Studio\nIntegrating with R Studio Projects and here\nSome use-cases from us\nBonus session on data sharing! (w/ Matt Jones)\nPractical (bring your problem code!)"
  },
  {
    "objectID": "presentations/intro_to_github/intro_to_github.html#three-aims",
    "href": "presentations/intro_to_github/intro_to_github.html#three-aims",
    "title": "Making reproducible projects",
    "section": "Three aims:",
    "text": "Three aims:\n\n \n\n\n\n+\n\n\n\n\n\n+\n\n\n{here}\n\n\n \n \n \n \n\n\n\nKnow basics of using git/GitHub for version control\n\n\n\n\nLearn how to create R Studio Projects\n\n\n\n\nUnderstand how the here package works"
  },
  {
    "objectID": "presentations/intro_to_github/intro_to_github.html#preparation",
    "href": "presentations/intro_to_github/intro_to_github.html#preparation",
    "title": "Making reproducible projects",
    "section": "Preparation",
    "text": "Preparation\n\nChecked git is installed\nSigned up for a GitHub account\nAuthenticated GitHub on your machine\n\n  \n\nLet us know if you are stuck on any of these\n(Extra guidance is on the Exeter data Analytics Hub)"
  },
  {
    "objectID": "presentations/intro_to_github/intro_to_github.html#slide5-id",
    "href": "presentations/intro_to_github/intro_to_github.html#slide5-id",
    "title": "Making reproducible projects",
    "section": "",
    "text": "Why use version control?\n\n\n\n\n\ncomic: “Piled Higher and Deeper” by Jorge Cham (www.phdcomics.com)"
  },
  {
    "objectID": "presentations/intro_to_github/intro_to_github.html#slide6-id",
    "href": "presentations/intro_to_github/intro_to_github.html#slide6-id",
    "title": "Making reproducible projects",
    "section": "",
    "text": "This illustration is created by Scriberia with The Turing Way community.  Used under a CC-BY 4.0 licence. DOI: 10.5281/zenodo.3332807"
  },
  {
    "objectID": "presentations/intro_to_github/intro_to_github.html#slide7-id",
    "href": "presentations/intro_to_github/intro_to_github.html#slide7-id",
    "title": "Making reproducible projects",
    "section": "",
    "text": "Version control allows us to:\n\nAvoid confusing file names\nKeep track of changes made over time\nTinker with code without worrying about breaking it\nEasily revert when code does break\nIntegrate with other software for online back-ups"
  },
  {
    "objectID": "presentations/intro_to_github/intro_to_github.html#slide4-id",
    "href": "presentations/intro_to_github/intro_to_github.html#slide4-id",
    "title": "Making reproducible projects",
    "section": "",
    "text": "Installed locally\nFree version control system (often pre-installed)\nManages the evolution of files in a sensible, highly structured way\nStructured around repositories (aka a ‘repo’) as units of organisation\n\n\n\n\n\n\n \n \n\n\n\nCloud-based\nHosting service for git-based projects (others: BitBucket, GitLab)\nSimilar to DropBox/Google Docs but better\nAllows others to see, synchronise with and contribute to your work"
  },
  {
    "objectID": "presentations/intro_to_github/intro_to_github.html#the-gitgithub-workflow",
    "href": "presentations/intro_to_github/intro_to_github.html#the-gitgithub-workflow",
    "title": "Making reproducible projects",
    "section": "The git/GitHub workflow",
    "text": "The git/GitHub workflow\n\nSpecific flow of actions that are usually followed:\n\n\n\nPull\n\n\nDownload everything from GitHub for the repo*\n\n\n\n\nStage\n\n\nAdd modified files to the commit queue\n\n\n\n\n\n\n\nCommit\n\n\nConfirm your changes locally (with message)\n\n\n\n\n\n\n\nPush\n\n\nUpload committed changes to GitHub\n\n\n\n\n\n\n\n\n\n*Optional — but good practice to do when starting for the day"
  },
  {
    "objectID": "presentations/intro_to_github/intro_to_github.html#interacting-with-gitgithub",
    "href": "presentations/intro_to_github/intro_to_github.html#interacting-with-gitgithub",
    "title": "Making reproducible projects",
    "section": "Interacting with git/GitHub",
    "text": "Interacting with git/GitHub\n\n\n \n\n\n\n\n\n\nThe R Studio git pane will only appear when you activate a project in version control"
  },
  {
    "objectID": "presentations/intro_to_github/intro_to_github.html#the-gitgithub-workflow-rstudio",
    "href": "presentations/intro_to_github/intro_to_github.html#the-gitgithub-workflow-rstudio",
    "title": "Making reproducible projects",
    "section": "The git/GitHub workflow (RStudio)",
    "text": "The git/GitHub workflow (RStudio)\n\nSame flow:\n\n\n\n\nPull\n\n\n\n\n\n\n\nStage\n\n\n\n\n\n\n\n\n\n\nCommit\n\n\n\n\n\n\n\n\n\n\nPush"
  },
  {
    "objectID": "presentations/intro_to_github/intro_to_github.html#gitignore",
    "href": "presentations/intro_to_github/intro_to_github.html#gitignore",
    "title": "Making reproducible projects",
    "section": "       .gitignore",
    "text": ".gitignore\n\n\nText file that lists large/specific files you don’t want to sync\nExclude all files of one type with * wildcard (e.g., *.png)\n\n\n\nEdit the .gitignore file (left) or select files to exclude (right)"
  },
  {
    "objectID": "presentations/intro_to_github/intro_to_github.html#slide11-id",
    "href": "presentations/intro_to_github/intro_to_github.html#slide11-id",
    "title": "Making reproducible projects",
    "section": "",
    "text": "R Studio Projects"
  },
  {
    "objectID": "presentations/intro_to_github/intro_to_github.html#why-r-studio-projects-are-great",
    "href": "presentations/intro_to_github/intro_to_github.html#why-r-studio-projects-are-great",
    "title": "Making reproducible projects",
    "section": "       Why R Studio Projects are great:",
    "text": "Why R Studio Projects are great:\n\n\nEach Project file opens a new session and environment\nFile paths start relative to the .Rproj file (much shorter)\nImproves code reproducibility — even better if you use here\n\n\n\nSelf-contained project folder makes a perfect GitHub repo:\n\n\n\nGuide to using R Studio projects can be found on the Exeter Data Analytics intros page"
  },
  {
    "objectID": "presentations/intro_to_github/intro_to_github.html#how-to-create-an-r-studio-project",
    "href": "presentations/intro_to_github/intro_to_github.html#how-to-create-an-r-studio-project",
    "title": "Making reproducible projects",
    "section": "How to create an R Studio project",
    "text": "How to create an R Studio project\n\nGuide to using R Studio projects can be found on the Exeter Data Analytics intros page"
  },
  {
    "objectID": "presentations/intro_to_github/intro_to_github.html#create-project-by-cloning-a-github-repo",
    "href": "presentations/intro_to_github/intro_to_github.html#create-project-by-cloning-a-github-repo",
    "title": "Making reproducible projects",
    "section": "Create project by cloning a GitHub repo",
    "text": "Create project by cloning a GitHub repo\n\nNote: You can edit/save files from a cloned repo, but won’t be able to push unless you are the repo owner or a collaborator (details here)"
  },
  {
    "objectID": "presentations/intro_to_github/intro_to_github.html#sec-slide10id",
    "href": "presentations/intro_to_github/intro_to_github.html#sec-slide10id",
    "title": "Making reproducible projects",
    "section": "",
    "text": "{here}\n\n\n\n\n\nIllustration by Allison Horst"
  },
  {
    "objectID": "presentations/intro_to_github/intro_to_github.html#here-makes-filepaths-that",
    "href": "presentations/intro_to_github/intro_to_github.html#here-makes-filepaths-that",
    "title": "Making reproducible projects",
    "section": "{here} makes filepaths that:",
    "text": "{here} makes filepaths that:\n\nWork magically on both macOS & Windows (/ or \\)\nStart at the root of the repository (using .Rproj file)\n\n\nhere::here() will show the root of your project directory:\n\n#install.packages(\"here\")\nlibrary(here)\nhere()\n\n[1] \"/Users/dp415/Desktop/exeter_open_reproducibility_website\"\n\n\n\n\nThis path is included at the start of every filepath here creates\n\nRecommended reading: Project-oriented workflows by Jenny Bryant"
  },
  {
    "objectID": "presentations/intro_to_github/intro_to_github.html#using-here",
    "href": "presentations/intro_to_github/intro_to_github.html#using-here",
    "title": "Making reproducible projects",
    "section": "Using here:",
    "text": "Using here:\n\nFilepaths are created in a similar way to paste():\nWe list quoted names of folders, comma separated\n\n\nWe can create a test filepath to a folder within the repository:\n\nhere(\"data\",\"raw_data\") #creating a filepath to a folder within the repository\n\n[1] \"/Users/dp415/Desktop/exeter_open_reproducibility_website/data/raw_data\"\n\n\n\n\nCreate paths directly within a file/filename argument:\n\nread_csv(file = here(\"data\",\"raw_data\",\"data.csv\"))\nggsave(plot, filename = here(\"output\",\"figures\"))\n\n\n\nOr turn long paths into an object for regular use:\n\noutput_path &lt;- here(\"output\",\"movement\",\"figures\") ; output_path\n\n[1] \"/Users/dp415/Desktop/exeter_open_reproducibility_website/output/movement/figures\"\n\n\n\nggsave(plot, filename = here(output_path,\"plot1.png\"))"
  },
  {
    "objectID": "presentations/intro_to_github/intro_to_github.html#how-i-use-github",
    "href": "presentations/intro_to_github/intro_to_github.html#how-i-use-github",
    "title": "Making reproducible projects",
    "section": "How I use GitHub:",
    "text": "How I use GitHub:\n\n\nI usually start by making an empty repo on GitHub\nClone the empty repo to my machine as an R Studio Project\nCreate a few core folders (data, scripts, outputs)\nAdd relevant files and commit-push everything\nFire up a script and read in data with here\nUse pacman::p_load for loading packages\n…spend the next three hours faffing with ggplot"
  },
  {
    "objectID": "presentations/intro_to_github/intro_to_github.html#slide23-id",
    "href": "presentations/intro_to_github/intro_to_github.html#slide23-id",
    "title": "Making reproducible projects",
    "section": "",
    "text": "– Intermission –"
  },
  {
    "objectID": "presentations/intro_to_github/intro_to_github.html#slide24-id",
    "href": "presentations/intro_to_github/intro_to_github.html#slide24-id",
    "title": "Making reproducible projects",
    "section": "",
    "text": "Why you should share your data,  why you shouldn’t share it via GitHub,  and where you should share it instead.\n\n\nMatt Lloyd Jones\n\n\n\n\n\n\n\n\nImage (UK Reproducibility Network logo): UKRN via University of Portsmouth Twitter\nImage (Sad Mona): GitHub Twitter"
  },
  {
    "objectID": "presentations/intro_to_github/intro_to_github.html#talk-outline",
    "href": "presentations/intro_to_github/intro_to_github.html#talk-outline",
    "title": "Making reproducible projects",
    "section": "Talk outline",
    "text": "Talk outline\n\nWhy you should share your data\nWhy you shouldn’t share your data via GitHub\nWhere you should share your data instead\n\n*Assuming you are permitted share your data"
  },
  {
    "objectID": "presentations/intro_to_github/intro_to_github.html#talk-outline-1",
    "href": "presentations/intro_to_github/intro_to_github.html#talk-outline-1",
    "title": "Making reproducible projects",
    "section": "Talk outline",
    "text": "Talk outline\n\nWhy you should share your data\nWhy you shouldn’t share your data via GitHub\nWhere you should share your data instead\n\n*Assuming you are permitted share your data"
  },
  {
    "objectID": "presentations/intro_to_github/intro_to_github.html#you-will-soon-have-to-share-your-data-anyway.",
    "href": "presentations/intro_to_github/intro_to_github.html#you-will-soon-have-to-share-your-data-anyway.",
    "title": "Making reproducible projects",
    "section": "You will soon have to share your data anyway.",
    "text": "You will soon have to share your data anyway.\n\n\n\nConcordat on Open Research Data  (signed by HEFCE, UKRI, Universities UK,  the Wellcome Trust and more1.\nNational Institutes of Health (NIH) has required  its fundees to eventually make their data  publicly available (as of January 2023)2.\nUS Government moving towards a position  of making sharing data mandatory  where possible)3.\n\n\n\n\n\n\n1. Information of Concordat on Open research Data\n2. Nature article on NIH data sharing mandate\n3. White House’s August 2022 announcement on Data Sharing"
  },
  {
    "objectID": "presentations/intro_to_github/intro_to_github.html#like-sharing-code-sharing-data-improves-the-quality-of-your-science.",
    "href": "presentations/intro_to_github/intro_to_github.html#like-sharing-code-sharing-data-improves-the-quality-of-your-science.",
    "title": "Making reproducible projects",
    "section": "Like sharing code, sharing data improves the quality of your science.",
    "text": "Like sharing code, sharing data improves the quality of your science.\n\n\nIn the process of making your data  publication-ready, you will also  find yourself:\n\nFinding mistakes and correcting them\nMaking sure the data inputted and  outputted from your code is consistent\nImproving its documentation  (for future re-use - most likely by you!)\n\n\n\n\n\n\nImage source: CMSWire"
  },
  {
    "objectID": "presentations/intro_to_github/intro_to_github.html#your-code-wont-work-without-your-data.",
    "href": "presentations/intro_to_github/intro_to_github.html#your-code-wont-work-without-your-data.",
    "title": "Making reproducible projects",
    "section": "Your code won’t work without your data.",
    "text": "Your code won’t work without your data."
  },
  {
    "objectID": "presentations/intro_to_github/intro_to_github.html#talk-outline-2",
    "href": "presentations/intro_to_github/intro_to_github.html#talk-outline-2",
    "title": "Making reproducible projects",
    "section": "Talk outline",
    "text": "Talk outline\n\nWhy you should share your data\nWhy you shouldn’t share your data via GitHub\nWhere you should share your data instead"
  },
  {
    "objectID": "presentations/intro_to_github/intro_to_github.html#you-cannot-assign-a-doi-to-a-github-repository.",
    "href": "presentations/intro_to_github/intro_to_github.html#you-cannot-assign-a-doi-to-a-github-repository.",
    "title": "Making reproducible projects",
    "section": "You cannot assign a DOI to a GitHub repository.",
    "text": "You cannot assign a DOI to a GitHub repository.\n\n\nLike your publication, your data  should have a persistent identifier  like a Digital Object Identifier (DOI)\nHowever, you can’t DOI your  GitHub repo, or versions of it!\nFor this reason, GitHub cannot be  considered a FAIR (Findable, Accessible,  Interoperable and Reusable) data repository\n\n\n\n\n\nImage (DOI logo): Internatinal DOI Foundation via Wikimedia"
  },
  {
    "objectID": "presentations/intro_to_github/intro_to_github.html#memory-limits",
    "href": "presentations/intro_to_github/intro_to_github.html#memory-limits",
    "title": "Making reproducible projects",
    "section": "Memory limits",
    "text": "Memory limits\n\n\nSize of repository as a whole  cannot exceed 100 GB  (warnings &gt;75 GB and &gt;5GB)1,2\nSize of an individual push  (which may contain multiple  files) cannot exceed 2 GB1,2\nSize of each file in it cannot  exceed 100 MB (warnings &gt; 50MB)1\nIn order to prevent negatively  impacting GitHub’s infrastructure1\n\n\n\n\n\n\n\n\n\n1. Managing Large Files (GitHub)\n2. GitHub Repository Size limits discussion (Stack OverFlow)\nImage (Goldfish): The Economic Times"
  },
  {
    "objectID": "presentations/intro_to_github/intro_to_github.html#inconveniences-others",
    "href": "presentations/intro_to_github/intro_to_github.html#inconveniences-others",
    "title": "Making reproducible projects",
    "section": "Inconveniences others",
    "text": "Inconveniences others\n\n\nUsers who just want to play around  with your code are forced to download  all of your research data too  (potentially up to 100 GB!)\nSmaller repositories are faster to clone  and easier to work with\n\n\n\n\n\nImage (Atlas): Gordon Johnson via Pixabay"
  },
  {
    "objectID": "presentations/intro_to_github/intro_to_github.html#git-is-not-set-up-for-handling-data",
    "href": "presentations/intro_to_github/intro_to_github.html#git-is-not-set-up-for-handling-data",
    "title": "Making reproducible projects",
    "section": "git is not set up for handling data",
    "text": "git is not set up for handling data\n\n\ngit version control system is based  around code, not data1\ngit knows nothing about the structure of  common data formats we use  (e.g. the tabular structure of CSV files)2\nMay result in merge conflicts  emerging where there are none2\n\n\n\n\n\n\n\n1. “using Git for data is…like using a hammer to fasten a screw.” [Image source: Pixabay]\n2. Merge conflicts with CSV files in git\nImage: Masterfile 640-02951002"
  },
  {
    "objectID": "presentations/intro_to_github/intro_to_github.html#talk-outline-3",
    "href": "presentations/intro_to_github/intro_to_github.html#talk-outline-3",
    "title": "Making reproducible projects",
    "section": "Talk outline",
    "text": "Talk outline\n\nWhy you should share your data\nWhy you shouldn’t share your data via GitHub\nWhere you should share your data instead"
  },
  {
    "objectID": "presentations/intro_to_github/intro_to_github.html#rawness-of-data",
    "href": "presentations/intro_to_github/intro_to_github.html#rawness-of-data",
    "title": "Making reproducible projects",
    "section": "Rawness of data",
    "text": "Rawness of data\n\n\n\n\n\n\n\n\n\n\nImages 1 and 2: The Spruce Eats\nImage 3: Reno Gazette Journal"
  },
  {
    "objectID": "presentations/intro_to_github/intro_to_github.html#rawness-of-data-1",
    "href": "presentations/intro_to_github/intro_to_github.html#rawness-of-data-1",
    "title": "Making reproducible projects",
    "section": "Rawness of data",
    "text": "Rawness of data\n\n\n\n\n\n\n\n\n\n\nStatology: What Is considered raw data?"
  },
  {
    "objectID": "presentations/intro_to_github/intro_to_github.html#things-to-consider-when-choosing-somewhere-to-store-your-raw-data",
    "href": "presentations/intro_to_github/intro_to_github.html#things-to-consider-when-choosing-somewhere-to-store-your-raw-data",
    "title": "Making reproducible projects",
    "section": "Things to consider when choosing somewhere to store your raw data",
    "text": "Things to consider when choosing somewhere to store your raw data\n\n\nEasy to download/upload  data from/to via code\nLikely to stick around\nDOI-able\n\n\n\n\n\nImage: Chickens In The Road"
  },
  {
    "objectID": "presentations/intro_to_github/intro_to_github.html#example-open-science-framework",
    "href": "presentations/intro_to_github/intro_to_github.html#example-open-science-framework",
    "title": "Making reproducible projects",
    "section": "Example: Open Science framework",
    "text": "Example: Open Science framework\n\n\nEasy to download/upload  via the osfr package1\nHere to stay for the open  science revolution\nAllows you to assign DOIs  to projects and/or datasets\n\n\n\n\n\n\n\n\n1. ‘osfr’ package on GitHub\nImage: Open Science Framework via Data CC\nImage: osfr via ropensci"
  },
  {
    "objectID": "presentations/intro_to_github/intro_to_github.html#things-to-consider-when-choosing-somewhere-to-store-your-processed-data",
    "href": "presentations/intro_to_github/intro_to_github.html#things-to-consider-when-choosing-somewhere-to-store-your-processed-data",
    "title": "Making reproducible projects",
    "section": "Things to consider when choosing somewhere to store your processed data",
    "text": "Things to consider when choosing somewhere to store your processed data\n\nHigher memory limits\nCan store your final data in a  file structure (ideally alongside  the code that produced it)\nDOI-able\n\n\n\n\n\nImage: Wikimedia Commons"
  },
  {
    "objectID": "presentations/intro_to_github/intro_to_github.html#example-zenodo",
    "href": "presentations/intro_to_github/intro_to_github.html#example-zenodo",
    "title": "Making reproducible projects",
    "section": "Example: Zenodo",
    "text": "Example: Zenodo\n\n\n50GB file size limit1\nYou can just zip up your local version  of your Github repository (with both  code and data) at the end of running  all your code/analysis, and upload it\nAllows you to assign a DOI to the  repository as a whole, as well as  to different versions of that repository  as it evolves through time  (and peer review)\nCan also ‘reserve’ a DOI which is  really handy when writing a manuscript \n\n\n\n\n\n\n\n\n\n1. Zenodo FAQs\nImage (Zenodo logo): Zenodo\nImage (Zenodo versioning): GitHub"
  },
  {
    "objectID": "presentations/intro_to_github/intro_to_github.html#zenodo-example-expanded",
    "href": "presentations/intro_to_github/intro_to_github.html#zenodo-example-expanded",
    "title": "Making reproducible projects",
    "section": "Zenodo example expanded",
    "text": "Zenodo example expanded\n\nWould recommend NOT using the automated GitHub integration, because:\n\nManual option allows you to reserve a DOI for use in submitted manuscripts (before making the dataset public)\nThis only allows you to archive your code (since you’re not storing your data on GitHub anymore, right?)\nBy zipping up and uploading the final, populated repository from your local machine, you can upload data and code together\n\n\n\n\n\n\n\n\n\nImage: Code Refinery"
  },
  {
    "objectID": "presentations/intro_to_github/intro_to_github.html#storing-data-outside-of-github-but-in-a-repositories-friendly-to-github-keeps-everyone-happy",
    "href": "presentations/intro_to_github/intro_to_github.html#storing-data-outside-of-github-but-in-a-repositories-friendly-to-github-keeps-everyone-happy",
    "title": "Making reproducible projects",
    "section": "Storing data outside of GitHub, but in a repositories friendly to GitHub keeps everyone happy!",
    "text": "Storing data outside of GitHub, but in a repositories friendly to GitHub keeps everyone happy!\n\n\n\n\n\n\n\n\n\n\nImage (Sad Mona): GitHub Twitter\nImage (Happy Mona): GitHub Twitter"
  },
  {
    "objectID": "presentations/intro_to_github/intro_to_github.html#final-considerations",
    "href": "presentations/intro_to_github/intro_to_github.html#final-considerations",
    "title": "Making reproducible projects",
    "section": "Final considerations",
    "text": "Final considerations\n\n\nFollowing this schema, raw data will be archived on both OSF and Zenodo - but duplication is good in terms of data preservation\nOSF and Zenodo are generic repositories, but sometimes a more structured, subject-specific repository is required (e.g. NCBI or ENA for sequence data)\nYou should consider whether your institution/funder/etc require you to also upload and store the data elsewhere\nYou should consider whether you are allowed to share all of the data and/or whether you need to anonymise it (particularly raw data)\n\nYou can prevent data from being uploaded (‘pushed’) to your GitHub repository alongside changes to code by storing it in data folders (e.g. ‘raw’ and ‘processed’ folders) and including these in your .gitignore file"
  },
  {
    "objectID": "presentations/intro_to_github/intro_to_github.html#practical-time",
    "href": "presentations/intro_to_github/intro_to_github.html#practical-time",
    "title": "Making reproducible projects",
    "section": "Practical time!",
    "text": "Practical time!\nHow to spend the rest of the time:\n\nClone any GitHub repo (⭐️️)\nRepos you could clone: CoppeR, aRtist\nCreating a new empty GitHub repo from scratch (️⭐⭐️)\nResources: happygitwithr - new GitHub project\nTurn an existing project into a GitHub repo (⭐️⭐️⭐️)\nResources: (happygitwithr — GitHub first, GitHub last)\n\n\nFeel free to ask us about your pesky code problems!"
  },
  {
    "objectID": "presentations/intro_to_github/intro_to_github.html#resources",
    "href": "presentations/intro_to_github/intro_to_github.html#resources",
    "title": "Making reproducible projects",
    "section": "Resources",
    "text": "Resources\n\nExeDataHub: Installing, R, RStudio, and Git\nExeDataHub: Managing research projects with R Studio\nThe exceptional Happy Git and GitHub for the useR book\nOthers?"
  },
  {
    "objectID": "presentations/intro_to_github/intro_to_github.html#acknowledgements",
    "href": "presentations/intro_to_github/intro_to_github.html#acknowledgements",
    "title": "Making reproducible projects",
    "section": "Acknowledgements",
    "text": "Acknowledgements\n\nUKRN for some original funding\nVarious resources made by others"
  },
  {
    "objectID": "series.html",
    "href": "series.html",
    "title": "Open and Reproducible Science Series",
    "section": "",
    "text": "We are excited to host a new monthly series of workshops and talks to promote Open and Reproducible Science at the University of Exeter Penryn campus.\nThis series will include invited speakers from both within and outside the department, covering the life-cycle of a research project including Pre-registration, Project management, Reproducibility, ChatGPT (and AI in general), and more.\n\nSchedule\nWhere: Exeter University Penryn campus\nWhen: Tuesdays from 3:00-5:00pm, once a month\nFeed me?: yes, there will be tea, coffee, and snacks\n\n\n\nDate\nLocation\nTopic\n\n\n\n\n21st November\nExchange Lecture Theatre\nTidyverse data wrangling\n\n\n\nScheduled talks subject to change as the series evolves. Please check this website and look out for calendar invites for confirmed bookings and locations.\n\n\nPrevious workshops\n\n\n\nDate\nSpeakers\nTopic\nResources\n\n\n\n\n28th March 2023\nMark Kelson & Erik Postma\nReproducible and Open Science Symposium\nSlides | Talk recording: part1, part2\n\n\n9th May 2023\nMark Hanson\nThe ghosts of journals past, present, and future\nTalk recording (accessible with University of Exeter account only)\n\n\n20th June 2023\nExDataHub\nMaking reproducible projects\nSlides | Talk recording (open access) | intRo to RStudio Projects & git tools\n\n\n11th July 2023\nLotty Brand & Matt Jones\nAn introduction to open science and preregistration\n\n\n\n1st August 2023\nDan Sankey\nLet’s build R-Shiny web apps. For science!\n\n\n\n12th October 2023\nDaniel Padfield\nReproducible publication-ready plots and tables with ggplot2 and flextable.\nSlides | Talk recording (accessible with University of Exeter account only)\n\n\n\n\n\nGet Involved\nWe welcome all levels of involvement in the series. If you would like to contribute a talk, workshop, or conversation topic on any subject linked to open and reproducible science, please get in touch with us by email or come and speak to us at an event.\nContact: exedatahub@exeter.ac.uk"
  },
  {
    "objectID": "posts/intro_to_tidy_data_manipulation/index.html",
    "href": "posts/intro_to_tidy_data_manipulation/index.html",
    "title": "intRos: Tidy data manipulation",
    "section": "",
    "text": "Outline\nThe way we collect or download data can often be different to how we analyse them. For example, have you ever needed to work on a subset of your data, add new columns, or calculate summary values? There are good reasons to do this kind of data manipulation in a reproducible way: it’s less prone to errors, saves us time when repeating the process, and helps us to share our code with confidence. In this walkthrough, we’ll share some of our favourite tools for reproducible data manipulation in R and the tidyverse.\n\n\nPrerequisites\n\nHave R installed\nBe using R for your research\n\n\n\nLearning objectives\n\n\n\n\n\n\nLearning objectives\n\n\n\nThis introduction should allow you to:\n\nChange the structure of your data using reproducible code\nKnow where to learn more about tidyverse data manipulation\n\n\n\n\n\nTidy data manipulation\n\nWhy manipulate data in R?\n\nReproducible\nEasy to update and re-use with new data\nLess prone to human error\nCan be quick, easy, and fun…\n\nHere’s an example of how we can use the tidyverse packages (& janitor) to tidy, transform, and visualise the palmer penguins data:\n\nPenguin morphometrics\n\n\nCode\n# load the tidyverse, and other useful packages\npacman::p_load(flextable, palmerpenguins, \n               janitor, magrittr, tidyverse) \n\nvars_measurements &lt;- c(\"culmen_length_mm\", \"culmen_depth_mm\", \"flipper_length_mm\", \"body_mass_g\")\n\n# use pipe operative from magrittr\npenguins_example &lt;- penguins_raw %&gt;%\n  # column names to snake_case using janitor package\n  janitor::clean_names() %&gt;% \n  # rename columns using dplyr\n  rename(carbon = delta_13_c_o_oo,\n         nitrogen = delta_15_n_o_oo) %&gt;%\n  # add new columns using dplyr\n  mutate(\n    # extract year using lubridate\n    year = lubridate::year(date_egg), \n    # change case in sex column using stringr\n    sex = str_to_lower(sex), \n    # take first element of species column string using stringr and purrr\n    species = str_split_i(species, pattern = \" \", i = 1)) %&gt;% \n  # remove nas in sex column using dplyr\n  filter(!is.na(sex))\n\n# plot data using ggplot\nggplot(penguins_example, aes(x = flipper_length_mm, y = body_mass_g, \n                             # reorder sex using forcats\n                             col = fct_reorder2(sex, flipper_length_mm, body_mass_g))) +\n  facet_grid(cols = vars(species), scales = \"free\")+\n  geom_point()+\n  geom_smooth(method = lm)+\n  scale_colour_viridis_d(option = \"magma\", begin = 0.7, end = 0.3, name = \"Sex\")+\n  theme_minimal()+\n  theme(panel.border = element_rect(fill = NA))\n\n\n\n\n\nSexual dimporphism in Palmer penguins\n\n\n\n\n\n\nPenguin isotope signatures\n\n\nCode\npenguins_summary_isotopes &lt;- penguins_example %&gt;% \n  # change data format to long using tidyr\n  pivot_longer(cols = carbon:nitrogen, names_to = \"isotope\", values_to = \"value\") %&gt;%\n  # group data using dplyr\n  group_by(species, isotope) %&gt;% \n  # summarise data for each group using dplyr\n  summarize(mean = mean(value, na.rm = T),\n            sd =  sd(value, na.rm = T)) %&gt;%\n  # change data format to wide using tidyr\n  pivot_wider(id_cols = species, names_from = isotope, values_from=c(mean, sd))\n\nggplot(penguins_example, aes(x = carbon, y = nitrogen, col = species)) +\n  geom_point(alpha = 0.6)+\n  geom_errorbar(data = penguins_summary_isotopes, \n                aes(x = mean_carbon, \n                    ymax = mean_nitrogen+sd_nitrogen, ymin = mean_nitrogen-sd_nitrogen, col = species),\n                inherit.aes = F, width = 0.1)+\n  geom_errorbar(data = penguins_summary_isotopes, \n                aes(y = mean_nitrogen, \n                    xmax = mean_carbon+sd_carbon, xmin = mean_carbon-sd_carbon, col = species), \n                inherit.aes = F, width = 0.1)+\n  scale_colour_viridis_d(option = \"mako\", begin = 0.75, end = 0.1, name = \"Species\")+\n  theme_minimal()+\n  theme(panel.border = element_rect(fill = NA))\n\n\n\n\n\nIsotope signatures among Palmer penguins\n\n\n\n\n\n\n\nSome useful functions\nHere, we’ll dig into the functions used in the above example to see how we processed the data:\n\njanitor::clean_names()\nLet’s remind ourselves of the original data:\n\nglimpse(penguins_raw)\n\nRows: 344\nColumns: 17\n$ studyName             &lt;chr&gt; \"PAL0708\", \"PAL0708\", \"PAL0708\", \"PAL0708\", \"PAL…\n$ `Sample Number`       &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1…\n$ Species               &lt;chr&gt; \"Adelie Penguin (Pygoscelis adeliae)\", \"Adelie P…\n$ Region                &lt;chr&gt; \"Anvers\", \"Anvers\", \"Anvers\", \"Anvers\", \"Anvers\"…\n$ Island                &lt;chr&gt; \"Torgersen\", \"Torgersen\", \"Torgersen\", \"Torgerse…\n$ Stage                 &lt;chr&gt; \"Adult, 1 Egg Stage\", \"Adult, 1 Egg Stage\", \"Adu…\n$ `Individual ID`       &lt;chr&gt; \"N1A1\", \"N1A2\", \"N2A1\", \"N2A2\", \"N3A1\", \"N3A2\", …\n$ `Clutch Completion`   &lt;chr&gt; \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"No\", …\n$ `Date Egg`            &lt;date&gt; 2007-11-11, 2007-11-11, 2007-11-16, 2007-11-16,…\n$ `Culmen Length (mm)`  &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34…\n$ `Culmen Depth (mm)`   &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18…\n$ `Flipper Length (mm)` &lt;dbl&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190,…\n$ `Body Mass (g)`       &lt;dbl&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 34…\n$ Sex                   &lt;chr&gt; \"MALE\", \"FEMALE\", \"FEMALE\", NA, \"FEMALE\", \"MALE\"…\n$ `Delta 15 N (o/oo)`   &lt;dbl&gt; NA, 8.94956, 8.36821, NA, 8.76651, 8.66496, 9.18…\n$ `Delta 13 C (o/oo)`   &lt;dbl&gt; NA, -24.69454, -25.33302, NA, -25.32426, -25.298…\n$ Comments              &lt;chr&gt; \"Not enough blood for isotopes.\", NA, NA, \"Adult…\n\n\nAnd now tidy up our column names into snake_case. (This isn’t part of the tidyverse, but it is too useful not to mention)\n\npenguins_example &lt;- penguins_raw %&gt;%\n  janitor::clean_names() \n\nglimpse(penguins_example)\n\nRows: 344\nColumns: 17\n$ study_name        &lt;chr&gt; \"PAL0708\", \"PAL0708\", \"PAL0708\", \"PAL0708\", \"PAL0708…\n$ sample_number     &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1…\n$ species           &lt;chr&gt; \"Adelie Penguin (Pygoscelis adeliae)\", \"Adelie Pengu…\n$ region            &lt;chr&gt; \"Anvers\", \"Anvers\", \"Anvers\", \"Anvers\", \"Anvers\", \"A…\n$ island            &lt;chr&gt; \"Torgersen\", \"Torgersen\", \"Torgersen\", \"Torgersen\", …\n$ stage             &lt;chr&gt; \"Adult, 1 Egg Stage\", \"Adult, 1 Egg Stage\", \"Adult, …\n$ individual_id     &lt;chr&gt; \"N1A1\", \"N1A2\", \"N2A1\", \"N2A2\", \"N3A1\", \"N3A2\", \"N4A…\n$ clutch_completion &lt;chr&gt; \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"No\"…\n$ date_egg          &lt;date&gt; 2007-11-11, 2007-11-11, 2007-11-16, 2007-11-16, 200…\n$ culmen_length_mm  &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ culmen_depth_mm   &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;dbl&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;dbl&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;chr&gt; \"MALE\", \"FEMALE\", \"FEMALE\", NA, \"FEMALE\", \"MALE\", \"F…\n$ delta_15_n_o_oo   &lt;dbl&gt; NA, 8.94956, 8.36821, NA, 8.76651, 8.66496, 9.18718,…\n$ delta_13_c_o_oo   &lt;dbl&gt; NA, -24.69454, -25.33302, NA, -25.32426, -25.29805, …\n$ comments          &lt;chr&gt; \"Not enough blood for isotopes.\", NA, NA, \"Adult not…\n\n\n\n\ndplyr::rename()\nTo rename a column\n\npenguins_example %&lt;&gt;%\n  dplyr::rename(carbon = delta_13_c_o_oo,\n         nitrogen = delta_15_n_o_oo) \n\nglimpse(penguins_example)\n\nRows: 344\nColumns: 17\n$ study_name        &lt;chr&gt; \"PAL0708\", \"PAL0708\", \"PAL0708\", \"PAL0708\", \"PAL0708…\n$ sample_number     &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1…\n$ species           &lt;chr&gt; \"Adelie Penguin (Pygoscelis adeliae)\", \"Adelie Pengu…\n$ region            &lt;chr&gt; \"Anvers\", \"Anvers\", \"Anvers\", \"Anvers\", \"Anvers\", \"A…\n$ island            &lt;chr&gt; \"Torgersen\", \"Torgersen\", \"Torgersen\", \"Torgersen\", …\n$ stage             &lt;chr&gt; \"Adult, 1 Egg Stage\", \"Adult, 1 Egg Stage\", \"Adult, …\n$ individual_id     &lt;chr&gt; \"N1A1\", \"N1A2\", \"N2A1\", \"N2A2\", \"N3A1\", \"N3A2\", \"N4A…\n$ clutch_completion &lt;chr&gt; \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"No\"…\n$ date_egg          &lt;date&gt; 2007-11-11, 2007-11-11, 2007-11-16, 2007-11-16, 200…\n$ culmen_length_mm  &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ culmen_depth_mm   &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;dbl&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;dbl&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;chr&gt; \"MALE\", \"FEMALE\", \"FEMALE\", NA, \"FEMALE\", \"MALE\", \"F…\n$ nitrogen          &lt;dbl&gt; NA, 8.94956, 8.36821, NA, 8.76651, 8.66496, 9.18718,…\n$ carbon            &lt;dbl&gt; NA, -24.69454, -25.33302, NA, -25.32426, -25.29805, …\n$ comments          &lt;chr&gt; \"Not enough blood for isotopes.\", NA, NA, \"Adult not…\n\n\n\n\ndplyr::mutate()\nTo add a column\n\npenguins_example %&lt;&gt;%\n  mutate(year = 2007)\n\nglimpse(penguins_example)\n\nRows: 344\nColumns: 18\n$ study_name        &lt;chr&gt; \"PAL0708\", \"PAL0708\", \"PAL0708\", \"PAL0708\", \"PAL0708…\n$ sample_number     &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1…\n$ species           &lt;chr&gt; \"Adelie Penguin (Pygoscelis adeliae)\", \"Adelie Pengu…\n$ region            &lt;chr&gt; \"Anvers\", \"Anvers\", \"Anvers\", \"Anvers\", \"Anvers\", \"A…\n$ island            &lt;chr&gt; \"Torgersen\", \"Torgersen\", \"Torgersen\", \"Torgersen\", …\n$ stage             &lt;chr&gt; \"Adult, 1 Egg Stage\", \"Adult, 1 Egg Stage\", \"Adult, …\n$ individual_id     &lt;chr&gt; \"N1A1\", \"N1A2\", \"N2A1\", \"N2A2\", \"N3A1\", \"N3A2\", \"N4A…\n$ clutch_completion &lt;chr&gt; \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"No\"…\n$ date_egg          &lt;date&gt; 2007-11-11, 2007-11-11, 2007-11-16, 2007-11-16, 200…\n$ culmen_length_mm  &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ culmen_depth_mm   &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;dbl&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;dbl&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;chr&gt; \"MALE\", \"FEMALE\", \"FEMALE\", NA, \"FEMALE\", \"MALE\", \"F…\n$ nitrogen          &lt;dbl&gt; NA, 8.94956, 8.36821, NA, 8.76651, 8.66496, 9.18718,…\n$ carbon            &lt;dbl&gt; NA, -24.69454, -25.33302, NA, -25.32426, -25.29805, …\n$ comments          &lt;chr&gt; \"Not enough blood for isotopes.\", NA, NA, \"Adult not…\n$ year              &lt;dbl&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\n\n\ndplyr::mutate()\nTo add a column\n\npenguins_example %&lt;&gt;%\n  mutate(year = \"unknown\")\n\nglimpse(penguins_example)\n\nRows: 344\nColumns: 18\n$ study_name        &lt;chr&gt; \"PAL0708\", \"PAL0708\", \"PAL0708\", \"PAL0708\", \"PAL0708…\n$ sample_number     &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1…\n$ species           &lt;chr&gt; \"Adelie Penguin (Pygoscelis adeliae)\", \"Adelie Pengu…\n$ region            &lt;chr&gt; \"Anvers\", \"Anvers\", \"Anvers\", \"Anvers\", \"Anvers\", \"A…\n$ island            &lt;chr&gt; \"Torgersen\", \"Torgersen\", \"Torgersen\", \"Torgersen\", …\n$ stage             &lt;chr&gt; \"Adult, 1 Egg Stage\", \"Adult, 1 Egg Stage\", \"Adult, …\n$ individual_id     &lt;chr&gt; \"N1A1\", \"N1A2\", \"N2A1\", \"N2A2\", \"N3A1\", \"N3A2\", \"N4A…\n$ clutch_completion &lt;chr&gt; \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"No\"…\n$ date_egg          &lt;date&gt; 2007-11-11, 2007-11-11, 2007-11-16, 2007-11-16, 200…\n$ culmen_length_mm  &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ culmen_depth_mm   &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;dbl&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;dbl&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;chr&gt; \"MALE\", \"FEMALE\", \"FEMALE\", NA, \"FEMALE\", \"MALE\", \"F…\n$ nitrogen          &lt;dbl&gt; NA, 8.94956, 8.36821, NA, 8.76651, 8.66496, 9.18718,…\n$ carbon            &lt;dbl&gt; NA, -24.69454, -25.33302, NA, -25.32426, -25.29805, …\n$ comments          &lt;chr&gt; \"Not enough blood for isotopes.\", NA, NA, \"Adult not…\n$ year              &lt;chr&gt; \"unknown\", \"unknown\", \"unknown\", \"unknown\", \"unknown…\n\n\n\n\nlubridate::year()\nLubridate is great for working with dates and times. Here, we want to extract year from a column of date\n\npenguins_example %&lt;&gt;%\n  mutate(year = lubridate::year(date_egg))\n\nglimpse(penguins_example)\n\nRows: 344\nColumns: 18\n$ study_name        &lt;chr&gt; \"PAL0708\", \"PAL0708\", \"PAL0708\", \"PAL0708\", \"PAL0708…\n$ sample_number     &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1…\n$ species           &lt;chr&gt; \"Adelie Penguin (Pygoscelis adeliae)\", \"Adelie Pengu…\n$ region            &lt;chr&gt; \"Anvers\", \"Anvers\", \"Anvers\", \"Anvers\", \"Anvers\", \"A…\n$ island            &lt;chr&gt; \"Torgersen\", \"Torgersen\", \"Torgersen\", \"Torgersen\", …\n$ stage             &lt;chr&gt; \"Adult, 1 Egg Stage\", \"Adult, 1 Egg Stage\", \"Adult, …\n$ individual_id     &lt;chr&gt; \"N1A1\", \"N1A2\", \"N2A1\", \"N2A2\", \"N3A1\", \"N3A2\", \"N4A…\n$ clutch_completion &lt;chr&gt; \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"No\"…\n$ date_egg          &lt;date&gt; 2007-11-11, 2007-11-11, 2007-11-16, 2007-11-16, 200…\n$ culmen_length_mm  &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ culmen_depth_mm   &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;dbl&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;dbl&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;chr&gt; \"MALE\", \"FEMALE\", \"FEMALE\", NA, \"FEMALE\", \"MALE\", \"F…\n$ nitrogen          &lt;dbl&gt; NA, 8.94956, 8.36821, NA, 8.76651, 8.66496, 9.18718,…\n$ carbon            &lt;dbl&gt; NA, -24.69454, -25.33302, NA, -25.32426, -25.29805, …\n$ comments          &lt;chr&gt; \"Not enough blood for isotopes.\", NA, NA, \"Adult not…\n$ year              &lt;dbl&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\n\n\nstringr::str_to_lower()\nChange the contents of a column to lower case\n\npenguins_example %&lt;&gt;%\n  mutate(sex = str_to_lower(sex))\n\nglimpse(penguins_example)\n\nRows: 344\nColumns: 18\n$ study_name        &lt;chr&gt; \"PAL0708\", \"PAL0708\", \"PAL0708\", \"PAL0708\", \"PAL0708…\n$ sample_number     &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1…\n$ species           &lt;chr&gt; \"Adelie Penguin (Pygoscelis adeliae)\", \"Adelie Pengu…\n$ region            &lt;chr&gt; \"Anvers\", \"Anvers\", \"Anvers\", \"Anvers\", \"Anvers\", \"A…\n$ island            &lt;chr&gt; \"Torgersen\", \"Torgersen\", \"Torgersen\", \"Torgersen\", …\n$ stage             &lt;chr&gt; \"Adult, 1 Egg Stage\", \"Adult, 1 Egg Stage\", \"Adult, …\n$ individual_id     &lt;chr&gt; \"N1A1\", \"N1A2\", \"N2A1\", \"N2A2\", \"N3A1\", \"N3A2\", \"N4A…\n$ clutch_completion &lt;chr&gt; \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"No\"…\n$ date_egg          &lt;date&gt; 2007-11-11, 2007-11-11, 2007-11-16, 2007-11-16, 200…\n$ culmen_length_mm  &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ culmen_depth_mm   &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;dbl&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;dbl&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;chr&gt; \"male\", \"female\", \"female\", NA, \"female\", \"male\", \"f…\n$ nitrogen          &lt;dbl&gt; NA, 8.94956, 8.36821, NA, 8.76651, 8.66496, 9.18718,…\n$ carbon            &lt;dbl&gt; NA, -24.69454, -25.33302, NA, -25.32426, -25.29805, …\n$ comments          &lt;chr&gt; \"Not enough blood for isotopes.\", NA, NA, \"Adult not…\n$ year              &lt;dbl&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\n\n\ndplyr::select()\nKeep certain columns\n\npenguins_tiny &lt;- penguins_example %&gt;%\n  select(individual_id, sex, body_mass_g)\n\nglimpse(penguins_tiny)\n\nRows: 344\nColumns: 3\n$ individual_id &lt;chr&gt; \"N1A1\", \"N1A2\", \"N2A1\", \"N2A2\", \"N3A1\", \"N3A2\", \"N4A1\", …\n$ sex           &lt;chr&gt; \"male\", \"female\", \"female\", NA, \"female\", \"male\", \"femal…\n$ body_mass_g   &lt;dbl&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, 4250…\n\n\nRemove certain columns\n\npenguins_tiny2 &lt;- penguins_example %&gt;%\n  select(!c(study_name, sample_number, region, comments))\n\nglimpse(penguins_tiny2)\n\nRows: 344\nColumns: 14\n$ species           &lt;chr&gt; \"Adelie Penguin (Pygoscelis adeliae)\", \"Adelie Pengu…\n$ island            &lt;chr&gt; \"Torgersen\", \"Torgersen\", \"Torgersen\", \"Torgersen\", …\n$ stage             &lt;chr&gt; \"Adult, 1 Egg Stage\", \"Adult, 1 Egg Stage\", \"Adult, …\n$ individual_id     &lt;chr&gt; \"N1A1\", \"N1A2\", \"N2A1\", \"N2A2\", \"N3A1\", \"N3A2\", \"N4A…\n$ clutch_completion &lt;chr&gt; \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"No\"…\n$ date_egg          &lt;date&gt; 2007-11-11, 2007-11-11, 2007-11-16, 2007-11-16, 200…\n$ culmen_length_mm  &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ culmen_depth_mm   &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;dbl&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;dbl&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;chr&gt; \"male\", \"female\", \"female\", NA, \"female\", \"male\", \"f…\n$ nitrogen          &lt;dbl&gt; NA, 8.94956, 8.36821, NA, 8.76651, 8.66496, 9.18718,…\n$ carbon            &lt;dbl&gt; NA, -24.69454, -25.33302, NA, -25.32426, -25.29805, …\n$ year              &lt;dbl&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\n\n\ndplyr::filter()\nFilter the rows of a dataframe by different criteria\n\n\n\n\n\nindividual_id\nsex\nbody_mass_g\n\n\n\n\nN1A1\nmale\n3750\n\n\nN1A2\nfemale\n3800\n\n\nN2A1\nfemale\n3250\n\n\nN2A2\nNA\nNA\n\n\nN3A1\nfemale\n3450\n\n\nN3A2\nmale\n3650\n\n\nN4A1\nfemale\n3625\n\n\nN4A2\nmale\n4675\n\n\nN5A1\nNA\n3475\n\n\nN5A2\nNA\n4250\n\n\n\n\n\n\npenguins_tiny %&lt;&gt;%\n  filter(!is.na(sex))\n\n\n\n\n\n\nindividual_id\nsex\nbody_mass_g\n\n\n\n\nN1A1\nmale\n3750\n\n\nN1A2\nfemale\n3800\n\n\nN2A1\nfemale\n3250\n\n\nN3A1\nfemale\n3450\n\n\nN3A2\nmale\n3650\n\n\nN4A1\nfemale\n3625\n\n\nN4A2\nmale\n4675\n\n\n\n\n\n\n\ntidyr::pivot_longer()\nTransform data from wide to long (each observation in a single row)\n\npenguins_isotopes_tiny &lt;- penguins_example %&gt;% \n  select(species, individual_id, carbon, nitrogen)\n\n\n\n\n\n\nspecies\nindividual_id\ncarbon\nnitrogen\n\n\n\n\nAdelie\nN1A2\n-24.69454\n8.94956\n\n\nAdelie\nN2A1\n-25.33302\n8.36821\n\n\nChinstrap\nN61A1\n-24.30229\n9.03935\n\n\nChinstrap\nN61A2\n-24.23592\n8.92069\n\n\nGentoo\nN31A1\n-25.51390\n7.99300\n\n\nGentoo\nN31A2\n-25.39369\n8.14756\n\n\n\n\n\n\npenguins_isotopes_tiny %&lt;&gt;% \n  pivot_longer(cols = carbon:nitrogen, \n               names_to = \"isotope\", \n               values_to = \"value\") \n\n\n\ndplyr::group_by() & dplyr::summarise()\nGroup by organises the data into groups, based on column values. This is useful for performing functions on each group, or summarising the data to calculate summary statistics\n\n\n\n\n\nspecies\nindividual_id\nisotope\nvalue\n\n\n\n\nAdelie\nN1A2\ncarbon\n-24.69454\n\n\nAdelie\nN1A2\nnitrogen\n8.94956\n\n\nAdelie\nN2A1\ncarbon\n-25.33302\n\n\nAdelie\nN2A1\nnitrogen\n8.36821\n\n\nChinstrap\nN61A1\ncarbon\n-24.30229\n\n\nChinstrap\nN61A1\nnitrogen\n9.03935\n\n\nChinstrap\nN61A2\ncarbon\n-24.23592\n\n\nChinstrap\nN61A2\nnitrogen\n8.92069\n\n\nGentoo\nN31A1\ncarbon\n-25.51390\n\n\nGentoo\nN31A1\nnitrogen\n7.99300\n\n\nGentoo\nN31A2\ncarbon\n-25.39369\n\n\nGentoo\nN31A2\nnitrogen\n8.14756\n\n\n\n\n\n\npenguins_isotopes_tiny %&lt;&gt;%\n  group_by(species, isotope) %&gt;% \n  summarize(mean = mean(value),\n            sd =  sd(value)) \n\n\n\n\n\n\nspecies\nisotope\nmean\nsd\n\n\n\n\nAdelie\ncarbon\n-25.013780\n0.4514735\n\n\nAdelie\nnitrogen\n8.658885\n0.4110765\n\n\nChinstrap\ncarbon\n-24.269105\n0.0469307\n\n\nChinstrap\nnitrogen\n8.980020\n0.0839053\n\n\nGentoo\ncarbon\n-25.453795\n0.0850013\n\n\nGentoo\nnitrogen\n8.070280\n0.1092904\n\n\n\n\n\n\n\ntidyr::pivot_wider()\nTransform data from long to wide (observations across multiple columns)\n\npenguins_isotopes_tiny %&lt;&gt;%\n  pivot_wider(id_cols = species, \n              names_from = isotope, \n              values_from=c(mean, sd)) \n\n\n\n\n\n\nspecies\nmean_carbon\nmean_nitrogen\nsd_carbon\nsd_nitrogen\n\n\n\n\nAdelie\n-25.01378\n8.658885\n0.4514735\n0.4110765\n\n\nChinstrap\n-24.26910\n8.980020\n0.0469307\n0.0839053\n\n\nGentoo\n-25.45379\n8.070280\n0.0850013\n0.1092904\n\n\n\n\n\n\n\ndplyr joins\nThere are lots of different ways to join multiple dataframes, using shared columns (e.g., an ID column). Here’s an example using left_join(), which keeps all values in the left hand dataframe - in this case penguins_example.\n\npenguins_example %&lt;&gt;%\n  left_join(penguins_summary_isotopes,\n            by = join_by(species))\n\nglimpse(penguins_example)\n\nRows: 333\nColumns: 22\n$ study_name        &lt;chr&gt; \"PAL0708\", \"PAL0708\", \"PAL0708\", \"PAL0708\", \"PAL0708…\n$ sample_number     &lt;dbl&gt; 1, 2, 3, 5, 6, 7, 8, 13, 14, 15, 16, 17, 18, 19, 20,…\n$ species           &lt;chr&gt; \"Adelie\", \"Adelie\", \"Adelie\", \"Adelie\", \"Adelie\", \"A…\n$ region            &lt;chr&gt; \"Anvers\", \"Anvers\", \"Anvers\", \"Anvers\", \"Anvers\", \"A…\n$ island            &lt;chr&gt; \"Torgersen\", \"Torgersen\", \"Torgersen\", \"Torgersen\", …\n$ stage             &lt;chr&gt; \"Adult, 1 Egg Stage\", \"Adult, 1 Egg Stage\", \"Adult, …\n$ individual_id     &lt;chr&gt; \"N1A1\", \"N1A2\", \"N2A1\", \"N3A1\", \"N3A2\", \"N4A1\", \"N4A…\n$ clutch_completion &lt;chr&gt; \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"No\", \"Yes\"…\n$ date_egg          &lt;date&gt; 2007-11-11, 2007-11-11, 2007-11-16, 2007-11-16, 200…\n$ culmen_length_mm  &lt;dbl&gt; 39.1, 39.5, 40.3, 36.7, 39.3, 38.9, 39.2, 41.1, 38.6…\n$ culmen_depth_mm   &lt;dbl&gt; 18.7, 17.4, 18.0, 19.3, 20.6, 17.8, 19.6, 17.6, 21.2…\n$ flipper_length_mm &lt;dbl&gt; 181, 186, 195, 193, 190, 181, 195, 182, 191, 198, 18…\n$ body_mass_g       &lt;dbl&gt; 3750, 3800, 3250, 3450, 3650, 3625, 4675, 3200, 3800…\n$ sex               &lt;chr&gt; \"male\", \"female\", \"female\", \"female\", \"male\", \"femal…\n$ nitrogen          &lt;dbl&gt; NA, 8.94956, 8.36821, 8.76651, 8.66496, 9.18718, 9.4…\n$ carbon            &lt;dbl&gt; NA, -24.69454, -25.33302, -25.32426, -25.29805, -25.…\n$ comments          &lt;chr&gt; \"Not enough blood for isotopes.\", NA, NA, NA, NA, \"N…\n$ year              &lt;dbl&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n$ mean_carbon       &lt;dbl&gt; -25.81356, -25.81356, -25.81356, -25.81356, -25.8135…\n$ mean_nitrogen     &lt;dbl&gt; 8.859398, 8.859398, 8.859398, 8.859398, 8.859398, 8.…\n$ sd_carbon         &lt;dbl&gt; 0.5871106, 0.5871106, 0.5871106, 0.5871106, 0.587110…\n$ sd_nitrogen       &lt;dbl&gt; 0.4282233, 0.4282233, 0.4282233, 0.4282233, 0.428223…\n\n\n\n\n\n\nSummary\nWe have shown a range of functions in R for changing the structure of your data. Hopefully, this has given an insight of how powerful the tidyverse is for efficient data manipulation. Importantly all of these processes are reproducible and limit the chances of us making errors in our workflow.\nIf we’ve missed a function that you find useful, we’d love to hear what it is. Leave us a comment below!\n\nAdditional Resources\n\nEach tidyverse package has great documentation and useful cheatsheets, available via the tidyverse website\nThe book R for Data Science, which is available online for free, explains how to get your data into R, get it into the most useful structure, transform and visualize it.\nFor helpful visualisations of how tidyverse functions transform data structures, check out Garrick Aden-Buie’s tidyexplain\nLiza Wood, now part of the Exeter Data Analytics Hub, has a great tutorial on the tidyverse on her website\n\n\n\nAcknowledgements\nWe did not create this content alone! Inspiration, tips, and resources have been borrowed from multiple sources."
  },
  {
    "objectID": "posts/intro_to_flextable/index.html",
    "href": "posts/intro_to_flextable/index.html",
    "title": "intRos: Reproducible tables with flextable",
    "section": "",
    "text": "As scientists we make A LOT of tables, be it of summary statistics, model parameter values, contrast tables, or anything else. It used to be common practice to copy these individually into Excel or Word and then create a table to insert into your work, but this is prone to errors. No one is perfect after all.\nThis problem has now been solved by a bunch of different packages that allow for tables to be created in R and exported to a variety of formats. This means that fewer mistakes are made, that tables can easily be remade if the data or stats change, and that the process of making your tables is completely reproducible. The R package we will introduce to do this today is flextable, but there are other options available we won’t cover here such as gt and huxtable. I love flextable because it seems infinitely customisable and produces tables that are formatted beautifully to go into research articles.\nflextable provides an easy and flexible way to create tables for reporting and presentations. It allows for customization of the table appearance, including cell borders, font styles, background color, and more. The package also supports different formats such as HTML, Microsoft Word, and PowerPoint. It is well suited for creating reports and presentations with a high level of formatting, while still being simple and straightforward to use. Additionally, flextable has a number of features that make it particularly useful for data visualisation, including the ability to merge cells, add custom functions for data formatting, and control the appearance of the table based on data values (i.e. making p values that are &lt;0.05 bold).\nNote: There are likely to be more elegant ways to do some of the code we present here. If you know of a simpler/easier way then we would love for you to help contribute to this project."
  },
  {
    "objectID": "posts/intro_to_flextable/index.html#make-table-of-summary-stats",
    "href": "posts/intro_to_flextable/index.html#make-table-of-summary-stats",
    "title": "intRos: Reproducible tables with flextable",
    "section": "Make table of summary stats",
    "text": "Make table of summary stats\nIt is common to make tables summarising aspects of any given dataset. We will do that here with flextable. Lets say we want to know the averages and standard deviations of body mass, bill length and bill depth of the different penguins species split by sex.\nWe can easily do this using group_by() and summarise() in dplyr. More information on these functions can be found in our tidyverse intRo.\nAlso because I dislike the behaviour of factors in R sometimes, we will first convert all columns that are factors into characters.\n\n# convert all columns that are factors into characters\nd &lt;- mutate(d, across(where(is.factor), as.character))\n\n# create summary data\nd_summary &lt;- group_by(d, species, sex) %&gt;%\n  summarise(\n    # calculate number in each group\n    num_penguins = n(),\n    # can use across to apply the same functions to multiple columns\n    across(c(bill_length_mm, bill_depth_mm, body_mass_g), list(mean = ~mean(.x, na.rm = TRUE), sd = ~sd(.x, na.rm = TRUE))),\n    # specify we want to drop the groups\n    .groups = 'drop')\n\n# look at our table\nd_summary\n\n# A tibble: 8 × 9\n  species   sex    num_penguins bill_length_mm_mean bill_length_mm_sd\n  &lt;chr&gt;     &lt;chr&gt;         &lt;int&gt;               &lt;dbl&gt;             &lt;dbl&gt;\n1 Adelie    female           73                37.3              2.03\n2 Adelie    male             73                40.4              2.28\n3 Adelie    &lt;NA&gt;              6                37.8              2.80\n4 Chinstrap female           34                46.6              3.11\n5 Chinstrap male             34                51.1              1.56\n6 Gentoo    female           58                45.6              2.05\n7 Gentoo    male             61                49.5              2.72\n8 Gentoo    &lt;NA&gt;              5                45.6              1.37\n# ℹ 4 more variables: bill_depth_mm_mean &lt;dbl&gt;, bill_depth_mm_sd &lt;dbl&gt;,\n#   body_mass_g_mean &lt;dbl&gt;, body_mass_g_sd &lt;dbl&gt;\n\n\nWe can see there are some NAs for the sex of the penguins. We can make these unknown using replace_na().\n\n# replace NAs in our sex column\nd_summary &lt;- mutate(d_summary, sex = replace_na(sex, 'unknown'))\n\nWe can immediately make this into a flextable using the flextable() function.\n\n# make our table into a flex table\ntable1 &lt;- flextable(d_summary)\n\ntable1\n\n\nspeciessexnum_penguinsbill_length_mm_meanbill_length_mm_sdbill_depth_mm_meanbill_depth_mm_sdbody_mass_g_meanbody_mass_g_sdAdeliefemale7337.257532.02888317.621920.94299273,368.836269.3801Adeliemale7340.390412.27713119.072601.01888564,043.493346.8116Adelieunknown637.840002.80232018.320001.26964563,540.000477.1661Chinstrapfemale3446.573533.10866917.588240.78112773,527.206285.3339Chinstrapmale3451.094121.56455819.252940.76127303,938.971362.1376Gentoofemale5845.563792.05124714.237930.54024934,679.741281.5783Gentoomale6149.473772.72059415.718030.74105965,484.836313.1586Gentoounknown545.625001.37447014.550000.81034974,587.500338.1937\n\n\nThis already looks pretty good. But we can now add layers of formatting onto this table. The layers of formatting work much like the layers of ggplot2. You start with the base level and each new command creates new formatting of the whole table. Because of this, the order of the commands can make a big difference.\nFor this table, we will do a few different things:\n\nrename the column names\nmerge the species column vertically to only have one instance of species\nround the numbers to 2 decimal places\nchange the font and font size\nplay around with the column names to create a tiered column name for each trait (bill length, bill depth, body mass)\nadd in some horizontal lines to split species apart\n\nWe can do this by adding new layers using the %&gt;%. Many of the functions involve you specifying what part you want the change to apply to (‘all’, ‘body’, or ‘header’). Specific rows can be specified using the i argument, and columns using j.\nTo add a new header row, we need to create a new object which contains the text we want to add.\nFor each new thing we do, I have commented what the command is doing so that the code makes as much sense as possible.\n\n# text to be new header label\n# add same name for columns we want to merge later on\npars &lt;- as_paragraph(\n  as_chunk(c('Species', 'Sex', 'n', 'Bill Length (mm)', 'Beak Length (mm)', 'Body Mass (g)'))\n)\n\n\n# edit table1\ntable1 &lt;- flextable(d_summary) %&gt;%\n  # rename all column names\n  set_header_labels(species = 'Species',\n                    sex = 'Sex',\n                    num_penguins = 'n',\n                    bill_length_mm_mean = 'mean',\n                    bill_length_mm_sd = 'sd',\n                    bill_depth_mm_mean = 'mean',\n                    bill_depth_mm_sd = 'sd',\n                    body_mass_g_mean = 'mean',\n                    body_mass_g_sd = 'sd') %&gt;%\n  add_header_row(values = pars, colwidths = c(1, 1, 1, 2, 2, 2), top = TRUE) %&gt;% # add header row\n  merge_v(j = c(1,2,3), part = 'header') %&gt;%\n  merge_v(~species) %&gt;% # merge multiple instances of the same species name\n  fix_border_issues() %&gt;% # fix random border issues after merging\n  valign(valign = 'top', j = 1, part = 'body') %&gt;% # align Species column\n  colformat_double(j = c(4,5,6,7,8,9), digits = 2) %&gt;% # round numbers of specific columns to 2 decimal places\n  align(align = 'center', part = 'header') %&gt;% # align column names centrally\n  align(align = 'left', part = 'body') %&gt;% # align table output to the left\n  hline(i = c(3, 5), border = fp_border_default()) %&gt;% # add in custom horizontal lines to split penguins up\n  font(fontname = 'Times', part = 'all') %&gt;% # set font name for the table\n  fontsize(size = 12, part = 'all') %&gt;% # set font size for the table\n  autofit()\n  \n# view table\ntable1\n\n\nSpeciesSexnBill Length (mm)Beak Length (mm)Body Mass (g)meansdmeansdmeansdAdeliefemale7337.262.0317.620.943,368.84269.38male7340.392.2819.071.024,043.49346.81unknown637.842.8018.321.273,540.00477.17Chinstrapfemale3446.573.1117.590.783,527.21285.33male3451.091.5619.250.763,938.97362.14Gentoofemale5845.562.0514.240.544,679.74281.58male6149.472.7215.720.745,484.84313.16unknown545.621.3714.550.814,587.50338.19\n\n\nThis is looking really good now! We can now export this table in a variety of formats (suitable for word documents, as an image or for powerpoint). I will write an exhaustive list of them here, but I generally like using an image for as long as possible so that I do not have as many formatting issues when putting tables into Microsoft Word or Google Docs.\n\n# save as image\nsave_as_image(table1, 'where/to/save/table1.png', webshot = 'webshot2')\n\n# save for word document\nsave_as_docx(table1, 'where/to/save/table1.docx', align = 'center')\n\n# save for powerpoint\nsave_as_pptx(table1, path = 'where/to/save/table1.ppt')"
  },
  {
    "objectID": "posts/intro_to_flextable/index.html#make-table-of-model-parameters",
    "href": "posts/intro_to_flextable/index.html#make-table-of-model-parameters",
    "title": "intRos: Reproducible tables with flextable",
    "section": "Make table of model parameters",
    "text": "Make table of model parameters\nWe can use very similar methods to summarise parameters from statistical models. To demonstrate this, I will fit a linear model looking at how body mass is different between species of penguins. We will then use emmeans() to extract model parameters and put them into a table.\n\n# do models\nmod1 &lt;- lm(body_mass_g ~ species, d)\nmod2 &lt;- lm(body_mass_g ~ 1, d)\n\n# check for significance using an anova\nanova(mod1, mod2)\n\nAnalysis of Variance Table\n\nModel 1: body_mass_g ~ species\nModel 2: body_mass_g ~ 1\n  Res.Df       RSS Df  Sum of Sq      F    Pr(&gt;F)    \n1    339  72443483                                   \n2    341 219307697 -2 -146864214 343.63 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# super different\n\n# grab estimates for each species using emmeans\nd_params &lt;- emmeans(mod1, ~species)\n\nd_params\n\n species   emmean   SE  df lower.CL upper.CL\n Adelie      3701 37.6 339     3627     3775\n Chinstrap   3733 56.1 339     3623     3843\n Gentoo      5076 41.7 339     4994     5158\n\nConfidence level used: 0.95 \n\n\nWe can now turn this into a nice table using similar methods to those we used above.\n\n# text to be new header label\n# add same name for columns we want to merge later on\npars &lt;- as_paragraph(\n  as_chunk(c('Species', 'Body Mass (g)', 'd.f.'))\n)\n\ndata.frame(d_params) %&gt;%\n  # rejig column name orders to have degrees of freedom at the end\n  select(species, emmean, SE, lower.CL, upper.CL, df) %&gt;%\n  flextable() %&gt;%\n  set_header_labels(species = 'Species',\n                    emmean = 'Estimate',\n                    SE = 's.e.',\n                    df = 'd.f.',\n                    lower.CL = \"2.5%CI\",\n                    upper.CL = \"97.5%CI\") %&gt;%\n  add_header_row(values = pars, colwidths = c(1, 4, 1), top = TRUE) %&gt;% # add header row\n  merge_v(j = c(1,6), part = 'header') %&gt;% # merge column names together\n  italic(j = c(3, 6), part = 'header') %&gt;% # make some column names italic\n  colformat_double(j = c(2:5), digits = 2) %&gt;% # round numbers of specific columns to 2 decimal places\n  align(align = 'center', part = 'header') %&gt;% # align column names centrally\n  align(align = 'left', part = 'body') %&gt;% # align table output to the left\n  font(fontname = 'Times', part = 'all') %&gt;% # set font name for the table\n  fontsize(size = 12, part = 'all') %&gt;% # set font size for the table\n  autofit() # fix any random size issues\n\n\nSpeciesBody Mass (g)d.f.Estimates.e.2.5%CI97.5%CIAdelie3,700.6637.623,626.673,774.66339Chinstrap3,733.0956.063,622.823,843.36339Gentoo5,076.0241.684,994.035,158.00339\n\n\nNice. This table can easily be extended to have other parameters in, and this approach can be reused for countless types of models."
  },
  {
    "objectID": "posts/intro_to_flextable/index.html#make-table-of-contrasts",
    "href": "posts/intro_to_flextable/index.html#make-table-of-contrasts",
    "title": "intRos: Reproducible tables with flextable",
    "section": "Make table of contrasts",
    "text": "Make table of contrasts\nAnother common thing we might want to do is look at which (if any) species have significantly different body mass to each other. We can do this by doing post-hoc contrasts between the different species, given we know there is a significant effect of species on body mass.\nAgain we can look at this using emmeans().\n\ncontrasts &lt;- emmeans(mod1, pairwise ~ species)\ncontrasts$contrasts\n\n contrast           estimate   SE  df t.ratio p.value\n Adelie - Chinstrap    -32.4 67.5 339  -0.480  0.8807\n Adelie - Gentoo     -1375.4 56.1 339 -24.495  &lt;.0001\n Chinstrap - Gentoo  -1342.9 69.9 339 -19.224  &lt;.0001\n\nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\nWe can easily turn this into a publication-ready table using flextable(). As the p values are so low, we will conver them to being &lt;0.0001 where appropriate, and convert that column to a character. This gives us more control over how flextable prints the output.\nThis is appropriate here but might not be the best way in other cases.\n\ncontrasts$contrasts %&gt;%\n  data.frame() %&gt;%\n  mutate(p.value = ifelse(p.value &lt; 0.0001, \"&lt;0.0001\", as.character(round(p.value, 2)))) %&gt;%\n  flextable() %&gt;%\n  set_header_labels(contrast = 'Contrast',\n                    emmean = 'Estimate',\n                    SE = 's.e.',\n                    df = 'd.f.',\n                    t.ratio = \"t-ratio\",\n                    p.value = \"p value\") %&gt;%\n  italic(j = c(3:6), part = 'header') %&gt;% # make some column names italic\n  colformat_double(j = c(2:6), digits = 2) %&gt;% # round numbers of specific columns to 2 decimal places\n  align(align = 'center', part = 'header') %&gt;% # align column names centrally\n  align(align = 'left', part = 'body') %&gt;% # align table output to the left\n  font(fontname = 'Times', part = 'all') %&gt;% # set font name for the table\n  bold(~p.value == \"&lt;0.0001\", j = \"p.value\") %&gt;% # make significant p values bold\n  fontsize(size = 12, part = 'all') %&gt;% # set font size for the table\n  autofit() # fix any random size issues\n\n\nContrastestimates.e.d.f.t-ratiop valueAdelie - Chinstrap-32.4367.51339.00-0.480.88Adelie - Gentoo-1,375.3556.15339.00-24.50&lt;0.0001Chinstrap - Gentoo-1,342.9369.86339.00-19.22&lt;0.0001"
  },
  {
    "objectID": "posts/intro_to_projects/index.html",
    "href": "posts/intro_to_projects/index.html",
    "title": "intRos: Managing research projects with R Studio",
    "section": "",
    "text": "When we’re doing research, we often find ourselves working on more than one research project at time. Managing multiple folders of data and code can be confusing enough, but when we throw R into the mix, things can get messy very quickly. A large part of the confusion comes from how most folks manage research projects on their own system — which was been aptly summarised by the wonderful Jenny Bryan:\n\nIf the first line of your R script is\nsetwd(\"C:\\Users\\jenny\\path\\that\\only\\I\\have\")\nI will come into your office and SET YOUR COMPUTER ON FIRE 🔥.\nIf the first line of your R script is\nrm(list = ls())\nI will come into your office and SET YOUR COMPUTER ON FIRE 🔥.\n\nWhile I won’t come and set your computer on fire (especially because I have committed both of these coding crimes in the past), I do want to teach you about two really useful tools for making your life easier for managing research projects, and much nicer for sharing code with collaborators.These two tools are R Studio projects and the here package."
  },
  {
    "objectID": "posts/intro_to_projects/index.html#the-magic-of-rstudio-and-projects",
    "href": "posts/intro_to_projects/index.html#the-magic-of-rstudio-and-projects",
    "title": "intRos: Managing research projects with R Studio",
    "section": "The Magic of RStudio and Projects",
    "text": "The Magic of RStudio and Projects\nR Studio is great, and if you don’t already use it I’d highly recommend starting now! R Studio is what we’d call a IDE (Integrated Development Environment) that allows us to integrate with R to do much more than execute code. Despite its wide use in the science community, there’s one bit of functionality that seems to regularly overlooked by many folks: “Projects”. These files — which you can start to explore from the drop-down menu in the top-right of R Studio — are designed for compartmentalising your work into separate contexts, which are great if you have multiple research projects on the go at once.\nRStudio projects resolve the issue of rm(list = ls()) by creating a separate R Studio instance for each new project, each with its own R session, environment, temporary files and working directory (see next section on the ‘here’ package for more on filepaths).\nIf we navigate to New Project... (either from File or the drop-down Projects toolbar in the top-right), we are given the option of creating a new R Studio project in one of three ways:\n\nNew directory (i.e. start from an empty folder)\nExisting directory (when you already have a folder of code and data that you want to turn into a project)\nVersion control (when you want to start version control of a project — see Next steps section for more info on this)\n\nMost of the time you’ll be starting a new project in an empty folder, so we’ll use the New Directory option:\n\n\n\n\n\nNext you’re provided with a bunch of alternate project types, but we’ll go for the New Project option:\n\n\n\n\n\nLastly, we get to choose the directory name (this is the name the project folder will have, so make sure it identifies your project clearly) and where this directory will be saved (i.e where on your computer you want to keep your research project folders):\nNote: If you tick the box to open in a new session (circled red), whatever you’re currently working on in R Studio won’t have to be saved and closed.\n\n\n\n\n\nAnd that’s it! You now have a nice folder ready to fill with data and code, and never need to use rm(list = ls()) ever again. R Studio also did a couple of things for us in the background when we hit the Create Project button:\n\nCreated an .Rproj file in the top level of the folder (can be used to directly open each project in a new session, and also contains various options for project-specific settings)\nCreated a hidden directory .Rproj.user where project-specific temporary files are stored (useful if you have a session terminate unexpectedly)\nLoaded the project into R Studio, with the project name displayed in the Projects toolbar (top-right) and also in front of the RStudio icon in the task bar (appearance differs between macOS and Windows)\nWhile your root filepath would normally have started at the top of your disk drive, it should now start wherever the Project’s .Rproj file is (you’ll see this pathn when you first load here). So no more writing — C:\\extremely\\long\\filepath\\that\\only\\I\\have every time (BUT see next section on the here package for making this even better)."
  },
  {
    "objectID": "posts/intro_to_projects/index.html#best-practices-for-structuring-project-folders",
    "href": "posts/intro_to_projects/index.html#best-practices-for-structuring-project-folders",
    "title": "intRos: Managing research projects with R Studio",
    "section": "Best practices for structuring project folders",
    "text": "Best practices for structuring project folders\nOnce a project is created, it’s worth adding folders to contain each of the relevant types of data for your research. For most of us this usually means something like the following: raw_data, code, outputs (folders should only contain data relevant to the research project).\n\n\n\n\n\nExample of what your sub-folders might look like inside your Project folder (adapted from Martin Chan’s beginner’s guide to using R Studio projects)"
  },
  {
    "objectID": "posts/intro_to_projects/index.html#building-robust-filepaths-with-the-here-package",
    "href": "posts/intro_to_projects/index.html#building-robust-filepaths-with-the-here-package",
    "title": "intRos: Managing research projects with R Studio",
    "section": "Building robust filepaths with the here package",
    "text": "Building robust filepaths with the here package\nR Studio projects go a long way to making simpler filepaths for our code by removing the need for absolute filepaths (which are specific to your system), and introducing relative filepaths (i.e. ones that start at the location of .Rproj file). That said, we can make things even better and more reproducible by making use of the here package. here helps make filepaths consistent between different operating systems. For example, while macOS users have filepaths separated by forward slashes (/), windows users have filepaths with backward slashes (\\). This means that even with the shorter relative filepaths we get when using Projects, paths like raw_data/2023.csv will only work for macOS users, and if you share this fragile code with someone using Windows it will just throw errors.\nIn comes the here package to the rescue — we can use it to make reproducible filepaths that work for everyone! here works in a similar way to file.path, where we build a path to a folder or file by listing elements of the path as arguments. For example, if we had some data in a project sub folder of raw_data called 2023.csv, we’d make a reproducible filepath to it by writing here(\"raw_data, \"2023.csv\"). here does some helpful stuff in the background by a) adding our project’s root directory (which you can look at by just running here()), b) starts evaluating path from the project up (which file.path doesn’t), and c) all while using the right / or \\ delimiter depending on your system!\nWhen you want to actually use a filepath for say, reading some data with read_csv, you just have to provide the here function and folder arguments wherever you’d normally be putting a Users/stephen's/long/winded/filepath/to/a/file:\n\ninstall.packages(\"here\") #install\nlibrary(here) #load\n\nhere() #this will magically show the project root directory on your machine\nhere(\"raw_data\", \"2019.csv\") #this builds a filepath for where you want to go\nread_csv(file = here(\"raw_data\", \"2019.csv\")) #build this same filepath in read_csv"
  },
  {
    "objectID": "posts/intro_to_projects/index.html#next-steps",
    "href": "posts/intro_to_projects/index.html#next-steps",
    "title": "intRos: Managing research projects with R Studio",
    "section": "Next steps",
    "text": "Next steps\nUsing these two sets of tools together, you’re now primed to do some reproducible science on as many different projects as you want! But how can we go about sharing this with other researchers if we wanted to? While you can technically share the entire project folder with collaborators directly, we can also use version control software and online repositories to manage changes to our code and make it accessible. The main way folks do this is using git and GitHub, and we will do a later post on how to get set up with both in due course.\nintRo to using git, GitHub & R Studio (post coming soon)"
  },
  {
    "objectID": "posts/intro_to_ggplot2/index.html",
    "href": "posts/intro_to_ggplot2/index.html",
    "title": "intRos: Publication ready plots using ggplot2",
    "section": "",
    "text": "I love making graphs! To explore data, to plot model results, just for the fun of making plots. I imagine we have all spent a huge amount of time making our plots ready for publication. Here we have a quick walkthrough of using ggplot2 to create graphs for publication, and a few tips and tricks we have learned along the way. If you have a favourite ggplot2 trick or tip then we would love for you to get in contact with us and for you add it to this walkthrough. Or if you’re feeling adventurous, clone the GitHub repo, add in your tip, re-render this quarto post, commit and push the changes, and start a Pull Request!\nggplot2 is a popular data visualisation package in R that allows users to create high-quality and customisable graphics for data exploration, scientific publications, and presentations. Developed by Hadley Wickham, ggplot2 is built on the principles of the Grammar of Graphics, which defines a set of rules for constructing and interpreting visualisations. With ggplot2, users can easily create a wide range of plots with advanced features such as facets, themes, and labels. Whether you are at the beginning of your science journey, or an old and grumpy PI, ggplot2 provides a powerful and flexible tool for visualising data and gaining insights from it.\nMaking a ggplot2 is like writing and following a recipe. You start with the first instructions (a call to ggplot()), the next instruction (like the next item of the recipe) is signalled using a +, a new line, and new instructions are added sequentially. ggplot2 works in a sequential fashion, later instructions add to what is already there, and sometimes counteract or override what previous instructions.\n\n\n\nMaking a plot using ggplot2 works much like a recipe"
  },
  {
    "objectID": "posts/intro_to_ggplot2/index.html#preparing-your-dataset-for-ggplot2",
    "href": "posts/intro_to_ggplot2/index.html#preparing-your-dataset-for-ggplot2",
    "title": "intRos: Publication ready plots using ggplot2",
    "section": "Preparing your dataset for ggplot2",
    "text": "Preparing your dataset for ggplot2\nFor ggplot2, your data wants to be organised in long format, as opposed to wide."
  },
  {
    "objectID": "posts/intro_to_ggplot2/index.html#make-a-first-plot-using-ggplot2",
    "href": "posts/intro_to_ggplot2/index.html#make-a-first-plot-using-ggplot2",
    "title": "intRos: Publication ready plots using ggplot2",
    "section": "Make a first plot using ggplot2",
    "text": "Make a first plot using ggplot2\nWe will make a boxplot of how bill length changes between different species of penguin and add the raw points over the top. After making an initial plot call using ggplot(), different types of plot are specified using a geom_ function (e.g. geom_boxplot() for boxplots, geom_point() for points, geom_line() for a line graph). In each layer, the aes() function is used to tell ggplot2 which arguments depend on aspects of the dataset (e.g. col = species). Arguments set outside of aes() do not rely on the dataset (e.g. size = 3, col = ‘black’).\n\n# make first box plot with raw points\nggplot(d, aes(species, bill_length_mm, col = species)) + # add first empty plot layer\n  geom_boxplot(outlier.shape = NA) + # add boxplot, do not plot the outliers\n  geom_jitter(width = 0.2) # add jittered raw points\n\n\n\n\n\n\n\n\nOk this looks nice. Here I put the aes() commands inside the original call to ggplot() as then those arguments are passed to all other layers (unless overridden by another aes()). This looks quite nice, but we can change a bunch of other things to make it look nicer."
  },
  {
    "objectID": "posts/intro_to_ggplot2/index.html#adding-extra-layers-to-ggplot2",
    "href": "posts/intro_to_ggplot2/index.html#adding-extra-layers-to-ggplot2",
    "title": "intRos: Publication ready plots using ggplot2",
    "section": "Adding extra layers to ggplot2",
    "text": "Adding extra layers to ggplot2\nThere are a bunch of things I do as standard when I am making a plot (and a boxplot in particular):\n\nChange the axis labels\nRemove the legend if not needed\nChange the axis limits\nChange the theme and the text size\nI dislike the default box plot so I colour the whole thing and then add a median bar in white (see below)\n\nThe important thing is to create your own style. To do what you think looks best, but all the options are documented nicely on their website. There are many more detailed resources around, so instead here we will document common things we do.\n\n# make next box plot with raw points\np1 &lt;- ggplot(d, aes(species, bill_length_mm, col = species, fill = species)) + # add first empty plot layer\n  geom_boxplot(outlier.shape = NA) + # add boxplot, do not plot the outliers\n  stat_summary(geom = \"crossbar\", fatten = 2, color = \"white\", width = 0.4, \n               fun.data = function(x){return(c(y = stats::median(x), ymin = stats::median(x), ymax = stats::median(x)))}) + # add median line to the boxplots\n  geom_jitter(width = 0.2, shape = 21, fill = 'white') + # add jittered raw points\n  theme_bw(base_size = 14) + # change theme to the one I like (others are available)\n  labs(x = 'Species',\n       y = 'Bill length (mm)',\n       title = 'Do different penguin species have different bill lengths?') + # change labels\n  ylim(c(30,65)) + # change y limits\n  guides(col = 'none',\n         fill = 'none') # turn off legend\n\np1\n\n\n\n\n\n\n\n\nAnd there - with relatively few lines of code - we have a plot that is ready for inserting into a publication. This can be easily saved out using ggsave()\n\n# save out plot using ggsave\nggsave('plot_one.png', p1, width = 5, height = 7)"
  },
  {
    "objectID": "posts/intro_to_ggplot2/index.html#our-favourite-tips-and-tricks-when-making-plots",
    "href": "posts/intro_to_ggplot2/index.html#our-favourite-tips-and-tricks-when-making-plots",
    "title": "intRos: Publication ready plots using ggplot2",
    "section": "Our favourite tips and tricks when making plots",
    "text": "Our favourite tips and tricks when making plots\nI have a set of rules I follow when making plots with ggplot2. Some may be different to yours but they might have some use:\n\nGenerally I make my datasets BEFORE they go into ggplot2. This includes checking for NA and Inf values as these are super important to know about when visualising and analysing your data!\n\nPlot the raw data whenever possible.\n\nLayers work iteratively (later layers go on top of earlier layers) so make sure layers you want to be in the background (e.g. boxplot) are before layers you want to overlay (e.g. the raw data)\nOnly use stat_smooth() for exploring datasets. When plotting model predictions and confidence intervals, create a dataframe for them and then feed that into ggplot2.\n\n\nConvert text size in geom_label() or geom_text() to pts used for labels\nLets say we wanted to add text to the plot. For example, in the plot above maybe we want to add the total number of penguins sampled for each species. By default geom_text() and geom_label() use a size guide that is different to the font size set for the axis and tick labels. However, Andrew Heiss has written a function to convert between those so we can easily define the font size in the regular way.\nWe will use functions from the tidyverse to create a data frame with the total number of penguins sampled and the max bill length for each penguin. Creating a data frame with the x and y coordinates of where I want the text to go on the plot, and a column for the label, is how I approach adding text labels to an existing ggplot.\n\n# define function for pts\npts &lt;- function(x){\n    as.numeric(grid::convertX(grid::unit(x, \"points\"), \"mm\"))\n  }\n\n# calculate n for putting each species\n# remove NAs because they are not in the plot\nd_n &lt;- filter(d, !is.na(bill_length_mm)) %&gt;%\n  group_by(species) %&gt;%\n  summarise(n = n(),\n            max_bill_length = max(bill_length_mm)) %&gt;%\n  ungroup()\n\n# make plot with text\n# make next box plot with raw points\np1 +\n  geom_text(aes(x = species, y = max_bill_length + 5, label = paste('n = ', n, sep = '')), d_n, size = pts(12), col = 'black') # add text label\n\n\n\n\n\n\n\n\n\n\nSoft wrap or stagger text labels to stop them overlapping\nOk these two are quite recent finds for me, and I absolutely love them. Sometimes your x labels might be really long, and the text can overlap with each other. I used to manually add new lines into my text by inserting \\n where I wanted new text to me. But now two approaches exist that can automatically improve spacing between labels on the x axis. These are scales::label_wrap which can automatically create new lines for long axis labels and guide_axis() which can stagger axis labels to prevent overlapping.\nWe will demonstrate them both below, by adding the Latin names for each penguin species to the dataset. We will add them in using case_when() and then show both plots next to each other using patchwork which makes it really easy to combine individual plots. For free you also get code to italicise the axis labels as they are latin names!\n\nd &lt;- mutate(d, latin_name = case_when(species == \"Gentoo\" ~ \"Pygoscelis papua\",\n                                      species == \"Chinstrap\" ~ \"Pygoscelis antarcticus\",\n                                      species == \"Adelie\" ~ \"Pygoscelis adeliae\"))\n\n# first to softwrap the text\np2 &lt;- ggplot(d, aes(latin_name, bill_length_mm, col = species, fill = species)) + # add first empty plot layer\n  geom_boxplot(outlier.shape = NA) + # add boxplot, do not plot the outliers\n  stat_summary(geom = \"crossbar\", fatten = 2, color = \"white\", width = 0.4, \n               fun.data = function(x){return(c(y = stats::median(x), ymin = stats::median(x), ymax = stats::median(x)))}) + # add median line to the boxplots\n  geom_jitter(width = 0.2, shape = 21, fill = 'white') + # add jittered raw points\n  theme_bw(base_size = 14) + # change theme to the one I like (others are available)\n  labs(x = 'Species',\n       y = 'Bill length (mm)') + # change labels\n  ylim(c(30,65)) + # change y limits\n  guides(col = 'none',\n         fill = 'none') + # turn off legend\n  theme(axis.text.x = element_text(face = \"italic\")) + # italicise x axis labels\n  scale_x_discrete(labels = scales::label_wrap(10)) # soft wrap labels\n\np3 &lt;- p2 +\n  scale_x_discrete(guide = guide_axis(n.dodge = 2)) # stagger axis labels\n\np2 + p3\n\n\n\n\n\n\n\n\nEither of these look great, but I think scales::label_wrap() is my favourite. So simple and so effective!\n\n\nAutomatically label facets with letters\nOne thing I often do is create faceted plots where each plot represents a different treatment or subset of the data. This is really easy to do with ggplot2 using facet_wrap(). However, the default facets are poorly labelled, are not numbered (generally with a, b, c for publications), and have a grey fill colour.\nHowever, these are all issues we can solve to create publication-ready facet labels. We will demonstrate this by plotting a different trait on each facet (bill length, bill depth, flipper length, body mass). We will create the long data using pivot_longer() and use case_when() to make sure the labels on each facet are formatted correctly (i.e. spaces instead of underscores.\nWe can write a function to automatically label each facet, and change theme elements to align text and change the strip background colour.\n\nlong_format &lt;- d %&gt;%\n  mutate(id = 1:n()) %&gt;% # create column to id each individual penguin\n  pivot_longer(., cols = c(bill_length_mm:body_mass_g), names_to = 'trait', values_to = 'value') %&gt;% # go from wide to long format for the traits\n  mutate(facet_label = case_when(trait == 'bill_length_mm' ~ \"Bill Length (mm)\",\n                                 trait == \"bill_depth_mm\" ~ \"Bill Depth (mm)\",\n                                 trait == \"body_mass_g\" ~ \"Body Mass (g)\",\n                                 trait == \"flipper_length_mm\" ~ \"Flipper Length (mm)\"))\n\n# create function to add letter to facet labels\nletter_facets &lt;- function (string){\n    len &lt;- length(string)\n    string = paste(\"(\", letters[1:len], \") \", string, sep = \"\")\n    return(string)\n}\n\n# create plot\nggplot(long_format, aes(species, value, col = species, fill = species)) + # add first empty plot layer\n  geom_boxplot(outlier.shape = NA) + # add boxplot, do not plot the outliers\n  stat_summary(geom = \"crossbar\", fatten = 2, color = \"white\", width = 0.4, \n               fun.data = function(x){return(c(y = stats::median(x), ymin = stats::median(x), ymax = stats::median(x)))}) + # add median line to the boxplots\n  geom_jitter(width = 0.2, shape = 21, fill = 'white') + # add jittered raw points\n  theme_bw(base_size = 14) + # change theme to the one I like (others are available)\n  labs(x = 'Species',\n       y = 'Trait measurement') + # change labels\n  guides(col = 'none',\n         fill = 'none') +\n  facet_wrap(~facet_label, scales = 'free_y', labeller = labeller(facet_label = letter_facets)) + # facet by the label column so the text is correct \n  theme(strip.background = element_blank(),\n        strip.text = element_text(hjust = 0))\n\n\n\n\n\n\n\n\nBINGO THIS LOOKS SHAMAZING.\n\n\nBeautiful colour schemes with MetBrewer\nI take a lot of time looking at colours to use in my plots. I have used viridis before, the colour schemes from colorbrewer, and Pokémon inspired colour schemes using palettetown. If you are so inclined, I have also started writing a small package (BrewerUoE) to access the new University of Exeter colours in R.\nHowever, the most beautiful colour schemes I have ever seen are those provided by MetBrewer, which are palettes inspired by works at the Metropolitan Museum of Art in New York. They are just sensational, and easy to add directly to your plots! We show an example how to below, but there are more examples on MetBrewer’s GitHub.\n\np1 + \n  scale_color_manual(values=met.brewer(\"Klimt\", 3)) +\n  scale_fill_manual(values=met.brewer(\"Klimt\", 3))\n\n\n\n\n\n\nAdd a final NULL layer to allow you to comment out layers\nIt is common for me to play around with different layers and switch some on and off with a hashtag. If you hash out the final layer then the layer before it with a + carries on to the next set of code. If you add a final + NULL to the plot then this behaviour doesn’t happen.\nLets say we wanted to see the plot without the text labels. We can turn them off by hashtagging them out (in case we want to add them back in later), and the final NULL means the plot still works.\n\np1 +\n  #geom_text(aes(x = species, y = max_bill_length + 5, label = paste('n = ', n, sep = '')), d_n, size = pts(12), col = 'black') + # add text label\n  NULL # add NULL layer to allow easy commenting out of layers\n\n\n\n\n\n\n\n\n\n\nControl legend shapes and override legend appearance\nSometimes you colour text, lines, points with the same colours. When you do this, each of the legends for them get displayed in the legend overlaid with each other. To show this we will create a line plot of the relationship between\n\nggplot(d, aes(body_mass_g, bill_length_mm, col = species)) +\n  geom_point() +\n  stat_smooth(method = 'lm', se = FALSE) +\n  theme_bw()\n\n\n\n\n\n\n\n\nI do not like the legend with multiple geoms, to me it looks like a tram stop sign. Luckily there are easy ways to manually edit the legend.\nFirst we can switch off the legend of any of the elements using the argument show.legend.\n\nggplot(d, aes(body_mass_g, bill_length_mm, col = species)) +\n  geom_point() +\n  stat_smooth(method = 'lm', se = FALSE, show.legend = FALSE) +\n  theme_bw()\n\n\n\n\n\n\n\n\nThis looks nicer. An alternative to this is to use key_glyph which allows you to control the shape of the legend.\n\nggplot(d, aes(body_mass_g, bill_length_mm, col = species)) +\n  geom_point() +\n  stat_smooth(method = 'lm', se = FALSE,  key_glyph = 'point') +\n  theme_bw()\n\n\n\n\n\n\n\n\nWe can also control the size of the shape in the legend by altering the legend defaults using the guides() function.\n\nggplot(d, aes(body_mass_g, bill_length_mm, col = species)) +\n  geom_point() +\n  stat_smooth(method = 'lm', se = FALSE, show.legend = FALSE) +\n  theme_bw() +\n  guides(colour = guide_legend(override.aes = list(size = 20)))\n\n\n\n\n\n\n\n\nOk that last size is a bit much. But you get the idea and now have the tools to manually edit your legend."
  },
  {
    "objectID": "posts/intro_to_ggplot2/index.html#awesome-ggplot2-extensions",
    "href": "posts/intro_to_ggplot2/index.html#awesome-ggplot2-extensions",
    "title": "intRos: Publication ready plots using ggplot2",
    "section": "Awesome ggplot2 extensions",
    "text": "Awesome ggplot2 extensions\ngpgplot2 is so popular as a plotting system that it has become its own ecosystem, with people developing tools and extensions to complement and build extra functionality to ggplot2. All of the extensions can be found here, but below are some of our favourites:\n\nggdist which provides geoms and stats for visualising distributions and uncertainty.\nggtree to make phylogenetic trees using ggplot2.\npatchwork for combining separate plots.\npalettetown for Pokémon themed colour schemes. Whats better than Charizard colours!"
  },
  {
    "objectID": "posts/intro_to_geometric_morphometrics/index.html",
    "href": "posts/intro_to_geometric_morphometrics/index.html",
    "title": "intRos: Geometric Morphometrics",
    "section": "",
    "text": "Geometric morphometrics is a field of study that uses mathematical and statistical methods to analyze the shape and variation of biological forms. It is based on the use of landmarks, which are points that can be identified on each specimen and that correspond to homologous features. Geometric morphometrics can be applied to various biological questions, such as phylogeny, development, evolution, ecology, and function."
  },
  {
    "objectID": "posts/intro_to_geometric_morphometrics/index.html#introduction",
    "href": "posts/intro_to_geometric_morphometrics/index.html#introduction",
    "title": "intRos: Geometric Morphometrics",
    "section": "Introduction",
    "text": "Introduction\nGeomorph is a package for geometric morphometric analysis of two- and three-dimensional landmark data in R. It provides functions for data manipulation, shape alignment, multivariate statistics, visualization, and more. In this tutorial, we will use the Tribolium castaneum dataset, which contains landmark coordinates for elytra and pronotum, to demonstrate some of the basic functionalities of geomorph. For simplicity, we will use the tpsUtil and tpsDig programs and integrate them within this workflow. Users are welcome to utilise the digitisation process within geomorph, if they wish to do this (see section 5 of the GeoMorph manual).\nThis tutorial has been developed as a companion to [manuscript in prep]."
  },
  {
    "objectID": "posts/intro_to_geometric_morphometrics/index.html#data-preparation",
    "href": "posts/intro_to_geometric_morphometrics/index.html#data-preparation",
    "title": "intRos: Geometric Morphometrics",
    "section": "Data preparation",
    "text": "Data preparation\nBefore we can use geomorph, we need to have our landmark data in a specific format called tps. A tps file is a plain text file that contains the coordinates of landmarks for one or more specimens, along with some optional metadata. A tps file has the following structure:\n\nLM=number of landmarks\nx1 y1\nx2 y2\n...\nxn yn\nID=FB001\nIMAGE=FB001.jpg\nSCALE=scale factor\n...\nLM=number of landmarks\nx1 y1\nx2 y2\n...\nxn yn\nID=FB050\nIMAGE=FB050.jpg\nSCALE=scale factor\n...\n\n\nEach specimen is separated by a blank line, and each line starts with a keyword followed by an equal sign and a value. The keywords are case insensitive and can be in any order, except for LM, which must be the first line for each specimen. The LM keyword specifies the number of landmarks, followed by the x and y coordinates of each landmark in separate lines. The ID keyword specifies a unique identifier for the specimen, which can be a number or a string. The IMAGE keyword specifies the name of the image file that contains the specimen, which can be in any format supported by R. The SCALE keyword specifies a scale factor that converts the pixel coordinates to real-world units, such as millimeters or centimeters. Other keywords, such as COMMENT, CURVE, or POINT, can also be used to store additional information, but they are not required by geomorph.\n\nUsing tpsUtil and tpsDig\nDownload and install the tpsUtil32, tpsDig2w64 and tpsRelww64 software from the Stony Brook Morphometrics website. tpsUtil32 is a utility tool and tpsDig2w64 helps digitize coordinates of landmarks and capture outlines. tpsRelww64 performs a relative warp analysis.\nDownload this zip file, and unzip it into a folder on your computer. Remember the name and location of this folder as you will need it in the next few steps. Open tpsUtil, and under the Operation box’s dropdown menu select “Build tps file from images”, select the “input” directory where all your images are stored and then select the location and name for the “output file”. In this example, we have called the tps file – 01_Tribolium_castaneum.tps and saved it in the root of our data folder.\n\n\n\n\n\n\n\n\n\n\n\n\nClick on the “Setup” button to see the list of files that were found in the folder you pointed towards. Exclude any files that should not need digitisation. Press create when ready. This will create a TPS file that we can use for landmarking all our images.\n\n\nRandomise\nNow, using tpsUtil, randomise the order of the files by selecting “randomly order specimens” from the Operations menu. This helps spread out any confounding factors (e.g. photos taken at different times of day / different lighting etc.). In this example, the 01_Tribolium_castaneum.tps file you created in the earlier step is used as the input. Load that file and in the Output option choose the new file-name as 02_Tribolium_castaneum_randomised.tps and press Create. This will create the randomised file. Now, you may close tpsUtil until it is needed again later. Please make sure you take a look at the file you have just created, within a text editor like Context or Notepad++, and then adjust any file paths that need adjusting.\n\n\n\n\n\n\n\nLet’s set some landmarks\nStart the tpsDig program, and make some initial (one time) adjustments to the settings to make viewing and workflows easier. For e.g., first you can adjust the colour, size, and numbering of points under Options &gt; Image Tools &gt; Colors. Labels at width 35 look readable to me.\n\nHowever, you may find a completely different setting, more useful for your set-up (e.g. due to display screen size, resolution etc.). I would suggest initially doing this for 5-10 files and finding a setting that you are comfortable with, before proceeding with a larger batch. Similarly, under Options, select the Label landmarks and Mouse wheel zooms features.\n\nNow, open the 02_Tribolium_castaneum_randomised.tps file image you want to landmark images in, and then use the landmark tool (accessed via  in the top tool bar) to set landmarks on this image.\nUse the landmark tool (accessed via  in the top tool bar), to click on the points identified in numbers 1-17 in the image above. Pay careful attention to the order of the landmarks. If you choose to use a different order, you will need to change the link and slider files accordingly. The landmarking order is particularly important if you have multiple operators who would be contributing to landmarking. In such cases, you should always take some inter-operator repeatability estimates to assess operator error within your data.\nAfter the initial landmarks have been sitiuated, use the background curve tool (accessed via in the top tool bar) to draw a multipoint line between landmark 1 and landmark 2. Usually, you need to just have a few well-spaced points that follow the outline of the shape you are measuring and one must start and end with a landmark.\nRepeat this process to follow the curves between landmark 2-3 and then 12-13. Right-click, double-click, or click on the Edit mode button on the toolbar to start a new curve. Note: This order (i.e. going between 1-2, then 2-3 and finally 12-13 is important - do not jumble these steps at all).\nIn edit mode, moving the cursor over the curve will select it and the number of points will be displayed on the status panel. If one right-clicks on the curve a context menu will be displayed that will allow one to: delete the entire curve, delete a selected point (if the cursor was on a curve point), resample the curve, switch back to digitize landmark mode, or just cancel the context menu.\nNow, while hovering your mouse pointer over one of the blue lines, right-click on the image, select “Resample Curve” and enter 12 number of points.\n\nWhen Resample is selected, then a window will be opened that displays the current number of points on the selected curve and allows one to specify a new number. If the “By steps” option is selected then the new number must be less than the existing number. Points will be deleted to achieve the desired number of points. Usually one will wish to use the “By length” option. With this option the new number of points can be less than, equal to, or larger than the existing number of points. The new points will be computed by linear interpolation along the curve. They will be approximately equally spaced (it is difficult to achieve exact equal spacing because deleting points changes the length of the curve when the curve changes shape as a result of the deletion of points). In both cases the minimum number of new points is two.\n\n\n\n\n\nRepeat this step for each of the three lines. This will convert these curve points to landmark points for further processing. Note: other software may not recognize curve points as landmark points. The tpsUtil program has an option to append curve points as landmarks in order after any existing landmarks.\nNow, before moving on to other images, select the template mode (a choice on the Options menu). This will copy the landmarks from the current specimen onto the next image as long as the next image does not have any landmarks already entered. You can then drag the landmarks to their appropriate locations to make landmarking more efficient. Note that the first landmark you move will translate the locations of all the landmarks. This does not necessarily need to be landmark number one, basically whichever you click on first and drag, decides the shifts associated with the other landmarks. Subsequent landmarks can be moved individually. This option helps minimize the chance of making the common error of digitizing the landmarks out of order.\n\nOnce the landmarks and curves have been digitized for the current specimen, click on the right arrow button or press ALT+N. Similarly, you can press ALT+P or the left arrow to go to the previous specimen\nWhen you are done landmarking, save the file with a unique name and then open this file within tpsUtil to restore the original “Restore original order. This step will reorder the specimens in your TPS file back into their original order using the “OrigNum” keyword in the variables field. Our landmarked and order restored example is available as 03_Tribolium_castaneum_landmarked_original_order.TPS\n\n\nConvert curve to landmarks:\nNow, load the 03_Tribolium_castaneum_landmarked_original_order.TPS file within the tpsUtil program and use the option to append curve points as landmarks in order after any existing landmarks. Save this file as 04_Tribolium_castaneum_curves_removed.TPS\n\nWhen opened in tpsDig, this file will appear like this - note that the blue lines are gone and they have been replaced by actual landmarks.\n\n\n\nDelete extra landmarks:\nNow, load the 04_Tribolium_castaneum_curves_removed.TPS file within the tpsUtil program and use the option to Delete/reorder landmarks. In this case, landmarks 1,18; 2,29,30; 3,41; 12,42; and 13,53 are overlapping with each other so we will delete the higher order landmarks in each of those sets.\n\n\n\n\n\nThis window lists the identification numbers of the landmarks in the input file. You can highlight one or more landmarks (holding down the shift or control keys while clicking with the mouse) and then using the buttons at the right to move the selected landmarks up or down in the list. Landmarks can also be excluded from the output file by unchecking them or by highlighting them with the mouse and then clicking on the “Exclude” button. You can include a landmark again by clicking on its checkbox or by selecting it and then clicking on the “Include” button. Remember to keep the Plot new nums option checked in this step.\nSet the Output filename as 05_Tribolium_castaneum_overlapping_lm_removed.TPS and then click on the “Create” button to actually create the file. Now, the TPS file is nearly ready to be analysed.\n\n\n\n\n\n\n\nSet a scale:\nOpen the scale image within tpsDig, and zoom-in to a size that seems easy to work with. Click on the image tools icon and go to the Measure tab. To set a scale factor, enter the known length of a structure in the edit box and then digitize the two endpoints of the scale. Enter just the numerical value, do not enter the units. Press the OK button to accept the scale factor or the cancel button to ignore any changes you may have made in the scale factor. Make a note of this scale factor as we will be including it in all the .tps files that use this particular calibration image.\n\n\n\n\n\n\n\n\n\n\n\n\nIf your digital scale was included in the images you captured (e.g. an eyepice graticle or equivalent), then you can do the above “set scale” activity within your main .tps digitisation as well. In this case, all subsequent images you digitize will be assumed to have the same scale factor unless you explicitly give them their own scale factors. Using this feature causes the “SCALE=” keyword to be inserted in the output file.\nHere, the value is: SCALE=0.002137. The scale factor is the entered length in user units divided by the measured length in pixels. It also scales the coordinates appropriately (by default the coordinates are in pixel units). The scale factor can also be recorded in the listing window. The scale factor is taken into account in the computation of image areas, perimeters, and linear distance measurements. It has no effect on the landmark coordinates – they remain in pixel units.\n\n\nSplit Pronotum & Elytra landmarks:\nNow, load the 05_Tribolium_castaneum_overlapping_lm_removed.TPS file within the tpsUtil program and use the option to Delete/reorder landmarks. In this case, once we will delete all the elytra landmarks and save the output as 06_Tribolium_castaneum_pronotum.TPS and then repeat this process to delete the pronotum landmarks and save the output as 07_Tribolium_castaneum_elytra.TPS. As earlier, please pay attention to the file-names within the Input and Output boxes and press Create when you are ready to create the outputs.\n\n\n\n\n\nMake a link file:\nWithin tpsUtil, select the “Make links file” option, select the 06_Tribolium_castaneum_pronotum.TPS file as an input (if trying this with a different file, note that at-least one image should be completely landmarked) and select an output destination for the link file as 06_Tribolium_castaneum_pronotum_links.TPS.\nThe locations of the landmarks on the first specimen in a TPS or NTS file will be displayed. With the mouse, one can draw links between any pairs of landmarks.\nWhen drawing the program will display a “rubber band” line and will beep when you are sufficiently close to another landmark so that one can lift the left mouse button to end the current link. To delete the last link drawn, right-click on an unused portion of the background. To delete a specific link, right-click on it.\n\n\n\n\n\nPress “Create File” when done.. keep this file safe - you will be needing it soon. Repeat this step for 07_Tribolium_castaneum_elytra.TPS to create 07_Tribolium_castaneum_elytra_links.NTS.\n\n\n\nMake a sliders file:\nThis file defines how semi landmarks can be slid so as to minimize bending energy during a generalized Procrustes analysis (GPA) superimposition. The locations of the landmarks on the first specimen in a TPS or NTS file are displayed. With the mouse, one can draw links between any triplets of landmarks. The middle landmark of a triplet is then considered a semi landmark (it will be displayed using an open circle) and it will be allowed to slide in a direction parallel to the difference between the other two landmarks. Note: this program does not do the actual sliding. It is just used as a convenience to build the file that defines which points slide between which other points.\nWithin tpsUtil, select the “Make sliders file” option, select the 06_Tribolium_castaneum_pronotum.TPS file as an input and select an output destination for the link file as 06_Tribolium_castaneum_pronotum_sliders.NTS. Click setup, and then go-ahead and draw the connections as described above.\n\nThis image illustrates the sliders used for the pronotum (two fixed landmarks). When ready, click Create to complete this step for the pronotum.\n\nNow, repeat these steps for 07_Tribolium_castaneum_elytra.TPS and create 07_Tribolium_castaneum_elytra_sliders.NTS. This image illustrates sliders used for elytra here (note the 2 fixed landmarks for the pronotum and the 7 fixed landmarks for the elytra). Have a look at our sliders files here: 06_Tribolium_castaneum_pronotum_sliders.NTS and 07_Tribolium_castaneum_elytra_sliders.NTS\n\nThis file has to be entered into the tpsRelw program using the “Open sliders file …” menu option.\n\n\n\n\n\n\nGuess what!\n\n\n\nPhew, that’s most of the hard work done. Ideally, you would work through the rest of the tutorial within R. However, if you are having repeated Arrgh moments within R, you can…. open tpsRelw, Load the *.tps file you created with tpsDig and click through the Compute workflow: Consensus, Partial Warps, Relative Warps. The PCA plot can be found under the Display button “Relative Warps” and you can visualize shapes within the PCA morphospace, using the Camera tool and clicking on a point to visualize.\n\n\n\n\n\n\n\n\nExpand To Learn About this step within Geomorph\n\n\n\n\n\nIf you want to do this step within geomorph, please follow these steps instead:\nTo create a tps file from a directory of image files, we can use the tps.write function from geomorph. This function takes a list of image file names, a matrix of landmark coordinates, and an optional list of specimen identifiers, and writes them to a tps file. The landmark coordinates can be obtained by manually digitizing the landmarks on the images using a software such as tpsDig, or by using an automated method such as the digitize2d function from geomorph. The specimen identifiers can be extracted from the image file names, or assigned by the user. For example, suppose we have a directory called data\\images that contains a subset of 50 image files of Gnatocerus cornutus specimens, named FB001.JPG .. FB150.JPG. We can create a tps file called Gnatocerus_cornutus.tps with the following code:\n\nif (!require(geomorph)) install.packages('geomorph')\nif (!require(stringr)) install.packages('stringr')\nif (!require(tidyverse)) install.packages('tidyverse')\n\n\n# This line checks if the required packages is available.\n# If not, it then proceeds to install that package.\n\n# Load the geomorph package\nlibrary(geomorph, quietly=T)\n\n# Set the working directory to the current folder\nsetwd(\"./\")\n\n# Get the list of image file names from the data/images folder\nimages &lt;- list.files(\"./data/\",pattern = \"\\\\.jpg$\", include.dirs = TRUE,recursive = TRUE, full.names = TRUE)\n\n# Digitize the landmarks on the images using tpsDig or digitize2d\n# For this example, we assume that we have a matrix of landmark coordinates called landmarks\n# The matrix has 50 rows (one for each specimen) and 20 columns (two for each landmark)\n# The landmarks are in the same order and position for all specimens\n\n# Extract the specimen identifiers from the image file names\n# We use the stringr package to remove the extension and the prefix\n\nids &lt;- str_remove(images, \"./data/full_body/\")\nids1 &lt;- str_remove(ids, \"\\\\.jpg$\")\nids2 &lt;- str_remove(ids1, \"PAM\")\nids3 &lt;- str_remove(ids2, \"PAF\")\nids3 &lt;- str_remove(ids3, \"MAM\")\nids4 &lt;- str_remove(ids3, \"MAF\")\n\n# digitize2d(images, nlandmarks=10, scale = 1, tpsfile=\"./data/Tribolium_castaneum.tps\",verbose = TRUE)\n\n# Write the tps file\n# writeland.tps(ids, file = \"Tribolium_castaneum.tps\")\n\n\n\n\n\n\nData analysis\nOnce we have our tps file, we can use geomorph to read, plot, and analyze our landmark data. We can use the readland.tps function to read the tps file and store it in a list of two elements: a matrix of landmark coordinates and a vector of specimen identifiers. We can also use the plotTangentSpace function to plot the landmark data in a two-dimensional space that preserves the shape variation among the specimens. For example, we can read and plot the beetles data with the following code:\n\nif (!require(geomorph)) install.packages('geomorph')\nif (!require(stringr)) install.packages('stringr')\nif (!require(tidyverse)) install.packages('tidyverse')\nlibrary(geomorph, quietly=T)\n\n# Read the landmarks in\n# specID - a character specifying whether to extract the specimen ID names from the ID or IMAGE lines\n\ntribolium &lt;- readland.tps(\"data/full_body/03_Tribolium_castaneum_landmarked_original_order.TPS\", specID=\"ID\")\n\n\n 36 curve points detected per specimen and are appended to fixed landmarks.\n\n# tribolium &lt;- readland.tps(\"data/full_body/pronotum_slider.nts\", readcurves = TRUE)\n\n# The readcurves argument is set to TRUE because we have semilandmarks in our example\n\n# Plot the landmark data in tangent space\n# plot(tribolium)\n\nThe plot shows the shape variation among the beetles specimens along the first two principal components of the Procrustes shape space. The Procrustes shape space is a mathematical space that represents the shapes of objects after removing the effects of translation, rotation, and scaling. The principal components are the directions of maximum variation in the shape space, and they can be interpreted as shape modes or shape factors. The plot also shows the mean shape of the specimens as a black dot, and the shape of each specimen as a blue dot connected to the mean shape by a line. The shape of each specimen can be visualized by hovering over the corresponding dot on the plot.\nWe can also use geomorph to perform various statistical analyses on our landmark data, such as testing for differences in shape among groups, testing for correlations between shape and other variables, testing for allometry or size-shape relationships, testing for phylogenetic signal or evolutionary patterns, and more. Geomorph provides a unified framework for these analyses, based on the generalized Procrustes analysis (GPA) and the Procrustes ANOVA. The GPA is a procedure that aligns the landmark coordinates of the specimens to a common orientation and scale, and calculates the Procrustes shape coordinates and the Procrustes distances. The Procrustes ANOVA is a method that partitions the shape variation among the specimens into different sources, such as group, size, or error, and tests for their significance using permutation tests.\nFor example, suppose we want to test if there is a difference in shape between male and female beetles, and if there is a correlation between shape and body length. We can use the procD.lm function from geomorph to perform these analyses. This function takes a formula that specifies the response variable (shape) and the explanatory variables (sex and length), and a data frame that contains the landmark data and the covariates. The function performs the GPA and the Procrustes ANOVA, and returns a list of results, such as the Procrustes sums of squares, the Procrustes mean squares, the F-statistics, the p-values, and the effect sizes. The function also plots the residuals of the shape variation against the covariates, and the shape changes associated with the covariates. For example, we can perform these analyses on the beetles data with the following code:\n\n# Create a data frame with the landmark data and the covariates\n# For this example, we assume that we have a vector of sex (M or F) and a vector of length (in mm) for each specimen\n# beetles.data &lt;- data.frame(beetles$coords, sex, length)\n\n# Perform the shape analysis\n# beetles.shape &lt;- procD.lm(coords ~ sex + length, data = beetles.data)\n\nThe output of the function shows that there is a significant difference in shape between male and female beetles (p &lt; 0.001), and a significant correlation between shape and length (p &lt; 0.001). The plots show that the shape variation is mostly explained by sex (PC1) and length (PC2), and that the shape changes involve changes in the head, the body, and the tail regions. The plots also show the mean shapes of the male and female beetles, and the shape changes associated with a unit increase in length.\n\n\nGeneralized Procrustes Analysis\nNext, we need to perform a generalized Procrustes analysis (GPA) to align the landmark configurations and remove the effects of translation, rotation, and scaling. We can use the gpagen() function to do this, which returns a list containing the aligned coordinates, the consensus configuration, and the Procrustes distances.\n\n# Y.gpa &lt;- gpagen(plethspecies$land)\n\n\n\nPrincipal component analysis\nOne of the most common methods to explore shape variation is principal component analysis (PCA), which reduces the dimensionality of the shape data and identifies the main axes of variation. We can use the gm.prcomp() function to perform a PCA on the aligned coordinates, which returns an object of class gm.prcomp that contains the eigenvalues, eigenvectors, and scores of the PCA.\n\n# PCA &lt;- gm.prcomp(Y.gpa$coords)\n\n\n\nShape deformation\nTo visualize the shape changes associated with the PCs, we can use the plotRefToTarget() function to produce deformation grids that compare the shapes corresponding to the extremes of a chosen PC axis. For example, to compare the shapes at the minimum and maximum scores of PC1, we can use the following code:"
  },
  {
    "objectID": "posts/install_rstudio_git/index.html",
    "href": "posts/install_rstudio_git/index.html",
    "title": "intRos: Installing, R, RStudio, and Git",
    "section": "",
    "text": "R is one of the most popular programming language for statistical programming in ecology, evolution, and conservation. It is also the language taught in the University of Exeter’s undergraduate and masters courses within the Centre for Ecology and Conservation.\nRStudio is probably the most popular IDE for people using R. An IDE is an integrated development environment, which allows users to combine multiple aspects of software development in one place to make it easier to use. For RStudio, that means you have different panes that contain the R Console, your script, a file explorer, a pane where plots appear, and much more.\nGit is a version control system that allows you to track versions of your files. It is popular when programming to be able to keep a history of all the changes you have made to your work, and be able to retrieve old instances of your work if you have made a mistake later down the line. GitHub is an online hosting service that allows you to share your code and data for free, which is super important in science for being open and reproducible and transparent.\nThis post gives some details on how to install R, RStudio, and git. This information is mainly aggregated from other sources (see acknowledgements below), but there is some specific advice for Windows users for University of Exeter staff and students."
  },
  {
    "objectID": "posts/install_rstudio_git/index.html#for-mac-users",
    "href": "posts/install_rstudio_git/index.html#for-mac-users",
    "title": "intRos: Installing, R, RStudio, and Git",
    "section": "For Mac Users",
    "text": "For Mac Users\nMac users should click the “Download R for macOS” link in the “Download and Install R” section. Click on the link for the latest version of R and click on the “Download R-[version].pkg” link to download the installer package. Open the downloaded file and follow the installation instructions. You can launch R by clicking the R icon on your Launchpad or in your Applications folder of your Finder, but we recommend using RStudio to code in R because it makes programming in R a much easier and more pleasant experience."
  },
  {
    "objectID": "posts/install_rstudio_git/index.html#for-windows-users",
    "href": "posts/install_rstudio_git/index.html#for-windows-users",
    "title": "intRos: Installing, R, RStudio, and Git",
    "section": "For Windows Users",
    "text": "For Windows Users\nWindows users should click the “Download R for Windows” link in the “Download and Install R” section. Click on the “base” directory and click on the link for the latest version of R. Then click on the “Download R[version] for Windows” link, open the downloaded file and follow the installation instructions. You can launch R by double-clicking the R icon on your Desktop or in your start menu, but we recommend using RStudio to code in R because it makes programming in R a much easier and more pleasant experience."
  },
  {
    "objectID": "posts/install_rstudio_git/index.html#some-general-tips-for-global-options-of-rstudio",
    "href": "posts/install_rstudio_git/index.html#some-general-tips-for-global-options-of-rstudio",
    "title": "intRos: Installing, R, RStudio, and Git",
    "section": "Some general tips for Global Options of RStudio",
    "text": "Some general tips for Global Options of RStudio\nWhen setting up RStudio, there are a bunch of things you can change to configure your RStudio to how you like it. Some of these are likely personal, but we have some settings we like to use. These are settings you can change by clicking Tools -&gt; Global Options.\n\nUnclick “Restore .RData into workspace at startup”. This means that each time you start a new RStudio instance, your environment starts clean and you can be confident that any results you have, or weird R behaviour, is not due to any previous code or session.\n\n\n\n\nDo not restore Rdata\n\n\n\nUnclick “Restore most recently opened project at startup”. This means that when you start a new RStudio, it will open an RStudio not associated with a project. This is more of a personal preference of mine as more often than not I have multiple RStudio instances open not associated with the same work, and I got tired of closing the project of new RStudio instances.\n\n\n\n\nDo not restore most recent project\n\n\n\nIn the “Code” tab, tick “Soft-wrap R source files”. This means that instead of having long lines of code that go off of your source pane and require you to move from left-to-right to view them, the source pane instead “soft-wraps” them into a new line, while preserving the fact that they are, in fact, a single line of code.\n\n\n\n\nTurn on Soft wrap\n\n\n\nIn the “Code -&gt; Display” tab, unclick “Show Margin” which will get rid of the annoying line that can be present in the Source pane sometimes.\n\n\n\n\nTurn off Show margin\n\n\n\nI leave the default colours on for RStudio, but you can change to black background (and loads of other alternatives) in the “Appearance” tab."
  },
  {
    "objectID": "posts/install_rstudio_git/index.html#managing-where-your-r-packages-install",
    "href": "posts/install_rstudio_git/index.html#managing-where-your-r-packages-install",
    "title": "intRos: Installing, R, RStudio, and Git",
    "section": "Managing where your R packages install",
    "text": "Managing where your R packages install\nOver many years, I have helped a fair few people at the University try and work out how to install R packages on their University computers. This has mainly been on Windows machines, and is most likely due to the lack of read/write access wherever R by default chooses to install packages.\nConsequently, it might be good for you to choose a new location on your computer where your R packages will install. And if you cannot install R packages at the moment then we are here to help! You can learn how to do change the default location your R packages install into by watching this video, but we will whizz through the process here too.\n\nCheck where R is installing packages by typing .libPaths() into your R console.\nCreate a new folder which you would like to install packages, somewhere where you DEFINITELY have read/write access (e.g. “~/Desktop/r_packages”)\nWe can then change .libPaths() for this R session.\n\n\n\n\nChange where you R packages install in an individual session\n\n\n\nHowever, when we start a new R session, this change will be lost. To change it by default for R, we need to change our .Renviron file, which allows us to change variables that R looks for every time it starts. To easily manipulate this file, we will first install the R package usethis using install.packages(usethis). It should install into the new directory you have just created, for me it installed into “~/Desktop/r_packages”.\nRun usethis::edit_r_environ() in your Console and it should open your .Renviron file in your source pane. From here, add R_LIBS_USER=“~/Desktop/r_packages” to it (obviously change the folder name to the folder you have created), click Save, and close the file.\n\n\n\n\nRun usethis::edit_r_environ\n\n\n\nNow when you start a new R instance, your chosen folder should be present when you run .libPaths() and you should not have problems installing R packages anymore!\n\n\n\n\n.libPaths() changes by default"
  },
  {
    "objectID": "posts/install_rstudio_git/index.html#for-mac-users-1",
    "href": "posts/install_rstudio_git/index.html#for-mac-users-1",
    "title": "intRos: Installing, R, RStudio, and Git",
    "section": "For Mac Users",
    "text": "For Mac Users\nGit can be installed on Mac from some of the options on the git website. Another alternative is GitHub Desktop, which will install the latest version of Git with itself if you do not already have it."
  },
  {
    "objectID": "posts/install_rstudio_git/index.html#for-windows-users-1",
    "href": "posts/install_rstudio_git/index.html#for-windows-users-1",
    "title": "intRos: Installing, R, RStudio, and Git",
    "section": "For Windows Users",
    "text": "For Windows Users\nGit can be installed from gitforwindows, but if you install GitHub Desktop, the latest version of Git will be installed if you do not already have it. With GitHub Desktop, you get a command line version of Git with a robust GUI."
  },
  {
    "objectID": "posts/install_rstudio_git/index.html#make-a-github-profile",
    "href": "posts/install_rstudio_git/index.html#make-a-github-profile",
    "title": "intRos: Installing, R, RStudio, and Git",
    "section": "Make a GitHub profile",
    "text": "Make a GitHub profile\nGit is version control on your local machine, to share code with other you should create a GitHub (or GitLab) profile. Information on setting up a GitHub account can be found here."
  },
  {
    "objectID": "posts/install_rstudio_git/index.html#configure-github-with-local-git-and-rstudio",
    "href": "posts/install_rstudio_git/index.html#configure-github-with-local-git-and-rstudio",
    "title": "intRos: Installing, R, RStudio, and Git",
    "section": "Configure GitHub with local git and RStudio",
    "text": "Configure GitHub with local git and RStudio\nNow we have git, R, and RStudio installed, and an RStudio profile, we need to make sure they can all talk each other, and that you can communicate with your GitHub profile from your local machine. An extensive walkthrough is available here, but we will cover the main steps.\nFirst, we need to create an access token so that GitHub can securely link your local computer to GitHub. To do this go to Settings -&gt; Developer Settings -&gt; Tokens and create an access token.\nFor the scopes, I ticked most that I thought would be useful and understood, which were gist, repo, user, workflow. Make sure you name it something useful like “R:GitHubPAT” so that is makes sense if you ever need to make a new token.\nOnce this code is created, save it to your clipboard or in a text editor while we are storing it in your RStudio. Once you close the webpage you will never see this token again, so make sure you make a note of it! Then going forwards, treat this access token as you would a password. For some of you that might mean using a password manager, and for some of you that might mean a book, your hand, or post it notes on your computer screen.\nWe will use the R package gitcreds to associate your GitHub PAT with RStudio. First install gitcreds using install.packages(‘gitcreds’). If you want a way to be able to install and load R packages from CRAN, GitHub, and Bioconductor in a simpler way without calls to both install.packages() and library(), then I recommend looking at librarian!\nNext run gitcreds::gitcreds_set() and follow the prompts. Specifically make sure you copy in your GitHub PAT!\n\n\n\ngitcreds_set()\n\n\nOnce you have set this, you can check it has stored properly using gitcreds::gitcreds_get().\nThis should mean you are ready to roll (i.e. Git and GitHub like a boss), but if you are getting stuck some common troubleshoots can be found here.\nOne thing you can check is that RStudio knows where your git executable is. In Tools -&gt; Global Options, you can check where git is stored and being accessed from.\n\n\n\nCheck where RStudio is looking for git"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Exeter Data Analytics Hub: Promoting Open and Reproducible Science",
    "section": "",
    "text": "Welcome to the homepage of the Exeter Data Analytics Hub."
  },
  {
    "objectID": "index.html#what-is-the-exeter-data-analytics-hub",
    "href": "index.html#what-is-the-exeter-data-analytics-hub",
    "title": "Exeter Data Analytics Hub: Promoting Open and Reproducible Science",
    "section": "What is the Exeter Data Analytics Hub?",
    "text": "What is the Exeter Data Analytics Hub?\nWe are an inter-college team of volunteers at the University of Exeter aiming to enhance ECR training in open and reproducible science."
  },
  {
    "objectID": "index.html#open-and-reproducible-science-series",
    "href": "index.html#open-and-reproducible-science-series",
    "title": "Exeter Data Analytics Hub: Promoting Open and Reproducible Science",
    "section": "Open and Reproducible Science Series",
    "text": "Open and Reproducible Science Series\nWe host a monthly series of workshops and talks to promote Open and Reproducible Science throughout the life-cycle of a research project.\nVisit the Series page to find out more."
  },
  {
    "objectID": "index.html#intros-walkthroughs",
    "href": "index.html#intros-walkthroughs",
    "title": "Exeter Data Analytics Hub: Promoting Open and Reproducible Science",
    "section": "intRos Walkthroughs",
    "text": "intRos Walkthroughs\nFind brief introductions and walkthroughs on common research programming activities on the Walkthroughs page."
  },
  {
    "objectID": "index.html#old-website-content",
    "href": "index.html#old-website-content",
    "title": "Exeter Data Analytics Hub: Promoting Open and Reproducible Science",
    "section": "Old website content",
    "text": "Old website content\nThere are a bunch of excellent resources made by previous members of the University of Exeter. These were hosted on a previous version of this website.\n\nIntroduction to R by TJ McKinley\nAdvanced Visualisation and Data Wrangling in R [Data] [Slides] by TJ McKinley\nOpen Science and Reproducible Research in R by Diego Barneche\nR Scripting and R Markdown [Data and scripts] by TJ McKinley\nStatistical Modelling in R [Data] [Slides] by JJ Valletta and TJ McKinley\nMachine Learning [Data] [Slides][Slides2019] by Chris Yeomans and Jiangjiao Xu\nSpatial Data Analysis [Data] [Slides] by D March, James Duffy & Chris Yeomans\nImageJ GUI\nImageJ Macros\nIntroduction to Python\nPython for Data Analysis"
  },
  {
    "objectID": "index.html#contact-us",
    "href": "index.html#contact-us",
    "title": "Exeter Data Analytics Hub: Promoting Open and Reproducible Science",
    "section": "Contact us",
    "text": "Contact us\nIf you want to help contribute to this project or have any questions about any of our content, please do not hesitate to contact us."
  },
  {
    "objectID": "presentations/ggplot2_flextable/ggplot2_flextable.html",
    "href": "presentations/ggplot2_flextable/ggplot2_flextable.html",
    "title": "Reproducible publication-ready plots and tables with ggplot2 and flextable",
    "section": "",
    "text": "This presentation was recorded. A recording can be found here for students and staff at the University of Exeter.\nSlides can be found here."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "intRos: Walkthroughs",
    "section": "",
    "text": "The intRos walkthroughs are designed to be a place to provide brief introductions and walkthroughs for activities that are common when programming for research purposes. For example, using ggplot2 and making publication-ready plots. Or making reproducible tables in R using flextable. These intRos are aimed to be brief and designed to get you started using these tools. Throughout we highlight where we find information and our favourite resources for when we get stuck!\nWe hope that intRos will be added to through time by other members of the University. If you would like to contribute, please get in touch."
  },
  {
    "objectID": "posts.html#what-are-intros",
    "href": "posts.html#what-are-intros",
    "title": "intRos: Walkthroughs",
    "section": "",
    "text": "The intRos walkthroughs are designed to be a place to provide brief introductions and walkthroughs for activities that are common when programming for research purposes. For example, using ggplot2 and making publication-ready plots. Or making reproducible tables in R using flextable. These intRos are aimed to be brief and designed to get you started using these tools. Throughout we highlight where we find information and our favourite resources for when we get stuck!\nWe hope that intRos will be added to through time by other members of the University. If you would like to contribute, please get in touch."
  },
  {
    "objectID": "posts.html#get-started",
    "href": "posts.html#get-started",
    "title": "intRos: Walkthroughs",
    "section": "Get started",
    "text": "Get started\nAll of the available intRos can be found below. All of the reproducible quarto (.qmd) files can be found at the source GitHub repository for this website. You can navigate to each of the directories individually in the posts/ folder, or better still clone the repo using git clone and open the project in RStudio."
  },
  {
    "objectID": "posts/github_copilot_integration/index.html",
    "href": "posts/github_copilot_integration/index.html",
    "title": "Integrating GitHub Copilot with RStudio and VScode",
    "section": "",
    "text": "Outline\nGitHub Copilot an AI coding assistant that can be ran directly within your coding environment (e.g. RStudio or VSCode). These tools should be used with care, and you still need to understand what the code does as you are ultimately responsible for your code and its output.\nThis walkthrough will provide the necessary steps to add Copilot to RStudio and VSCode.\n\n\nPrerequisites\nThere are two ways of getting GitHub Copilot for free. The first is to use GitHub Copilot Free which gives you 2000 autocompletions a month.\nHowever, the method we recommend is to get GitHub Copilot Pro free as a student or teacher. This method will also allow you to use GitHub Copilot on the RStudio servers. This susbscription gives you unlimited access to GitHub Copilot while you are a student or researcher at the University.\nTo do this, you will need to request GitHub Education benefits using the following steps:\n\nSign up for a GitHub account (ideally using your university account)\nFrom your new GitHub account, apply for free GitHub Education benefits (i.e., upload photo of your Unicard)\nOnce GitHub Education email to say your application is approved, head to https://github.com/settings/copilot and click the Start free trial button\nThe next screen will have a green button to Get access to GitHub Copilot\nFinally, you just have to check policies (for additional details see here) and click Save and complete setup\nYou’re now ready to set up GitHub Copilot!\n\n\n\nSetting up Copilot in RStudio\n\nTo set up the GitHub Copilot for RStudio, we navigate to Tools &gt; Global Options &gt; Copilot (see image below)\nFrom the Copilot tab, check the Enable Copilot box and hit Apply\nYou should then see a You are not currently signed in message below the checkbox. Click the Sign in button.\nCopy the 8-digit code, click the link, select Continue and paste the code\nCopilot should show as activated (if not click the Activate button in Copilot options)\nCopilot will now provide suggestions for code as you type (you can see status in bottom right of the code window)\n\n\n\n\nSetting up Copilot for Visual Studio Code\n\nOn VSCode we just have to install the GitHub Copilot extension\nOnce installed, we open the Copilot extension and use it to sign into our GitHub account\nCopilot will now give you suggestions as you type, and you can also use the additional Copilot chat feature (messages icon on left of window) to open a dialogue\n\n\n\n\nGeneral Copilot usage and pointers\nNow we have Copilot ready to provide suggestions for our code in our chosen software!\n\nTo approve Copilot suggestions that appear, press Tab\nTo give Copilot guidance on code suggestions, provide #code comments that describe steps required\nMake sure to check both the generated code itself and sense-check outputs to ensure that it is doing what is expected\nThe Copilot chat extension (installed automatically alongside Copilot) can be used for a broader range of tasks – see examples here\nIf you also use the Tab autocompletion capability of RStudio or VSCode to navigate files or autocomplete functions, sometimes Copilot’s suggestion comes up and Tab approves Copilot rather than the autocompletion. To stop this, you can play around with the Show code suggestions after keyboard idle (ms) in the Settings.\n\n\n\nAdditional Resources\n\nResponsible use of GitHub Copilot Chat in your IDE\n\n\n\nAcknowledgements\nWe did not create this content alone! Inspiration, tips, and resources have been borrowed from multiple sources.\n\nGitHub Copilot documentation"
  },
  {
    "objectID": "posts/intro_to_research_computing/index.html",
    "href": "posts/intro_to_research_computing/index.html",
    "title": "intRos: Research Computing",
    "section": "",
    "text": "These workshops are a valuable opportunity for you to up-skill yourself with tools of the future — please do join us if you think these workshops might apply to you!"
  },
  {
    "objectID": "posts/intro_to_research_computing/index.html#what-is-this-about",
    "href": "posts/intro_to_research_computing/index.html#what-is-this-about",
    "title": "intRos: Research Computing",
    "section": "",
    "text": "These workshops are a valuable opportunity for you to up-skill yourself with tools of the future — please do join us if you think these workshops might apply to you!"
  },
  {
    "objectID": "posts/intro_to_research_computing/index.html#why-should-i-learn-this",
    "href": "posts/intro_to_research_computing/index.html#why-should-i-learn-this",
    "title": "intRos: Research Computing",
    "section": "Why should I learn this?",
    "text": "Why should I learn this?\n\n“The revolution is here. Understanding how Research Computing works and being able to apply this to your work-stream could help your work and increase your impact. If you are all at sea with Research Computing then this is where to start.”\n\nThe key goals of these workshops are to remove barriers to people using Research Computing and up-skill our research community to be able to make better use of the facilities available."
  },
  {
    "objectID": "posts/intro_to_research_computing/index.html#ok-how-do-i-sign-up",
    "href": "posts/intro_to_research_computing/index.html#ok-how-do-i-sign-up",
    "title": "intRos: Research Computing",
    "section": "OK, how do I sign-up?",
    "text": "OK, how do I sign-up?\nThe current sign-up period has closed now. However, please feel free to fill up this form to express your interest in these workshops and we will look into arranging additional sessions.\n\nDay 1 - An Introduction to Research Computing\nOutline: We will deliver “An introduction to Research Computing”, which will be followed by examples of specific case studies based on the research needs of those of you who filled in the questionnaire. To aid in developing ARC skillsets there will be a worked example given to attendees to work through in preparation for the second session.\n\n\n\n\n\n\nNote\n\n\n\nSkill level: Foundational to intermediate\nPrerequisites: None\nWhen and where: 0900-1300 10th June Exchange Red\nCatering: Drinks and lunch provided.\n\n\n\n\nDay 2 - Research Computing Case Studies and Problem Solving\nOutline: Firstly, attendees will discuss best practices to overcome some of the challenges they identified in the earlier workshop. Secondarily, attendees will build upon the skills acquired within the self-learning exercise and use these skills to develop basics of two key types of research computing - virtual and high-performance for their specific research needs.\n\n\n\n\n\n\nNote\n\n\n\nSkill level: Foundational to intermediate\nPrerequisites: 1) A laptop, and 2) Completion of the worked example provided in the first meeting.\nWhen and where: 0900-1300 17th June Exchange Green\nCatering: Drinks and lunch provided.\n\n\n\n\nAdditional Resources\nMany respondents, especially early-career researchers, expressed a desire to improve their computing skills or learn how to use advanced reseach computing resources effectively.\n\n\n\n\n\n\nNote\n\n\n\n\nCheck out our tips here!\nTake advantage of online resources: Explore platforms like CornwallARC and ExeterARC for institution specific knowledge bases. Engage with discipline specific training sessions offered by ExeDataHub or CfRR - these are essential to gain hands-on experience with tools and techniques. Other online resources like Coursera, edX, or DataCamp can also be very useful for programming and data analysis learning.\nAsk for Help: Join the ExeDataHub Ask for Help channel to ask questions, share knowledge, and learn from peers. Active engagement with this resource is essential for the community to thrive.\n\n\n\n\n\nAcknowledgements\nThe inaugural set of workshops has been funded by a Researcher Led Initiative grant to Dr. Tom Horton, in collaboration with Dr. M.D. Sharma and Dr. Stephen Lang at the University of Exeter (Penryn Campus)."
  },
  {
    "objectID": "posts/intro_to_research_computing/tips.html",
    "href": "posts/intro_to_research_computing/tips.html",
    "title": "intRos: Research Computing - Tips",
    "section": "",
    "text": "Several common themes and issues have emerged within the responses collected from researchers regarding their computing needs and challenges. Below are some valuable tips designed to address these challenges and improve the efficiency and effectiveness of research-related computing tasks. These tips are tailored to the diverse needs of the respondents, who span various career stages (e.g., postgraduates, postdocs, lecturers) and research fields (e.g., genomics, deep learning, geospatial analysis), while remaining broadly applicable to enhance their use of advanced computing resources.\n\n\nMany researchers are working with computationally intensive tasks, such as simulations and machine learning models, which can benefit from optimized code.\n\nProfile your code: Use profiling tools (e.g., cProfile in Python or profvis in R) to identify bottlenecks and focus optimization efforts where they’ll have the most impact.\nUse efficient data structures: Opt for memory- and speed-efficient options, such as NumPy arrays instead of Python lists, to handle large datasets effectively.\nVectorize operations: Leverage vectorized operations in Python (e.g., with NumPy or Pandas) or R (e.g., avoiding loops with apply functions) to speed up computations by minimizing iterative processing.\n\n\n\n\nSeveral respondents highlighted the need for faster processing of large datasets or simulations, making parallel computing a key skill to develop.\n\nLearn parallel computing basics: Understand multi-threading and multi-processing to distribute tasks across multiple cores, reducing computation time.\nUse built-in libraries: Implement libraries like multiprocessing in Python or parallel in R to parallelize tasks on your local machine efficiently.\nScale with advanced tools: For larger-scale parallelism, explore frameworks like Dask or Apache Spark, which can distribute computations across clusters, especially when using cloud based resources.\n\n\n\n\nResearchers frequently mentioned needing significant computational power and storage, which ARC can provide if used effectively.\n\nFamiliarize yourself with the system: Learn the basics of your HPC platform, including how to submit jobs, monitor resource usage (e.g., CPU, memory), and manage data transfers. Bespoke, community contributed tutorials are available for the Athena HPC on the CornwallARC site. Similar tutorials are being developed for ISCA HPC by the RIT team on the ExeterARC site.\nOptimize job submissions: Use job schedulers like SLURM to request appropriate resources (e.g., number of CPUs, memory, runtime) tailored to your task, avoiding over- or under-allocation. Over- or under- allocation can be detrimental to all users on shared systems.\nTest locally first: Where possible, run smaller versions of your code on your local machine or a virtual machine to debug and optimize your code before scaling up to larger, shared resources (like Athena or ISCA HPC or the RStudio servers), minimizing trial-and-error on the server.\n\n\n\n\nMemory crashes and handling large datasets were recurring challenges, especially for those working with genomics, geospatial data, or machine learning. The key here is to know your data and be aware of the basics of data-handling within the software you are using - then you can try to optimse appropriately.\n\nUse memory-efficient data types: Reduce memory usage by choosing appropriate data types (e.g., float32 instead of float64 in NumPy) for your variables.\nLoad data in chunks: Process large files incrementally or use streaming techniques to avoid loading entire datasets into memory at once.\nConsider specialized formats: Adopt formats like HDF5 or NetCDF for efficient storage and retrieval of large scientific datasets, which are well-suited for HPC environments.\nConsider appropriate compute platforms: When using bespoke servers or HPC systems, look for the resources available on different machines and choose the most appropriate server or queue to submit your jobs to. For e.g. if you have a large amount of data that needs to be read-in prior to compute - you may want to select a high-memory server (or queue) instead.\n\n\n\n\nEffective data organization is crucial for researchers dealing with large and dynamic datasets, such as sequencing or geospatial data.\n\nOrganize your data: Maintain clear directory structures and consistent file-naming conventions to easily locate and manage files. Avoid using spaces or special-characters in your file-naming conventions.\nBackup regularly: Use institutional storage solutions or cloud backups to protect critical data and code from loss. Remember that the storage on compute platforms is finite and should not be used for short- or long-term data dumping. These spaces are usually not backed-up – your data, is your responsibility.\nUse databases for large datasets: For complex or live data (e.g., vessel tracking), consider using databases like PostgreSQL to query and manage data efficiently. However, these may need a combination of computational resources for your work-flow (e.g. a virtual machine for the database, while the computation happens on another server).\n\n\n\n\nResearchers use a variety of tools (e.g., Python, R, Matlab, GIS software), and maximizing their potential can streamline workflows.\n\nRead documentation: Consult official documentation or user guides for your software and libraries to understand their full capabilities and best practices. This is one of the core essentials that is ignored by researchers - Read the Friendly Manual!\nFollow best practices: Seek out tutorials specific to your research area (e.g., bioinformatics pipelines in Python, geospatial analysis in R) to optimize your approach.\nStay updated: Keep software and libraries current to benefit from performance enhancements and new features, especially on shared research computing systems.\n\n\n\n\nCollaboration and reproducibility are key in research, embrace these good-practice measures in your daily scientific computing workflows.\n\nUse version control: Employ tools like Git (with platforms like GitHub or GitLab) to track code changes and collaborate with others seamlessly.\nShare data responsibly: Deposit data-sets in institutional repositories (e.g. ORE) or platforms like Figshare, Dryad or Open Science Framework, ensuring proper attribution and access control.\nDocument your work: Add clear comments to your code and maintain documentation (e.g., README files) to make your workflows understandable and reproducible.\n\n\n\n\nThe inaugural set of workshops has been funded by a Researcher Led Initiative grant to Dr. Tom Horton, in collaboration with Dr. M.D. Sharma and Dr. Stephen Lang at the University of Exeter (Penryn Campus)."
  },
  {
    "objectID": "posts/intro_to_research_computing/tips.html#what-is-this-about",
    "href": "posts/intro_to_research_computing/tips.html#what-is-this-about",
    "title": "intRos: Research Computing - Tips",
    "section": "",
    "text": "Several common themes and issues have emerged within the responses collected from researchers regarding their computing needs and challenges. Below are some valuable tips designed to address these challenges and improve the efficiency and effectiveness of research-related computing tasks. These tips are tailored to the diverse needs of the respondents, who span various career stages (e.g., postgraduates, postdocs, lecturers) and research fields (e.g., genomics, deep learning, geospatial analysis), while remaining broadly applicable to enhance their use of advanced computing resources.\n\n\nMany researchers are working with computationally intensive tasks, such as simulations and machine learning models, which can benefit from optimized code.\n\nProfile your code: Use profiling tools (e.g., cProfile in Python or profvis in R) to identify bottlenecks and focus optimization efforts where they’ll have the most impact.\nUse efficient data structures: Opt for memory- and speed-efficient options, such as NumPy arrays instead of Python lists, to handle large datasets effectively.\nVectorize operations: Leverage vectorized operations in Python (e.g., with NumPy or Pandas) or R (e.g., avoiding loops with apply functions) to speed up computations by minimizing iterative processing.\n\n\n\n\nSeveral respondents highlighted the need for faster processing of large datasets or simulations, making parallel computing a key skill to develop.\n\nLearn parallel computing basics: Understand multi-threading and multi-processing to distribute tasks across multiple cores, reducing computation time.\nUse built-in libraries: Implement libraries like multiprocessing in Python or parallel in R to parallelize tasks on your local machine efficiently.\nScale with advanced tools: For larger-scale parallelism, explore frameworks like Dask or Apache Spark, which can distribute computations across clusters, especially when using cloud based resources.\n\n\n\n\nResearchers frequently mentioned needing significant computational power and storage, which ARC can provide if used effectively.\n\nFamiliarize yourself with the system: Learn the basics of your HPC platform, including how to submit jobs, monitor resource usage (e.g., CPU, memory), and manage data transfers. Bespoke, community contributed tutorials are available for the Athena HPC on the CornwallARC site. Similar tutorials are being developed for ISCA HPC by the RIT team on the ExeterARC site.\nOptimize job submissions: Use job schedulers like SLURM to request appropriate resources (e.g., number of CPUs, memory, runtime) tailored to your task, avoiding over- or under-allocation. Over- or under- allocation can be detrimental to all users on shared systems.\nTest locally first: Where possible, run smaller versions of your code on your local machine or a virtual machine to debug and optimize your code before scaling up to larger, shared resources (like Athena or ISCA HPC or the RStudio servers), minimizing trial-and-error on the server.\n\n\n\n\nMemory crashes and handling large datasets were recurring challenges, especially for those working with genomics, geospatial data, or machine learning. The key here is to know your data and be aware of the basics of data-handling within the software you are using - then you can try to optimse appropriately.\n\nUse memory-efficient data types: Reduce memory usage by choosing appropriate data types (e.g., float32 instead of float64 in NumPy) for your variables.\nLoad data in chunks: Process large files incrementally or use streaming techniques to avoid loading entire datasets into memory at once.\nConsider specialized formats: Adopt formats like HDF5 or NetCDF for efficient storage and retrieval of large scientific datasets, which are well-suited for HPC environments.\nConsider appropriate compute platforms: When using bespoke servers or HPC systems, look for the resources available on different machines and choose the most appropriate server or queue to submit your jobs to. For e.g. if you have a large amount of data that needs to be read-in prior to compute - you may want to select a high-memory server (or queue) instead.\n\n\n\n\nEffective data organization is crucial for researchers dealing with large and dynamic datasets, such as sequencing or geospatial data.\n\nOrganize your data: Maintain clear directory structures and consistent file-naming conventions to easily locate and manage files. Avoid using spaces or special-characters in your file-naming conventions.\nBackup regularly: Use institutional storage solutions or cloud backups to protect critical data and code from loss. Remember that the storage on compute platforms is finite and should not be used for short- or long-term data dumping. These spaces are usually not backed-up – your data, is your responsibility.\nUse databases for large datasets: For complex or live data (e.g., vessel tracking), consider using databases like PostgreSQL to query and manage data efficiently. However, these may need a combination of computational resources for your work-flow (e.g. a virtual machine for the database, while the computation happens on another server).\n\n\n\n\nResearchers use a variety of tools (e.g., Python, R, Matlab, GIS software), and maximizing their potential can streamline workflows.\n\nRead documentation: Consult official documentation or user guides for your software and libraries to understand their full capabilities and best practices. This is one of the core essentials that is ignored by researchers - Read the Friendly Manual!\nFollow best practices: Seek out tutorials specific to your research area (e.g., bioinformatics pipelines in Python, geospatial analysis in R) to optimize your approach.\nStay updated: Keep software and libraries current to benefit from performance enhancements and new features, especially on shared research computing systems.\n\n\n\n\nCollaboration and reproducibility are key in research, embrace these good-practice measures in your daily scientific computing workflows.\n\nUse version control: Employ tools like Git (with platforms like GitHub or GitLab) to track code changes and collaborate with others seamlessly.\nShare data responsibly: Deposit data-sets in institutional repositories (e.g. ORE) or platforms like Figshare, Dryad or Open Science Framework, ensuring proper attribution and access control.\nDocument your work: Add clear comments to your code and maintain documentation (e.g., README files) to make your workflows understandable and reproducible.\n\n\n\n\nThe inaugural set of workshops has been funded by a Researcher Led Initiative grant to Dr. Tom Horton, in collaboration with Dr. M.D. Sharma and Dr. Stephen Lang at the University of Exeter (Penryn Campus)."
  },
  {
    "objectID": "posts/intro_to_openstack/manage.html",
    "href": "posts/intro_to_openstack/manage.html",
    "title": "Managing Openstack",
    "section": "",
    "text": "Managing Your OpenStack Instances\nThis section contains some instructions on how to manage you OpenStack instances.\nWhen you’re not working on the course it is important and considerate to turn off your instance to avoid using unnecessary resource. If your instance is running, even if you are not doing anything on it, it will be using resource and may hinder others using the resources.\n\nFind your instance:\nLog onto the OpenStack console and find your instance. Click on Compute - Instances and if you cannot find your instance easily, use the filter button.\n\n\n\nsearch for your instance\n\n\n\n\nStopping Your Instance:\nIf you wish to keep your data, use the ‘Shut Off instance’ option. In this state the data written to disk will be preserved, but the instance will be ‘off’. It is the equivalent of turning off a computer.\nUse the dropdown next to your instance and select “Shut Off Instance” (near the bottom)\n\nYou will be asked to confirm - Double check it is the instance you intend to shut down.\n\n\nRestarting Your Instance:\nIf the status of your instance is Shutoff - click Start Instance.\nDouble check the IP address of your instance as it may change - you will need to use the new IP address to connect to you instance. In X2Go click on the drop down arrow to get the menu of options.\n\n\n\nDeleting an instance.\n\n\n\n\n\n\nCaution\n\n\n\nNote: this will destroy all work done to date.\n\n\nWhen you’re completely finished done with the workshop, click the drop-down menu next to your instance and select ‘Delete Instance’. It will ask you to confirm - You can then watch the status change from ‘shutting down’ to Deleted’.\n\n\n\n\n\nAcknowledgements\nWe did not create this content alone! Inspiration, tips, and resources have been borrowed from multiple sources. A large part of this material has been borrowed from an earlier workshop written up by the Exeter Biomedical Hub."
  },
  {
    "objectID": "posts/intro_to_openstack/index.html",
    "href": "posts/intro_to_openstack/index.html",
    "title": "intRos: Introduction to OpenStack",
    "section": "",
    "text": "Note\n\n\n\nBy the end of this workshop you will be expected to:\n\nLog into the UoE OpenStack Console and start your instance of the appropriate workshop.\nLog in to the OpenStack instance from your own computer.\nBe ready to start the tutorials at your own pace."
  },
  {
    "objectID": "posts/intro_to_openstack/01_OpenStack_Console.html",
    "href": "posts/intro_to_openstack/01_OpenStack_Console.html",
    "title": "OpenStack Console",
    "section": "",
    "text": "In addition to being extremely comprehensive, the Exeter OpenStack cloud has a relatively easy (but detailed!) interface for interacting with its offerings. All you have to do is log in via a web-browser and most of the functionality of the is available for you and relatively easy to use.\nTo get started go to the following URL and login with your University of Exeter username (in the format of user123) the domain (exeter.ac.uk), and the password you use when logging onto your university resources.\n\n\n\n\n\n\nWarning\n\n\n\nWe have set up a single workshop project which we will be using for training. Please do NOT abuse this project and do NOT put any personal, confidential or important data on the machines you will set-up during the training. You will all log in to the same management console and will be able to see machines which your friends and colleagues on the course will start.\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThis resource is only accessible within the University of Exeter network. So, if you are accessing this workshop from home or anywhere outside the campuses, you will need to connect to the University VPN service to be able to access the system.\n\n\nOnce logged in, you will see a web-page that looks similar to this:\n\nAs a first step - let’s associate ourselves with the correct project. This is specially important for all users who may have access to multiple “projects” associated with their research groups. In this instance, please click on the Project dropdown and select ARC_Intro_Penryn - Workshop as your project. If done correctly, you should see a tick mark next to the selected project, as illustrated below.\n\nWe can now take a look at the various tabs. These include:\n\n\n\n\n\n\nInstances\n\n\n\nThis is the list of virtual machines that have been created.\n\n\n\n\n\n\n\n\nCompute\n\n\n\nThe service OpenStack is known for. It enables you to create Linux and Windows Virtual machines. Best of all their computational and/or disk capacity can be increased or decreased at the click of a button! This section has most of the things we need for this course.\n\n\n\n\n\n\n\n\nVolumes\n\n\n\nA storage unit. These can be created and attached to any compute which you have running. Ideal for moving large amounts of data between machines. Think of it as a removal hard drive which you can attach to any machine.\n\n\n\n\n\n\n\n\nImages\n\n\n\nThese are the templates from which you create instances.\n\n\n\n\n\n\n\n\nObject store\n\n\n\nThis is another type of storage, but is relatively slow. It is ideal for archive or when storing large files (i.e. several Gb).However, it is currently disabled on the system. if you want to know more about it, please as your Research IT support team for assistance.\n\n\nClick on “Compute” and then “Overview” at the top left.\nOn this page you’ll get a summary of the state for your account. You can see an example below. Note that we are using a shared project where you’ll also see everyone else’s instances. If you were using your own project (or your reserch group’s project), you would only see your instances (or that of your research group).\n\nYou see here that we have a certain allocation of resources (memory, disk space, CPUs). We can choose how to allocate these to accomplish our scientific goals. For instance, we may need one very large machine with lots of resource to do a single task (e.g. a genome assembly). Or we may want lots of smaller machines, each doing a small task (e.g. a webserver and a database server). We may want half the machines running Unix and the other half running Windows. The point is, we are in control of the compute and can choose what suits us best and it can all be done, by us at the touch of a button.\nIn the example above, we have been allocated up-to 32 instances and a total of 32 cores with roughly 60GB of RAM so that each of the workshop attendees can spin up a virtual machine with 1vCPU and 1GB of RAM.\nClick on “Compute” and then “Instances” at the top left. When this tutorial was created, there were no existing instances within the workshop project, so, those of you following this tutorial within the second workshop, will see a page with No items to display and anyone who sign in after a few instances have been created, would see a list of instances that pre-exist at that time.\n\n\nOk, so how do we actually do all this? Well, from this page we can create our own servers on Exeter’s OpenStack using an image of a machine the Research IT team have created earlier for you. Servers which are created in this manner are called ‘Instances’. We can create as many servers as we like, start them, log-in to them, do some work, transfer data to/from them or destroy them altogether. We let the Research IT team worry about the hardware, power, cooling and maintenance - all we need to do is specify how powerful a computer we want (tiny, small, medium, large or extra-large).\nThe reason we are using the OpenStack here is that many of you have to analyse large datasets. But you only need to cruch those datasets occasionally. In the case of high-throughput sequencing data (e.g. Illumina or Oxford Nanopore) you will find that your desktop PC may not be powerful enough to cope with the data. Also installing and configuring many pieces of software (often written by other scientists), is frequently painful as they often require other programs (also often written by other scientists) to be installed as well. Factor in some obscure incompatibility and you can quickly end up in a situation where two programs that you need refuse to co-exist. With OpenStack, you can just start a server instance based on an image created by someone else who has already done all the hard work of installation and configuration. You may also want to use an existing docker- or singularity- container within such an instance to carry out your workflow.\nSo let’s get on and launch our first instance!"
  },
  {
    "objectID": "posts/intro_to_openstack/01_OpenStack_Console.html#logging-into-the-console",
    "href": "posts/intro_to_openstack/01_OpenStack_Console.html#logging-into-the-console",
    "title": "OpenStack Console",
    "section": "",
    "text": "In addition to being extremely comprehensive, the Exeter OpenStack cloud has a relatively easy (but detailed!) interface for interacting with its offerings. All you have to do is log in via a web-browser and most of the functionality of the is available for you and relatively easy to use.\nTo get started go to the following URL and login with your University of Exeter username (in the format of user123) the domain (exeter.ac.uk), and the password you use when logging onto your university resources.\n\n\n\n\n\n\nWarning\n\n\n\nWe have set up a single workshop project which we will be using for training. Please do NOT abuse this project and do NOT put any personal, confidential or important data on the machines you will set-up during the training. You will all log in to the same management console and will be able to see machines which your friends and colleagues on the course will start.\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThis resource is only accessible within the University of Exeter network. So, if you are accessing this workshop from home or anywhere outside the campuses, you will need to connect to the University VPN service to be able to access the system.\n\n\nOnce logged in, you will see a web-page that looks similar to this:\n\nAs a first step - let’s associate ourselves with the correct project. This is specially important for all users who may have access to multiple “projects” associated with their research groups. In this instance, please click on the Project dropdown and select ARC_Intro_Penryn - Workshop as your project. If done correctly, you should see a tick mark next to the selected project, as illustrated below.\n\nWe can now take a look at the various tabs. These include:\n\n\n\n\n\n\nInstances\n\n\n\nThis is the list of virtual machines that have been created.\n\n\n\n\n\n\n\n\nCompute\n\n\n\nThe service OpenStack is known for. It enables you to create Linux and Windows Virtual machines. Best of all their computational and/or disk capacity can be increased or decreased at the click of a button! This section has most of the things we need for this course.\n\n\n\n\n\n\n\n\nVolumes\n\n\n\nA storage unit. These can be created and attached to any compute which you have running. Ideal for moving large amounts of data between machines. Think of it as a removal hard drive which you can attach to any machine.\n\n\n\n\n\n\n\n\nImages\n\n\n\nThese are the templates from which you create instances.\n\n\n\n\n\n\n\n\nObject store\n\n\n\nThis is another type of storage, but is relatively slow. It is ideal for archive or when storing large files (i.e. several Gb).However, it is currently disabled on the system. if you want to know more about it, please as your Research IT support team for assistance.\n\n\nClick on “Compute” and then “Overview” at the top left.\nOn this page you’ll get a summary of the state for your account. You can see an example below. Note that we are using a shared project where you’ll also see everyone else’s instances. If you were using your own project (or your reserch group’s project), you would only see your instances (or that of your research group).\n\nYou see here that we have a certain allocation of resources (memory, disk space, CPUs). We can choose how to allocate these to accomplish our scientific goals. For instance, we may need one very large machine with lots of resource to do a single task (e.g. a genome assembly). Or we may want lots of smaller machines, each doing a small task (e.g. a webserver and a database server). We may want half the machines running Unix and the other half running Windows. The point is, we are in control of the compute and can choose what suits us best and it can all be done, by us at the touch of a button.\nIn the example above, we have been allocated up-to 32 instances and a total of 32 cores with roughly 60GB of RAM so that each of the workshop attendees can spin up a virtual machine with 1vCPU and 1GB of RAM.\nClick on “Compute” and then “Instances” at the top left. When this tutorial was created, there were no existing instances within the workshop project, so, those of you following this tutorial within the second workshop, will see a page with No items to display and anyone who sign in after a few instances have been created, would see a list of instances that pre-exist at that time.\n\n\nOk, so how do we actually do all this? Well, from this page we can create our own servers on Exeter’s OpenStack using an image of a machine the Research IT team have created earlier for you. Servers which are created in this manner are called ‘Instances’. We can create as many servers as we like, start them, log-in to them, do some work, transfer data to/from them or destroy them altogether. We let the Research IT team worry about the hardware, power, cooling and maintenance - all we need to do is specify how powerful a computer we want (tiny, small, medium, large or extra-large).\nThe reason we are using the OpenStack here is that many of you have to analyse large datasets. But you only need to cruch those datasets occasionally. In the case of high-throughput sequencing data (e.g. Illumina or Oxford Nanopore) you will find that your desktop PC may not be powerful enough to cope with the data. Also installing and configuring many pieces of software (often written by other scientists), is frequently painful as they often require other programs (also often written by other scientists) to be installed as well. Factor in some obscure incompatibility and you can quickly end up in a situation where two programs that you need refuse to co-exist. With OpenStack, you can just start a server instance based on an image created by someone else who has already done all the hard work of installation and configuration. You may also want to use an existing docker- or singularity- container within such an instance to carry out your workflow.\nSo let’s get on and launch our first instance!"
  },
  {
    "objectID": "posts/intro_to_openstack/01_OpenStack_Console.html#some-terminology",
    "href": "posts/intro_to_openstack/01_OpenStack_Console.html#some-terminology",
    "title": "OpenStack Console",
    "section": "Some Terminology",
    "text": "Some Terminology\nAn Image: This is the starting point or template for the course. Think of it as a master copy of the computer which contains all the programs and data that are required to follow the course. We will use it as a template to start your own Instance.\nAn Instance: Almost the first thing you will do is create your own copy of the image - we call this an instance or a virtual machine. It contains everything that was in the image plus any files you create during the course.\nA Volume This can be thought of as an additional hard disk which you can add or remove from an instance.\nRemote Desktop We can’t plug a monitor and keyboard into a virtual machine (instance) so we need a program on our computer which connects to the instance allows us to see and control the instance from our desktop. In the course we will use a program called X2Go.\nCommand Line Client It is not really necessary to connect to an instance using remote desktop software. It is possible to connect only via a command line interfaces - you will not get any menus, icons images - you will only be able to type commands to tell the instance what you want to do."
  },
  {
    "objectID": "posts/intro_to_openstack/01_OpenStack_Console.html#why-use-virtual-machines",
    "href": "posts/intro_to_openstack/01_OpenStack_Console.html#why-use-virtual-machines",
    "title": "OpenStack Console",
    "section": "Why Use Virtual Machines?",
    "text": "Why Use Virtual Machines?\nInstead of relying on dedicated physical hardware for every task, virtual machines (VMs) offer a highly flexible and efficient alternative. Imagine a scenario where you have a set number of powerful physical computers. Rather than assigning one computer to each user, VMs allow you to pool the resources of these physical machines and then divide them into multiple isolated virtual environments.\nThis approach offers significant advantages:\n\nOptimal Resource Utilization: Physical machines often sit idle or are underutilized. VMs allow you to share resources like disk space, memory, and processors among many virtual instances. This means you can run more virtual machines than you have physical ones, making the most of your hardware. When a VM isn’t heavily used, its resources can be dynamically allocated to other, more active VMs.\nCost and Environmental Benefits: By maximizing hardware utilization, you can reduce the number of physical servers needed. This translates to lower initial purchasing costs, reduced energy consumption for operation and cooling, and a smaller physical footprint. This makes virtual machines a greener option in many cases, contributing to sustainability efforts.\n\n\nThe Power of Full Control\nOne of the most compelling reasons to use virtual machines is the full control you gain over your environment. Each virtual machine acts like an independent computer, allowing you to configure and customize it precisely to your needs.\nHere’s how this translates into practical benefits:\n\nInstant Provisioning and Customization: You can quickly launch new virtual machines with a wide variety of pre-configured machine images. These images can range from basic operating systems to specialized environments equipped with specific software for scientific modeling, data analysis, or development. This eliminates the need for lengthy manual installations and configurations.\nShare and Collaborate with Ease: If you’ve developed a specific computational environment or analysis workflow, you can create and share your own VM images. This allows collaborators to instantly access and replicate your setup, ensuring consistency and simplifying the sharing of complex projects.\nDynamic Resource Scalability: Need more storage or processing power for a task? With VMs, you can often add storage on-the-fly by attaching virtual volumes, or increase processing power and RAM with just a few clicks. This agility means you can adapt your environment as your computational demands change, without the need for physical hardware upgrades or downtime.\nFlexible Testing and Development: VMs are invaluable for researchers. You can easily create a machone to test the inner workings of the code you are developing before deploying them to larger, more complex systems. This isolated testing environment prevents conflicts and ensures smooth transitions.\n\nEssentially, virtual machines empower you to create, modify, and manage your computational environment with unparalleled flexibility, giving you complete command over your digital workspace."
  },
  {
    "objectID": "posts/intro_to_openstack/01_OpenStack_Console.html#other-items-of-interest.",
    "href": "posts/intro_to_openstack/01_OpenStack_Console.html#other-items-of-interest.",
    "title": "OpenStack Console",
    "section": "Other Items of interest.",
    "text": "Other Items of interest.\n\nObject store: This is another type of storage, but is relatively slow. It is ideal for archive or when storing large files (i.e. several Gb).\nCompute clusters: A way of linking and managing groups of Compute instances to act as a cluster. Yes - you can create your own private cluster!\nA ton of other services that are geared towards building highly scalable and fault-tolerant web-based services. Many can be co-opted for use in research!\n\n\nAfter logging in you’ll be presented with a wide range of options.\n\n\nAn Image: This is the starting point or template for the course. Think of it as a master copy of the computer which contains all the programs and data that are required to follow the course. We will use it as a template to start your own Instance.\nAn Instance: Almost the first thing you will do is create your own copy of the image - we call this an instance or a virtual machine. It contains everything that was in the image plus any files you create during the course.\nA Volume This can be thought of as an additional hard disk which you can add or remove from an instance.\nRemote Desktop We can’t plug a monitor and keyboard into a virtual machine (instance) so we need a program on our computer which connects to the instance allows us to see and control the instance from our desktop. In the course we will use a program called X2Go.\nCommand Line Client It is not really necessary to connect to an instance using remote desktop software. It is possible to connect only via a command line interfaces - you will not get any menus, icons images - you will only be able to type commands to tell the instance what you want to do."
  },
  {
    "objectID": "posts/intro_to_openstack/01_OpenStack_Console.html#create-your-instance",
    "href": "posts/intro_to_openstack/01_OpenStack_Console.html#create-your-instance",
    "title": "OpenStack Console",
    "section": "Create your Instance",
    "text": "Create your Instance\nIn this section of the workshop we will create our Instance in Exeter’s OpenStack system.\n\n\n\n\n\n\nImportant\n\n\n\nIf you have an instance from a previous session do not create another one. Please go to the Managing OpenStack page and restart your existing instance.\nOtherwise let’s get on and launch the instance we need for our workshop !\n\n\nClick on the “Launch Instance” button.\n\nGive your instance your name (e.g. “Jane Doe’s Instance”)\nRemember that everyone is sharing the same project- so you will need to be identify your instance from everyone elses. The example above (“my_first_VM”) may be suitable for your own project, but it is not appropriate in a shared project.\nClick ‘Next’ and proceed to choose the image of the machine you want to replicate. In this example, we will be using the Ubuntu 22.04 (Desktop) image. Click on the highlighted arrow across from the “Ubuntu 22.04 (Desktop)” name and input a Volume Size of 20GB. Make sure to toggle the Delete Volume on Instance Delete option in this case.\n\n\n\n\n\n\nCaution\n\n\n\nWhen you are making your own VMs, within your own research project, you may chose to use different options based on your needs. Here, we have limited resources available for the workshop, so we need to make sure that everyone follows the instructions.\n\n\n\nClick Next and select the desired flavour for this instance. Here, we would like you to select the ARC_Intro_Penryn - small flavour that will allow you to launch an instance with 1vCPU and 1GB of RAM. When you register your own project for access to OpenStack, you will see additional options for these flavours.\n\n\nSecurity Groups\nClick ‘Next’ to move on through Networks and Network Ports section and arrive at the Security Groups section. Here, click on the default security group to expand it, and to have a look at what has already been allowed in the firewall.\n\nHere, we can see that SSH (port 22) has been allowed over the tcp protocol from anywhere (0.0.0.0/0). That is all we need to begin with, so we can go ahead and click Next.\n\n\n\n\n\n\nSecurity Groups\n\n\n\nIf you are planning on using additional software that may need some ports to be opended up, then you can create your own security group with relevant ports allowed. For e.g. if you want to use VNC type software for remote desktop access to your instance, you will need to allow port 5901 etc. over tcp.\n\n\n\n\nKey Pairs and Cloud Config\nSetting up virtual machine (VM) instances in a cloud environment like OpenStack requires robust security measures and efficient configuration methods. This section details the critical steps involved in establishing secure access using SSH keypairs and automating initial VM setup with cloud-init, providing a foundational understanding for workshop participants.\nClick ‘Next’ to move into the Key Pair section ::: callout-tip ## Key Pair OpenStack leverages SSH keypairs to secure VM instances at their launch time. When an instance is provisioned, OpenStack injects the user’s public key into the appropriate authorized_keys file on the new VM, making it ready for secure SSH access using the corresponding private key held by the user.\n\n\n\n\n\n\nPrivate Key\n\n\n\nThe private key, which is the secret component, must remain securely on the user’s local workstation and should never be uploaded to the cloud provider. This distinction is crucial for maintaining the integrity of the security model.\n\n\nA key pair allows you to SSH into your newly created instance. You may select an existing key pair, import a key pair, or generate a new key pair. Click on the Create Key Pair button highlighted above and then enter a Key Pair Name and Key Type in the pop-up box that appears next. Key Pairs are how you login to your instance after it is launched.\n\nChoose a key pair name you will recognize. Names may only include alphanumeric characters, spaces, or dashes. Once the Key Pair is generated, you will need to copy the Private Key to your clipborad and save it in a safe space. Do not forget where you keep this key - you will not be able to login to the instance without this key pair if you have not enabled any other login mechanism.\n\nOnce copied, click Done. Open a text editor like Notepad++, Context or similar and save this SSH Key. The image below shows the successfully created Key Pair.\n\n\n\n\n\n\n\nKey Pair vs Password\n\n\n\nThe choice of authentication method for remote access significantly impacts the security and operational efficiency of cloud environments. While password-based authentication is familiar, SSH keypair authentication offers substantial advantages, making it the preferred standard in cloud platforms like OpenStack.\n\nPassword authentication relies on a user remembering a secret string (the password) and providing it to the server for verification.\n\nSSH keypair authentication provides a superior level of cryptographic strength that even extremely long, complex passwords cannot match. The public keys, generated using sophisticated mathematical algorithms, are virtually impossible to guess or brute-force, offering a robust defense against common attack vectors - as long as un-authorized users do not gain access to your private key.\n\n\nClick Next to move on to the Configuration section.\n\n\n\n\n\n\nCloud Config\n\n\n\nBeyond establishing secure access, automating the initial configuration of virtual machines is crucial for efficiency, consistency, and scalability in cloud environments. Cloud-init serves as the industry-standard tool for this purpose.\n\n\nWhen a new cloud instance is deployed, cloud-init takes initial configuration data, often referred to as “user-data” or “vendor-data,” and automatically applies these settings during the instance’s first boot process. This functions much like a dynamic to-do list that cloud-init executes, configuring the VM according to predefined instructions. The significant advantage of cloud-init lies in its ability to reuse these configuration instructions across numerous deployments, consistently producing reliable results.\nCloud-init is capable of handling a wide array of initial configuration tasks. These include essential activities such as setting the hostname, configuring network interfaces, creating and managing user accounts, installing necessary software packages, and executing custom scripts. This comprehensive automation streamlines the deployment process, ensuring that all cloud instances are configured uniformly, thereby minimizing the potential for human error.\nBelow is a minimal cloud-init script to configure essentials such as adding a local user account with ability to install research related applications (e.g. sudo apt install package-name). This will be all you really need for most use cases, but for additional configuration options, please refer to the cloud-init documentation. Amend the settings as required to look as below (changing the ‘user123’, ‘User OneTwoThree’ and hashed_passwd lines):\n#cloud-config\nusers:\n - default\n - name: user123\n   gecos: User OneTwoThree\n   sudo: ALL=(ALL) NOPASSWD:ALL\n   shell: /bin/bash\n   lock_passwd: false\n   ssh-import-id: ab123\n   hashed_passwd: “$6$rTACxh6DNbF/$p ... &lt;snip&gt; ... xwWllho2N7nKq1nn1”\n# You have to keep the $6$ in front of the hashed password\n# Update system package database\npackage_update: true\n# Upgrade all packages to the latest versions\npackage_upgrade: true\n# Install specified basic tools using the package manager\npackages:\n  - openssh-server\n  - ntop\n  - nload\n  - htop\n  - vim\n  - multitail\n  - tmux\n# Install Mamba using the Miniforge distribution\nruncmd:\n  - echo \"Starting Miniforge installation...\"\n  - wget -O /tmp/Miniforge3.sh \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\"\n  - chmod +x /tmp/Miniforge3.sh\n  - /tmp/Miniforge3.sh -b -p /opt/miniforge3\n  - echo \"Miniforge installation complete. Initializing conda for all users...\"\n  - /opt/miniforge3/bin/conda init bash --system\n  - echo \"Conda initialization for all users complete. Users may need to log out and back in.\"\nCopy paste the above script into a text editor like Notepad++, Context, Subline etc. and edit the requisite sections such as name, gecos, hashed_password etc. as needed.\n\n\n\n\n\n\nNote\n\n\n\nNote: The ‘hashed_passwd’ string is a crypt-sha512 hash (i.e., an encrypted version) of the password you will use to log in with (we advise not using your university password). Hashed passwords can be generated using a web page such as https://www.mkpasswd.net. You need to replace all of the ‘hashed_passwd’ field inside the double quotes with the hashed password you have generated.\n\n\n\nOnce you have your template ready, paste it into the Customisation Script section:\n\nClick Next and skip over the Server Groups, Scheduler Hints and Metadata sections.\n\nYou have just configured an instance! Well done!\n\nIn summary, you have given your instance a name, chosen which source machine image to base the instance on, selected the size of the instance (i.e. amount of memory, disk space and processor power), configured the basic apps you wanted to pre-install on it and also configured how you can access it (key-pair vs username+password).\nWe’re ready to launch it! Click ‘Launch Instance’. At this point you wait for the instance to come online. This may take a few seconds for a small instance or about 10 minutes for an xx-large one.\nBelow you can see an example of an instance starting up (or spawning). It is running in the University data centre. Note that we don’t need to worry or think about the underlying hardware the system is running on. That’s all taken care of for us by OpenStack and the Research IT team.\n\nOnce it turns white and the ‘Status’ changes to ‘Active’, you should copy the IP address to the clipboard.\n\n\n\n\n\n\n\nNote\n\n\n\nNote: above is an example where the IP address is 10.121.4.232 (yours will be different). We will need it in a moment to connect via X2Go remote desktop. Make sure you copy the whole address and nothing but the address.\n\n\nCongratulations you have ‘configured’ and ‘turned on’ a new computer to use for the rest of this workshop.\nNext, you will learn how to connect to it from your PC."
  },
  {
    "objectID": "posts/intro_to_openstack/02_Connect_to_your_instance.html",
    "href": "posts/intro_to_openstack/02_Connect_to_your_instance.html",
    "title": "OpenStack Console",
    "section": "",
    "text": "Important\n\n\n\nDownload and install a tool like Putty, MobaXTerm or Termius on your computer.\n\nThe most commonly used, but basic program is PuTTy.\n\nThe one we recommend for Windows users is MobaXTerm - it provides all the important remote network tools (SSH, X11, RDP, VNC, FTP, MOSH, …) and Unix commands (bash, ls, cat, sed, grep, awk, rsync, …) to Windows desktop, in a single portable exe file which works out of the box.\n\nTermius is another recommended SSH client that works on macOS, Windows, Linux, iOS and Andriod.\n\nEnd-Users are responsible for ensuring that they comply with the license conditions for any of these software."
  },
  {
    "objectID": "posts/intro_to_openstack/02_Connect_to_your_instance.html#prerequisite",
    "href": "posts/intro_to_openstack/02_Connect_to_your_instance.html#prerequisite",
    "title": "OpenStack Console",
    "section": "",
    "text": "Important\n\n\n\nDownload and install a tool like Putty, MobaXTerm or Termius on your computer.\n\nThe most commonly used, but basic program is PuTTy.\n\nThe one we recommend for Windows users is MobaXTerm - it provides all the important remote network tools (SSH, X11, RDP, VNC, FTP, MOSH, …) and Unix commands (bash, ls, cat, sed, grep, awk, rsync, …) to Windows desktop, in a single portable exe file which works out of the box.\n\nTermius is another recommended SSH client that works on macOS, Windows, Linux, iOS and Andriod.\n\nEnd-Users are responsible for ensuring that they comply with the license conditions for any of these software."
  },
  {
    "objectID": "posts/intro_to_openstack/02_Connect_to_your_instance.html#connect-to-your-instance",
    "href": "posts/intro_to_openstack/02_Connect_to_your_instance.html#connect-to-your-instance",
    "title": "OpenStack Console",
    "section": "Connect to your instance",
    "text": "Connect to your instance\nIn addition to being extremely comprehensive, the Exeter OpenStack cloud has a relatively easy (but detailed!) interface for interacting with its offerings. All you have to do is log in via a web-browser and most of the functionality of the is available for you and relatively easy to use.\nTo get started go to the following URL and login with your University of Exeter username (in the format of user123) the domain (exeter.ac.uk), and the password you use when logging onto your university resources.\n\n\nCommand Line Connection\nThe simplest way you can connect to your instance is using SSH (secure shell). If you are happy using the command line only there are some other software that has additional functionality, and looks better. There are many choices - just Google ‘windows ssh client’\nThe most commonly used, but basic program is PuTTy. The one we recommend for Windows users is MobaXTerm - it provides all the important remote network tools (SSH, X11, RDP, VNC, FTP, MOSH, …) and Unix commands (bash, ls, cat, sed, grep, awk, rsync, …) to Windows desktop, in a single portable exe file which works out of the box.\nTermius is another recommended SSH client that works on macOS, Windows, Linux, iOS and Andriod.\nIn this section of the workshop we will create our Instance in Exeter’s OpenStack system.\n\n\n\n\n\n\nImportant\n\n\n\nIf you have an instance from a previous session do not create another one. Please go to the Managing OpenStack page and restart your existing instance.\nOtherwise let’s get on and launch the instance we need for our workshop !\n\n\n\n\nOnce it turns white and the ‘Status’ changes to ‘Active’, you should copy the IP address to the clipboard\n\nCopy the IP address address to the clipboard\nnote: above is an example where the IP address is 144.173.115.209 (yours will be different). We will need it in a moment to connect via X2Go remote desktop. Make sure you copy the whole address and nothing but the address.\nCongratulations you have ‘built’ a new computer to use for the rest of your course.\nNext, you will learn how to connect to it from your PC\n\n\n\nAcknowledgements\nWe did not create this content alone! Inspiration, tips, and resources have been borrowed from multiple sources."
  },
  {
    "objectID": "posts/intro_to_openstack/index.html#some-terminology",
    "href": "posts/intro_to_openstack/index.html#some-terminology",
    "title": "intRos: Introduction to OpenStack",
    "section": "3.1 Some Terminology",
    "text": "3.1 Some Terminology\nAn Image: This is the starting point or template for the course. Think of it as a master copy of the computer which contains all the programs and data that are required to follow the course. We will use it as a template to start your own Instance.\nAn Instance: Almost the first thing you will do is create your own copy of the image - we call this an instance or a virtual machine. It contains everything that was in the image plus any files you create during the course.\nA Volume This can be thought of as an additional hard disk which you can add or remove from an instance.\nRemote Desktop We can’t plug a monitor and keyboard into a virtual machine (instance) so we need a program on our computer which connects to the instance allows us to see and control the instance from our desktop. In the course we will use a program called X2Go.\nCommand Line Client It is not really necessary to connect to an instance using remote desktop software. It is possible to connect only via a command line interfaces - you will not get any menus, icons images - you will only be able to type commands to tell the instance what you want to do."
  },
  {
    "objectID": "posts/intro_to_openstack/index.html#why-use-virtual-machines",
    "href": "posts/intro_to_openstack/index.html#why-use-virtual-machines",
    "title": "intRos: Introduction to OpenStack",
    "section": "3.2 Why Use Virtual Machines?",
    "text": "3.2 Why Use Virtual Machines?\nInstead of relying on dedicated physical hardware for every task, virtual machines (VMs) offer a highly flexible and efficient alternative. Imagine a scenario where you have a set number of powerful physical computers. Rather than assigning one computer to each user, VMs allow you to pool the resources of these physical machines and then divide them into multiple isolated virtual environments.\nThis approach offers significant advantages:\n\nOptimal Resource Utilization: Physical machines often sit idle or are underutilized. VMs allow you to share resources like disk space, memory, and processors among many virtual instances. This means you can run more virtual machines than you have physical ones, making the most of your hardware. When a VM isn’t heavily used, its resources can be dynamically allocated to other, more active VMs.\nCost and Environmental Benefits: By maximizing hardware utilization, you can reduce the number of physical servers needed. This translates to lower initial purchasing costs, reduced energy consumption for operation and cooling, and a smaller physical footprint. This makes virtual machines a greener option in many cases, contributing to sustainability efforts.\n\n\nThe Power of Full Control\nOne of the most compelling reasons to use virtual machines is the full control you gain over your environment. Each virtual machine acts like an independent computer, allowing you to configure and customize it precisely to your needs.\nHere’s how this translates into practical benefits:\n\nInstant Provisioning and Customization: You can quickly launch new virtual machines with a wide variety of pre-configured machine images. These images can range from basic operating systems to specialized environments equipped with specific software for scientific modeling, data analysis, or development. This eliminates the need for lengthy manual installations and configurations.\nShare and Collaborate with Ease: If you’ve developed a specific computational environment or analysis workflow, you can create and share your own VM images. This allows collaborators to instantly access and replicate your setup, ensuring consistency and simplifying the sharing of complex projects.\nDynamic Resource Scalability: Need more storage or processing power for a task? With VMs, you can often add storage on-the-fly by attaching virtual volumes, or increase processing power and RAM with just a few clicks. This agility means you can adapt your environment as your computational demands change, without the need for physical hardware upgrades or downtime.\nFlexible Testing and Development: VMs are invaluable for researchers. You can easily create a machine to test the inner workings of the code you are developing before deploying them to larger, more complex systems. This isolated testing environment prevents conflicts and ensures smooth transitions.\n\nEssentially, virtual machines empower you to create, modify, and manage your computational environment with unparalleled flexibility, giving you complete command over your digital workspace.\nLet’s go to the next section and learn about the OpenStack Console."
  },
  {
    "objectID": "posts/intro_to_openstack/index.html#logging-into-the-console",
    "href": "posts/intro_to_openstack/index.html#logging-into-the-console",
    "title": "intRos: Introduction to OpenStack",
    "section": "3.3 Logging Into the Console",
    "text": "3.3 Logging Into the Console\nIn addition to being extremely comprehensive, the Exeter OpenStack cloud has a relatively easy (but detailed!) interface for interacting with its offerings. All you have to do is log in via a web-browser and most of the functionality of the system is available for you and relatively easy to use.\nTo get started go to the following URL and login with your University of Exeter username (in the format of user123) the domain (exeter.ac.uk), and the password you use when logging onto your university resources.\n\n\n\n\n\n\nWarning\n\n\n\nWe have set up a single workshop project which we will be using for training. Please do NOT abuse this project and do NOT put any personal, confidential or important data on the machines you will set-up during the training. You will all log in to the same management console and will be able to see machines which your friends and colleagues on the course will start.\n\n\n\nhttps://stack.exeter.ac.uk/\n\n\n\n\n\n\n\nImportant\n\n\n\nThis resource is only accessible within the University of Exeter network. So, if you are accessing this workshop from home or anywhere outside the campuses, you will need to connect to the University VPN service to be able to access the system. If using WiFi on campus, please use Eduroam or UoE_Secure only.\n\n\nOnce logged in, you will see a web-page that looks similar to this:\n\nAs a first step - let’s associate ourselves with the correct project. This is specially important for all users who may have access to multiple “projects” associated with their research groups. In this instance, please click on the Project dropdown and select ARC_Intro_Penryn - Workshop as your project. If done correctly, you should see a tick mark next to the selected project, as illustrated below.\n\nWe can now take a look at the various tabs. These include:\n\n\n\n\n\n\nInstances\n\n\n\nThis is the list of virtual machines that have been created.\n\n\n\n\n\n\n\n\nCompute\n\n\n\nThe service OpenStack is known for. It enables you to create Linux and Windows Virtual machines. Best of all their computational and/or disk capacity can be increased or decreased at the click of a button! This section has most of the things we need for this course.\n\n\n\n\n\n\n\n\nVolumes\n\n\n\nA storage unit. These can be created and attached to any compute which you have running. Ideal for moving large amounts of data between machines. Think of it as a removable hard drive which you can attach to any machine.\n\n\n\n\n\n\n\n\nImages\n\n\n\nThese are the templates from which you create instances.\n\n\n\n\n\n\n\n\nObject store\n\n\n\nThis is another type of storage, but is relatively slow. It is ideal for archive or when storing large files (i.e. several GB).However, it is currently disabled on the system. if you want to know more about it, please as your Research IT support team for assistance.\n\n\nClick on “Compute” and then “Overview” at the top left.\nOn this page you’ll get a summary of the state for your account. You can see an example below.\n\n\n\n\n\n\nNote\n\n\n\nNote that we are using a shared project where you’ll also see everyone else’s instances. If you were using your own project (or your research group’s project), you would only see your instances (or that of your research group).\n\n\n\nYou see here that we have a certain allocation of resources (memory, disk space, CPUs). We can choose how to allocate these to accomplish our scientific goals. For instance, we may need one very large machine with lots of resource to do a single task (e.g. a genome assembly). Or we may want lots of smaller machines, each doing a small task (e.g. a webserver and a database server). We may want half the machines running Unix and the other half running Windows. The point is, we are in control of the compute and can choose what suits us best and it can all be done, by us at the touch of a button.\nIn the example above, we have been allocated up-to 32 instances and a total of 32 cores with roughly 60GB of RAM so that each of the workshop attendees can spin up a virtual machine with 1vCPU and 2GB of RAM.\nClick on “Compute” and then “Instances” at the top left. When this tutorial was created, there were no existing instances within the workshop project, so, those of you following this tutorial within the second workshop, will see a page with No items to display and anyone who sign in after a few instances have been created, would see a list of instances that pre-exist at that time.\n\n\nOk, so how do we actually do all this? Well, from this page we can create our own servers on Exeter’s OpenStack using an image of a machine the Research IT team have created earlier for you. Servers which are created in this manner are called ‘Instances’. We can create as many servers as we like, start them, log-in to them, do some work, transfer data to/from them or destroy them altogether. We let the Research IT team worry about the hardware, power, cooling and maintenance - all we need to do is specify how powerful a computer we want (tiny, small, medium, large or extra-large).\nThe reason we are using the OpenStack here is that many of you have to analyse large datasets. But you only need to cruch those datasets occasionally. In the case of high-throughput sequencing data (e.g. Illumina or Oxford Nanopore) you will find that your desktop PC may not be powerful enough to cope with the data. Also installing and configuring many pieces of software (often written by other scientists), is frequently painful as they often require other programs (also often written by other scientists) to be installed as well. Factor in some obscure incompatibility and you can quickly end up in a situation where two programs that you need refuse to co-exist. With OpenStack, you can just start a server instance based on an image created by someone else who has already done all the hard work of installation and configuration. You may also want to use an existing docker- or singularity- container within such an instance to carry out your workflow.\nSo let’s get on and launch our first instance!"
  },
  {
    "objectID": "posts/intro_to_openstack/index.html#create-your-instance",
    "href": "posts/intro_to_openstack/index.html#create-your-instance",
    "title": "intRos: Introduction to OpenStack",
    "section": "3.4 Create your Instance",
    "text": "3.4 Create your Instance\nIn this section of the workshop we will create our Instance in Exeter’s OpenStack system.\n\n\n\n\n\n\nImportant\n\n\n\nIf you have an instance from a previous session do not create another one. Please go to the Managing Your OpenStack Instances and restart your existing instance.\nOtherwise let’s get on and launch the instance we need for our workshop !\n\n\nClick on the “Launch Instance” button.\n\nGive your instance your name (e.g. “Jane Doe’s Instance”)\n\n\n\n\n\n\n\nWarning\n\n\n\nRemember that everyone is sharing the same project- so you will need to be identify your instance from everyone elses. The example above (“my_first_VM”) may be suitable for your own project, but it is not appropriate in a shared project.\n\n\nClick ‘Next’ and proceed to choose the image of the machine you want to replicate. In this example, we will be using the Ubuntu 22.04 server image. Click on the highlighted arrow across from the “ubuntu_22.04” name and input a Volume Size of 20GB. Make sure to toggle the Delete Volume on Instance Delete option in this case.\n\n\n\n\n\n\nCaution\n\n\n\nWhen you are making your own VMs, within your own research project, you may choose to use different options based on your needs. Here, we have limited resources available for the workshop, so we need to make sure that everyone follows the instructions.\n\n\n\nClick Next and select the desired flavour for this instance. Here, we would like you to select the ARC_Intro_Penryn - small flavour that will allow you to launch an instance with 1vCPU and 2GB of RAM. When you register your own project for access to OpenStack, you will see additional options for these flavours.\n\n\nSecurity Groups\nClick ‘Next’ to move on through Networks and Network Ports section and arrive at the Security Groups section. Here, click on the default security group to expand it, and to have a look at what has already been allowed through the firewall.\n\nHere, we can see that SSH (port 22) has been allowed over the tcp protocol from anywhere (0.0.0.0/0). That is all we need to begin with, so we can go ahead and click Next.\n\n\n\n\n\n\nSecurity Groups\n\n\n\nIf you are planning on using additional software that may need some ports to be opened up, then you can create your own security group with relevant ports allowed. For e.g. if you want to use VNC type software for remote desktop access to your instance, you will need to allow port 5901 etc. over tcp.\nOur recipe for this workshop includes an installation of RStudio Server Open Source edition within the VM. We will show you how to edit the default security group to allow access to port 8787 for the RStudio Server later in the tutorial.\n\n\n\n\nKey Pairs and Cloud Config\nSetting up virtual machine (VM) instances in a cloud environment like OpenStack requires robust security measures and efficient configuration methods. This section details the critical steps involved in establishing secure access using SSH keypairs and automating initial VM setup with cloud-init, providing a foundational understanding for workshop participants.\nClick ‘Next’ to move into the Key Pair section. OpenStack leverages SSH keypairs to secure VM instances at their launch time. When an instance is provisioned, OpenStack injects the user’s public key into the appropriate authorized_keys file on the new VM, making it ready for secure SSH access using the corresponding private key held by the user.\n\n\n\n\n\n\nPrivate Key\n\n\n\nThe private key, which is the secret component, must remain securely on the user’s local workstation and should never be uploaded to the cloud provider. This distinction is crucial for maintaining the integrity of the security model.\n\n\nA key pair allows you to SSH into your newly created instance. You may select an existing key pair, import a key pair, or generate a new key pair. Click on the Create Key Pair button highlighted above and then enter a Key Pair Name and Key Type in the pop-up box that appears next. Key Pairs are how you login to your instance after it is launched.\n\nChoose a key pair name you will recognize. Here we chose user123_computer_name_projectX to remind ourselves what the default username as and which VM this key is associated with. Names may only include alphanumeric characters, spaces, or dashes. Once the Key Pair is generated, you will need to copy the Private Key to your clipboard and save it in a safe space. Do not forget where you keep this key - you will not be able to login to the instance without this key pair if you have not enabled any other login mechanism.\n\nOnce copied, click Done. Open a text editor like Notepad++, Context or similar and save this SSH Key. The image below shows the successfully created Key Pair. On most browsers, the key pair will automatically get downloaded when you click done as well.\n\n\n\n\n\n\n\nKey Pair vs Password\n\n\n\nThe choice of authentication method for remote access significantly impacts the security and operational efficiency of cloud environments. While password-based authentication is familiar, SSH keypair authentication offers substantial advantages, making it the preferred standard in cloud platforms like OpenStack.\n\nPassword authentication relies on a user remembering a secret string (the password) and providing it to the server for verification.\n\nSSH keypair authentication provides a superior level of cryptographic strength that even extremely long, complex passwords cannot match. The public keys, generated using sophisticated mathematical algorithms, are virtually impossible to guess or brute-force, offering a robust defense against common attack vectors - as long as un-authorized users do not gain access to your private key.\n\n\nClick Next to move on to the Configuration section.\n\n\n\n\n\n\nCloud Config\n\n\n\nBeyond establishing secure access, automating the initial configuration of virtual machines is crucial for efficiency, consistency, and scalability in cloud environments. Cloud-init serves as the industry-standard tool for this purpose.\n\n\nWhen a new cloud instance is deployed, cloud-init takes initial configuration data, often referred to as “user-data” or “vendor-data,” and automatically applies these settings during the instance’s first boot process. This functions much like a dynamic to-do list that cloud-init executes, configuring the VM according to predefined instructions. The significant advantage of cloud-init lies in its ability to reuse these configuration instructions across numerous deployments, consistently producing reliable results.\nCloud-init is capable of handling a wide array of initial configuration tasks. These include essential activities such as setting the hostname, configuring network interfaces, creating and managing user accounts, installing necessary software packages, and executing custom scripts. This comprehensive automation streamlines the deployment process, ensuring that all cloud instances are configured uniformly, thereby minimizing the potential for human error.\nBelow are two minimal cloud-init script examples to configure essentials such as adding a local user account with ability to install research related applications (e.g. sudo apt install package-name). This will be all you really need for most use cases, but for additional configuration options, please refer to the cloud-init documentation. Amend the settings as required to look as below (changing the ‘user123’, ‘Your Name’ and hashed_passwd lines):\n\n\n\n\n\n\nOption 1\n\n\n\nUse this config if you only want key-pair authentication:\n#cloud-config\n# Use this config if you only want key-pair authentication.\n# Change the default user to the name of my choice.\n# and give it sudo rights.\nsystem_info:\n  default_user:\n   name: user123\n   gecos: \"Your Name\"\n   groups: [sudo]\n   sudo: ALL=(ALL) NOPASSWD:ALL\n   shell: /bin/bash\n# Update system package database\npackage_update: true\n# Upgrade all packages to the latest versions\npackage_upgrade: true\n# Install specified basic tools using the package manager\npackages:\n  - openssh-server\n  - xubuntu-default-settings\n  - xubuntu-desktop\n  - xrdp\n  - nload\n  - htop\n  - vim\n  - multitail\n  - tmux\n  - r-base\n  - gdebi-core\n  - x2goserver\n  - x2goserver-xsession\n  - xfce4\n  - xfce4-goodies\n# Install Mamba using the Miniforge distribution\nruncmd:\n  - echo \"Starting Miniforge installation...\"\n  - wget -O /tmp/Miniforge3.sh \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\"\n  - chmod +x /tmp/Miniforge3.sh\n  - /tmp/Miniforge3.sh -b -p /opt/miniforge3\n  - echo \"Miniforge installation complete. Initializing conda for all users...\"\n  - /opt/miniforge3/bin/conda init bash --system\n  - echo \"Conda initialization for all users complete. Users may need to log out and back in.\"\n  - echo \"Installing RStudio Server Open Source v2025.05.1+513 ...\"\n  - wget -O /tmp/RStudio_Server.deb https://download2.rstudio.org/server/jammy/amd64/rstudio-server-2025.05.1-513-amd64.deb\n  - gdebi -n /tmp/RStudio_Server.deb\n  - echo \"Enabling Lightdm and XRDP ...\"\n  - systemctl enable --now lightdm\n  - systemctl enable --now xrdp\n  - systemctl set-default graphical.target\npower_state:\n   delay: 0\n   mode: reboot\n   message: \"Rebooting instance after cloud-init completion\"\n   condition: True\n\n\n\n\n\n\n\n\nOption 2\n\n\n\nUse this config if you want password authentication (or password authentication alongside key pair auth as an alternative shown earlier)\n#cloud-config\n# Change the default user to the name of my choice.\n# and give it sudo rights.\nsystem_info:\n  default_user:\n   name: user123\n   gecos: \"Your Name\"\n   sudo: ALL=(ALL) NOPASSWD:ALL\n   shell: /bin/bash\n   lock_passwd: false\n   hashed_passwd: “$6$rTACxh6 ... &lt;snip&gt; ... xwWllho2N7nKq1nn1”\n# You have to keep the $6$ in front of the hashed password\n# Enable password auth \nssh_pwauth: True\n# Update system package database\npackage_update: true\n# Upgrade all packages to the latest versions\npackage_upgrade: true\n# Install specified basic tools using the package manager\npackages:\n  - openssh-server\n  - xubuntu-default-settings\n  - xubuntu-desktop\n  - xrdp\n  - nload\n  - htop\n  - vim\n  - multitail\n  - tmux\n  - r-base\n  - gdebi-core\n  - x2goserver\n  - x2goserver-xsession\n  - xfce4\n  - xfce4-goodies\n# Install Mamba using the Miniforge distribution\nruncmd:\n  - echo \"Starting Miniforge installation...\"\n  - wget -O /tmp/Miniforge3.sh \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\"\n  - chmod +x /tmp/Miniforge3.sh\n  - /tmp/Miniforge3.sh -b -p /opt/miniforge3\n  - echo \"Miniforge installation complete. Initializing conda for all users...\"\n  - /opt/miniforge3/bin/conda init bash --system\n  - echo \"Conda initialization for all users complete. Users may need to log out and back in.\"\n  - echo \"Installing RStudio Server Open Source v2025.05.1+513 ...\"\n  - wget -O /tmp/RStudio_Server.deb https://download2.rstudio.org/server/jammy/amd64/rstudio-server-2025.05.1-513-amd64.deb\n  - gdebi -n /tmp/RStudio_Server.deb\n  - echo \"Enabling Lightdm and XRDP ...\"\n  - systemctl enable --now lightdm\n  - systemctl enable --now xrdp\n  - systemctl set-default graphical.target\npower_state:\n   delay: 0\n   mode: reboot\n   message: \"Rebooting instance after cloud-init completion\"\n   condition: True\n\n\nCopy paste the above script into a text editor like Notepad++, Context, Sublime etc. and edit the requisite sections such as name, gecos, hashed_password etc. as needed.\n\n\n\n\n\n\nNote\n\n\n\nNote: The ‘hashed_passwd’ string is a crypt-sha512 hash (i.e., an encrypted version) of the password you will use to log in with (we advise not using your university password). Hashed passwords can be generated using a web page such as https://www.mkpasswd.net. You need to replace all of the ‘hashed_passwd’ field inside the double quotes with the hashed password you have generated.\n\n\n\nOnce you have your template ready, paste it into the Customisation Script section:\n\nClick Next and skip over the Server Groups, Scheduler Hints and Metadata sections.\n\nYou have just configured an instance! Well done!\n\nIn summary, you have given your instance a name, chosen which source machine image to base the instance on, selected the size of the instance (i.e. amount of memory, disk space and processor power), configured the basic apps you wanted to pre-install on it and also configured how you can access it (key-pair vs username+password).\nWe’re ready to launch it! Click ‘Launch Instance’. At this point you wait for the instance to come online. This may take a few seconds for a small instance or about 10 minutes for an xx-large one.\nBelow you can see an example of an instance starting up (or spawning). It is running in the University data centre. Note that we don’t need to worry or think about the underlying hardware the system is running on. That’s all taken care of for us by OpenStack and the Research IT team.\n\nOnce it turns white and the ‘Status’ changes to ‘Active’, you should copy the IP address to the clipboard.\n\n\n\n\n\n\n\nNote\n\n\n\nNote: above is an example where the IP address is 10.121.4.147 (yours will be different). We will need it in a moment to connect via X2Go remote desktop. Make sure you copy the whole address and nothing but the address.\n\n\nCongratulations you have ‘configured’ and ‘turned on’ a new computer to use for the rest of this workshop.\nNext, you will learn how to connect to it from your PC. However, before you proceed any further - take a 20 minute break. The cloud-init script would be working in the background to finish installing software and configuring your machine during that time."
  },
  {
    "objectID": "posts/intro_to_openstack/index.html#connect-to-your-instance",
    "href": "posts/intro_to_openstack/index.html#connect-to-your-instance",
    "title": "intRos: Introduction to OpenStack",
    "section": "3.5 Connect to your instance",
    "text": "3.5 Connect to your instance\n\n\n\n\n\n\nPrerequisite\n\n\n\nDownload and install X2Go alongside a tool like Putty, MobaXTerm or Termius on your computer.\n\nThe most commonly used, but basic program is PuTTy.\n\nThe one we recommend for Windows users is MobaXTerm - it provides all the important remote network tools (SSH, X11, RDP, VNC, FTP, MOSH, …) and Unix commands (bash, ls, cat, sed, grep, awk, rsync, …) to Windows desktop, in a single portable exe file which works out of the box.\n\nTermius is another recommended SSH client that works on macOS, Windows, Linux, iOS and Andriod.\n\nEnd-Users are responsible for ensuring that they comply with the license conditions for any of these software.\n\n\nCheck the IP address of your instance and copy it to your clipboard.\n\n\nCommand Line Connection\nThe simplest way you can connect to your instance is using SSH (secure shell). If you are happy using the command line only, there are some other software that have additional functionality, and look better. There are many choices - just Google ‘windows ssh client’ or ‘OSX SSH Client’\nThe most commonly used, but basic program is PuTTy. The one we recommend for Windows users is MobaXTerm - it provides all the important remote network tools (SSH, X11, RDP, VNC, FTP, MOSH, …) and Unix commands (bash, ls, cat, sed, grep, awk, rsync, …) to Windows desktop, in a single portable exe file which works out of the box.\nTermius is another recommended SSH client that works on macOS, Windows, Linux, iOS and Andriod. Check out these detailed instructions on how to connect via SSH with an SSH Password or SSH Key.\n\n\n\n\n\n\nWarning\n\n\n\nWe do not recommend using Git Bash for this. Git Bash is an application for Microsoft Windows environments which provides an emulation layer for a Git command line experience. It does not have all the functionality one would usually need when accessing servers in this manner.\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf you are using a native terminal on MacOS or Unix, you could initiate a command line connection using:\nssh -i /path/to/your-private-key.key user123@&lt;vm-ip-address&gt;\n\n\n\n\nConsole connection using the browser\nWithin your OpenStack web console, navigate to the instance list and click on your instance in the list. In this example, we will click on the keypair_only instance.\n\nIn the next window click on the Console tab. Here, you will be able to see the main GUI of your computer. If you used the cloud-init Option 2 - then you will be able to login at this interface using the password you had used. If you used Option 1 during the cloud-init stage, then you will not be able to login using this method as you would not be aware of the correct password to use. In this case, you would need to reset the user password (covered elsewhere) to be able to use the method described here.\n\n\n\nGraphical connection using X2Go\nFor some modules you will need a full graphical desktop connection. This is a bit more complicated to set up, but will give you a full graphical desktop environment to work on. For this we use X2go\n\n\n\n\n\n\nHow to install X2Go\n\n\n\nSee here for detailed instructions on how to install X2Go client for your operating system. Note that you may need privilege escalation (a.k.a admin rights) on your machine to do this. If you have a University provided Windows laptop, you may also look for X2Go within the Company Portal for easy installation.\nIf you are on a University provided Mac running OSX, look for X2Go within the UoE Self Service app that has been pre-installed on your Mac device.\n\n\nOnce X2Go is installed, start the program from your desktop shortcut.\n\nOr look for it in your start menu\nWhen the programme launches for the first time, you may get a message as below, it only affects some features you don’t need.\n\nN.B - on some versions of windows you might get a different message - select ‘Allow Access’ if you have administrative rights or else click ‘keep blocking’ or ‘cancel’.\nYou should now see the main screen\n\nNow you need to tell your computer where to connect to. On a new installation the Session dialog (image below) will load automatically and you can start filling it up as shown below. On an existing installation of X2Go, you will need to Click on the ‘new session’ icon to set up a new connection.\n\nEnter the information as highlighted above. Make sure you use the IP address you copied earlier and use an appropriate name. If you decide to use the Key Pair, then click the Try auto login checkbox as well. If not using a key Pair, you can omit those two fields.\nOnce this has been done we need to change the connection settings. If you are doing this on campus, you should select LAN here. If out of campus, please choose a connection speed based on your internet connection. Most modern fiber broadband services at home can be categorized as WAN.\n\nNow let’s adjust the display settings so you can work comfortably at the correct screen resolution. Select the ‘Input/Output’ tab at the top and change the display to use the Use whole display as shown below. You may want to select Fullscreen instead.\n\nIn the Media tab, make sure that Enable sound support and Client side printer support are disabled (unless you need those options).\n\nOnce completed, click on OK. Your screen should now look like this:\n\nClick anywhere in the white area. The first time you connect to your instance (or if the IP address changes after a reboot) you will see a message like:\n\nClick Yes. If using a key-pair authentication, you will be connected automatically. If you have opted for password based authentication, then enter the password you chose when creating the crypt-sha512 hash."
  },
  {
    "objectID": "posts/intro_to_openstack/index.html#managing-your-openstack-instances",
    "href": "posts/intro_to_openstack/index.html#managing-your-openstack-instances",
    "title": "intRos: Introduction to OpenStack",
    "section": "3.6 Managing Your OpenStack Instances",
    "text": "3.6 Managing Your OpenStack Instances\nThis section contains some instructions on how to manage you OpenStack instances.\n\n\n\n\n\n\nImportant\n\n\n\nWhen you’re not working on the course please do be considerate and turn off your instance to avoid using unnecessary resource. If your instance is running, even if you are not doing anything on it, it will be using resource and may hinder others using the resources.\n\n\n\nFind your instance:\nLog onto the OpenStack console and find your instance. Click on Compute - Instances and if you cannot find your instance easily, use the filter button.\n\n\n\nStopping Your Instance:\nIf you wish to keep your data, use the Shut Off instance option. In this state the data written to disk will be preserved, but the instance will be ‘off’ (this is the equivalent of turning off a computer).\nUse the dropdown next to your instance and select Shut Off Instance (near the bottom)\n\nYou will be asked to confirm - double check it is the instance you intend to shut down.\n\n\nRestarting Your Instance:\nIf the status of your instance is Shutoff - click Start Instance.\nDouble check the IP address of your instance as it may change - you will need to use the new IP address to connect to you instance. In X2Go click on the drop down arrow to get the menu of options.\n\n\n\nDeleting an instance.\n\n\n\n\n\n\nCaution\n\n\n\nNote: this will destroy all work done to date.\n\n\nWhen you’re completely finished with the workshop, click the drop-down menu next to your instance and select Delete Instance. It will ask you to confirm - You can then watch the status change from Shutting down to Deleted."
  },
  {
    "objectID": "posts/intro_to_openstack/index.html#specific-use-cases",
    "href": "posts/intro_to_openstack/index.html#specific-use-cases",
    "title": "intRos: Introduction to OpenStack",
    "section": "3.7 Specific Use Cases",
    "text": "3.7 Specific Use Cases\nHere, we have configured a virtual machine that has an installation of RStudio Server (Open Source Edition) and it also has XRDP (a remote desktop for Ubuntu) installed on it. In this section, we will show you how to modify your Security Groups to allow access to these applications from your laptop.\nGo to the Network tab and then to the Security Groups pane within it. Click on Manage Rules on the far right-hand side.\n\nClick on Add Rule to create the new rules we need.\n\nRStudio Server listens on tcp port 8787 by default and XRDP listens on tcp port 3389. First, let’s add a custom rule for RStudio as follows:\n\nNow, we can add another rule for RDP. This is a well known service so we can select RDP from the drop down options and click Add.\n\nFinally, the new rules should look something like this:\n\nNow, you can access your RStudio Server’s web interface at http://10.121.4.35:8787/ (substitute the IP address shown here with your own VM’s IP address.\n\n\n\n\n\n\n\nCaution\n\n\n\nNote that the RStudio Server interface requires you to use a username and password combination. So, this example will only work if you chose Option 2 in the Cloud Config section."
  }
]